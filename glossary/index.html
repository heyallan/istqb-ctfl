<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ISTQB-CTFL Certified Tester Foundation Level</title>
	<meta name="description" content="Online Practice Assessment">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' width='48' height='48' viewBox='0 0 16 16'><text x='0' y='14'>ðŸ“•</text></svg>" />
	<link rel="stylesheet" href="https://unpkg.com/@subcomponents/reset@3.1.1/dist/reset.min.css">
	<link rel="stylesheet" href="https://unpkg.com/@subcomponents/layout@1.4.3/dist/layout.min.css">
	<link rel="stylesheet" href="https://unpkg.com/@subcomponents/grid@3.2.0/dist/grid.min.css"/>
	<link rel="stylesheet" href="https://unpkg.com/@subcomponents/properties@3.1.0/dist/properties.min.css">
	<link rel="stylesheet" href="/istqb-ctfl/styles.css">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;700&display=swap" rel="stylesheet">
	<script defer src="/istqb-ctfl/functions.js"></script>
	<script defer src="/istqb-ctfl/index.js"></script>
</head>
<body>

<h1><span class="underscore">ISTQB-CTFL Certified Tester Foundation Level</span></h1>
<p class="font-size-h2 font-family-heading font-weight-400 color-primary">Online Glossary</p>

<nav class="">
	<hr class="no-print margin-bottom-0.5">
	<p class="no-print margin-bottom-0.5">
		<a class="color-inherit" href="../">
			<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24" class="icon" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><circle cx="12" cy="12" r="10"></circle><polyline points="12 8 8 12 12 16"></polyline><line x1="16" x2="8" y1="12" y2="12"></line></svg>
			Back
		</a>
	</p>
	<hr>
</nav>


<div class="margin-bottom-1">
	<label class="font-weight-700" for="search-value">Search any term or concept</label>
	<div class="display-flex gap-1">
		<div class="width-max">
			<input class="display-block" id="search-value" type="search" placeholder="Search value">
		</div>
		<div class="width-min">
			<button class="" type="button">Search</button>
		</div>
	</div>
</div>


<dl id="dictionary">
	<dt>A/B testing</dt><dd>A statistical testing approach to determine which of two systems or components performs better.</dd><dt>abnormal end</dt><dd>The unintended termination of the execution of a component or system prior to completion.</dd><dt>abuse case</dt><dd>A use case in which some actors with malicious intent are causing harm to the system or to other actors.</dd><dt>acceptance criteria</dt><dd>The criteria that a component or system must satisfy in order to be accepted by a user, customer, or other authorized entity.</dd><dt>acceptance test-driven development</dt><dd>A collaboration-based test-first approach that defines acceptance tests in the stakeholders' domain language.</dd><dt>acceptance testing</dt><dd>A test level that focuses on determining whether to accept the system.</dd><dt>accessibility</dt><dd>The degree to which a component or system can be used by people with the widest range of characteristics and capabilities to achieve a specified goal in a specified context of use.</dd><dt>account harvesting</dt><dd>The process of obtaining user account information based on trial and error with the intention of using that information in a security attack.</dd><dt>accountability</dt><dd>The degree to which the actions of an entity can be traced uniquely to that entity.</dd><dt>actual result</dt><dd>The behavior produced/observed when a component or system is tested.</dd><dt>ad hoc review</dt><dd>A review technique performed informally without a structured process.</dd><dt>ad hoc testing</dt><dd>Informal testing performed without test analysis and test design.</dd><dt>adaptability</dt><dd>The degree to which a component or system can be adapted for different or evolving hardware, software or other operational or usage environments.</dd><dt>adversarial example</dt><dd>An input to an ML model created by applying small perturbations to a working example that results in the model outputting an incorrect result with high confidence.</dd><dt>adversarial testing</dt><dd>A test technique based on the attempted creation and execution of adversarial examples to identify defects in an ML model.</dd><dt>Agile Manifesto</dt><dd>A statement on the values that underpin Agile software development. The values are: individuals and interactions over processes and tools, working software over comprehensive documentation, customer collaboration over contract negotiation, responding to change over following a plan.</dd><dt>Agile software development</dt><dd>A group of software development methodologies based on iterative incremental development, where requirements and solutions evolve through collaboration between self-organizing cross-functional teams.</dd><dt>Agile test leader</dt><dd>A leadership role that serves agile teams, championing testing and quality at the organizational level.</dd><dt>Agile test team leader</dt><dd>A role that is responsible for maintaining solution quality within an agile delivery team.</dd><dt>alpha testing</dt><dd>A type of acceptance testing performed in the developer's test environment by roles outside the development organization.</dd><dt>analytical test strategy</dt><dd>A test strategy whereby the test team analyzes the test basis to identify the test conditions to cover.</dd><dt>analyzability</dt><dd>The degree to which an assessment can be made for a component or system of either the impact of one or more intended changes, the diagnosis of deficiencies or causes of failures, or the identification of parts to be modified.</dd><dt>anomaly</dt><dd>A condition that deviates from expectation.</dd><dt>anti-malware</dt><dd>Software that is used to detect and inhibit malware.</dd><dt>API testing</dt><dd>Testing performed by submitting requests to the test object using its application programming interface.</dd><dt>application programming interface</dt><dd>A type of interface in which the components or systems involved exchange information in a defined formal structure.</dd><dt>appropriateness recognizability</dt><dd>The degree to which users can recognize whether a component or system is appropriate for their needs.</dd><dt>atomic condition</dt><dd>A condition that does not contain logical operators.</dd><dt>attack vector</dt><dd>A path or means by which an attacker can gain access to a system for malicious purposes.</dd><dt>attacker</dt><dd>A person or process that attempts to access data, functions or other restricted areas of the system without authorization, potentially with malicious intent.</dd><dt>audio testing</dt><dd>Testing to determine if the game music and sound effects will engage the user in the game and enhance the game play.</dd><dt>audit</dt><dd>An independent examination of a work product or process performed by a third party to assess whether it complies with specifications, standards, contractual agreements, or other criteria.</dd><dt>authentication</dt><dd>A procedure determining whether a person or a process is, in fact, who or what it is declared to be.</dd><dt>authenticity</dt><dd>The degree to which the identity of a subject or resource can be proved to be the one claimed.</dd><dt>authorization</dt><dd>Permission given to a user or process to access resources.</dd><dt>automation code defect density</dt><dd>Defect density of a component of the test automation code.</dd><dt>automotive safety integrity level</dt><dd>One of four levels that specify the item's or element's necessary requirements of ISO 26262 and safety measures to avoid an unreasonable residual risk.</dd><dt>automotive SPICE</dt><dd>A process reference model and an associated process assessment model in the automotive industry.</dd><dt>availability</dt><dd>The degree to which a component or system is operational and accessible when required for use.</dd><dt>back-to-back testing</dt><dd>Testing to compare two or more variants of a test item or a simulation model of the same test item by executing the same test cases on all variants and comparing the results.</dd><dt>behavior-driven development</dt><dd>A collaborative approach to development in which the team is focusing on delivering expected behavior of a component or system for the customer, which forms the basis for testing.</dd><dt>beta testing</dt><dd>A type of acceptance testing performed at an external site to the developer's test environment by roles outside the development organization.</dd><dt>black-box test technique</dt><dd>A test technique based on an analysis of the specification of a component or system.</dd><dt>black-box testing</dt><dd>Testing based on an analysis of the specification of the component or system.</dd><dt>botnet</dt><dd>A network of compromised computers, called bots or robots, which is controlled by a third party and used to transmit malware or spam, or to launch attacks.</dd><dt>boundary value</dt><dd>A minimum or maximum value of an ordered equivalence partition.</dd><dt>boundary value analysis</dt><dd>A black-box test technique in which test cases are designed based on boundary values.</dd><dt>branch</dt><dd>A transfer of control between two nodes in the control flow graph of a test item.</dd><dt>branch coverage</dt><dd>The coverage of branches in a control flow graph.</dd><dt>branch testing</dt><dd>A white-box test technique in which the test conditions are branches.</dd><dt>bug hunting</dt><dd>An approach to testing in which gamification and awards for defects found are used as a motivator.</dd><dt>build verification test</dt><dd>An automated test that validates the integrity of each new build and verifies its key/core functionality, stability, and testability.</dd><dt>built-in quality</dt><dd>A set of practices to ensure that each solution meets quality standards throughout each increment of development, focusing on constructive quality assurance as a shared responsibility.</dd><dt>Capability Maturity Model Integration</dt><dd>A framework that describes the key elements of an effective product development and maintenance process. The Capability Maturity Model Integration covers best-practices for planning, engineering and managing product development and maintenance.</dd><dt>capacity</dt><dd>The degree to which the maximum limits of a component or system parameter meet requirements.</dd><dt>capacity testing</dt><dd>Testing to evaluate the capacity of a system.</dd><dt>capture/playback</dt><dd>A test automation approach in which inputs to the test object are recorded during manual testing to generate automated test scripts that can be executed later.</dd><dt>causal loop diagram</dt><dd>A graphical representation used to visualize cause-effect relationships and feedback loops in a system.</dd><dt>cause-effect diagram</dt><dd>A graphical representation used to organize and display the interrelationships of various possible root causes of a problem. Possible causes of a real or potential defect or failure are organized in categories and subcategories in a horizontal tree-structure, with the (potential) defect or failure as the root node.</dd><dt>cause-effect graph</dt><dd>A graphical representation of logical relationships between inputs (causes) and their associated outputs (effects) of a test object.</dd><dt>certification</dt><dd>The process of confirming that a component, system or person complies with specified requirements.</dd><dt>change-related testing</dt><dd>A type of testing initiated by modification to a component or system.</dd><dt>checklist-based review</dt><dd>A review technique guided by a list of questions or required attributes.</dd><dt>checklist-based testing</dt><dd>An experience-based test technique in which test cases are designed to exercise the items of a checklist.</dd><dt>classification tree</dt><dd>A tree diagram representing test data domains of a test object.</dd><dt>classification tree technique</dt><dd>A black-box test technique in which test cases are designed using a classification tree.</dd><dt>CLI testing</dt><dd>Testing performed by submitting commands to the software under test using a dedicated command-line interface.</dd><dt>closed-loop-system</dt><dd>A system in which the controlling action or input is dependent on the output or changes in output.</dd><dt>code injection</dt><dd>A type of security attack performed by inserting malicious code at an interface into an application to exploit poor handling of untrusted data.</dd><dt>coding standard</dt><dd>A standard that describes the characteristics of a design or a design description of data or program components.</dd><dt>coexistence</dt><dd>The degree to which a component or system can perform its required functions while sharing an environment and resources with other components or systems without a negative impact on any of them.</dd><dt>collaboration-based test approach</dt><dd>An approach to testing that focuses on defect avoidance by collaborating among stakeholders.</dd><dt>combinatorial testing</dt><dd>A black-box test technique in which test conditions are specific combinations of values of several parameters.</dd><dt>command-line interface</dt><dd>A type of interface in which the information is passed in form of command lines.</dd><dt>commercial off-the-shelf</dt><dd>A type of product developed in an identical format for a large number of customers in the general market.</dd><dt>compatibility</dt><dd>The degree to which a component or system can exchange information with other components or systems, and/or perform its required functions while sharing the same hardware or software environment.</dd><dt>complexity</dt><dd>The degree to which the design or code of a component or system is difficult to understand.</dd><dt>compliance</dt><dd>Adherence of a work product to standards, conventions or regulations in laws and similar prescriptions.</dd><dt>compliance testing</dt><dd>Testing to determine the compliance of the component or system.</dd><dt>component</dt><dd>A part of a system that can be tested in isolation.</dd><dt>component integration testing</dt><dd>The integration testing of components.</dd><dt>component testing</dt><dd>A test level that focuses on individual hardware or software components.</dd><dt>computer forensics</dt><dd>The practice of determining how a security attack has succeeded and assessing the damage caused.</dd><dt>concurrency</dt><dd>The simultaneous execution of multiple independent threads by a component or system.</dd><dt>concurrency testing</dt><dd>Testing to evaluate if a component or system involving concurrency behaves as specified.</dd><dt>condition coverage</dt><dd>The coverage of condition outcomes.</dd><dt>condition testing</dt><dd>A white-box test technique in which test conditions are outcomes of atomic conditions.</dd><dt>confidence interval</dt><dd>In managing project risks, the period of time within which a contingency action must be implemented in order to be effective in reducing the impact of the risk.</dd><dt>confidentiality</dt><dd>The degree to which a component or system ensures that data are accessible only to those authorized to have access.</dd><dt>configuration management</dt><dd>A discipline applying technical and administrative direction and surveillance to identify and document the functional and physical characteristics of a configuration item, control changes to those characteristics, record and report change processing and implementation status, and verify that it complies with specified requirements.</dd><dt>confirmation testing</dt><dd>A type of change-related testing performed after fixing a defect to confirm that a failure caused by that defect does not reoccur.</dd><dt>connectivity</dt><dd>The degree to which a component or system can connect to other components or systems.</dd><dt>consultative test strategy</dt><dd>A test strategy whereby the test team relies on the input of one or more key stakeholders to determine the details of the strategy.</dd><dt>context of use</dt><dd>Users, tasks, equipment (hardware, software and materials), and the physical and social environments in which a software product is used.</dd><dt>continuous integration</dt><dd>An automated software development procedure that merges, integrates and tests all changes as soon as they are committed.</dd><dt>continuous testing</dt><dd>An approach that involves a process of testing early, testing often, test everywhere, and automate to obtain feedback on the business risks associated with a software release candidate as rapidly as possible.</dd><dt>contractual acceptance testing</dt><dd>A type of acceptance testing performed to verify whether a system satisfies its contractual requirements.</dd><dt>control chart</dt><dd>A statistical process control tool used to monitor a process and determine whether it is statistically controlled. It graphically depicts the average value and the upper and lower control limits (the highest and lowest values) of a process.</dd><dt>control flow</dt><dd>The sequence in which operations are performed by a business process, component or system.</dd><dt>control flow analysis</dt><dd>A type of static analysis based on a representation of unique paths for executing a component or system.</dd><dt>convergence metric</dt><dd>A metric that shows progress toward a defined criterion, e.g., convergence of the total number of tests executed to the total number of tests planned for execution.</dd><dt>cost of quality</dt><dd>The total costs incurred on quality activities and issues and often split into prevention costs, appraisal costs, internal failure costs and external failure costs.</dd><dt>coverage</dt><dd>The degree to which specified coverage items are exercised by a test suite, expressed as a percentage.</dd><dt>coverage criteria</dt><dd>The criteria to define the coverage items required to reach a test objective.</dd><dt>coverage item</dt><dd>An attribute or combination of attributes derived from one or more test conditions by using a test technique.</dd><dt>Critical Testing Processes</dt><dd>A content-based model for test process improvement built around twelve critical processes. These include highly visible processes, by which peers and management judge competence and mission-critical processes in which performance affects the company's profits and reputation.</dd><dt>cross-browser compatibility</dt><dd>The degree to which a website or web application can function across different browsers and degrade gracefully when browser features are absent or lacking.</dd><dt>cross-site scripting</dt><dd>A vulnerability that allows attackers to inject malicious code into an otherwise benign website.</dd><dt>crowd testing</dt><dd>A test approach in which testing is distributed to a large group of testers.</dd><dt>custom tool</dt><dd>A software tool developed specifically for a set of users or customers.</dd><dt>cyclomatic complexity</dt><dd>The maximum number of linear, independent paths through a program.</dd><dt>dashboard</dt><dd>A representation of dynamic measurements of operational performance for some organization or activity, using metrics represented via metaphors such as visual dials, counters, and other devices resembling those on the dashboard of an automobile, so that the effects of events or activities can be easily understood and related to operational goals.</dd><dt>data flow analysis</dt><dd>A type of static analysis based on the lifecycle of variables.</dd><dt>data obfuscation</dt><dd>Data transformation that makes it difficult for a human to recognize the original data.</dd><dt>data privacy</dt><dd>The protection of personally identifiable information or otherwise sensitive information from undesired disclosure.</dd><dt>data-driven testing</dt><dd>A scripting technique that uses data files to contain the test data and expected results needed to execute the test scripts.</dd><dt>debugging</dt><dd>The process of finding, analyzing and removing the causes of failures in a component or system.</dd><dt>decision coverage</dt><dd>The coverage of decision outcomes.</dd><dt>decision table testing</dt><dd>A black-box test technique in which test cases are designed to exercise the combinations of conditions and the resulting actions shown in a decision table.</dd><dt>decision testing</dt><dd>A white-box test technique in which test cases are designed to execute decision outcomes.</dd><dt>defect</dt><dd>An imperfection or deficiency in a work product where it does not meet its requirements or specifications.</dd><dt>defect density</dt><dd>The number of defects per unit size of a work product.</dd><dt>Defect Detection Percentage</dt><dd>The number of defects found by a test level, divided by the number found by that test level and any other means afterwards.</dd><dt>defect management</dt><dd>The process of recognizing, recording, classifying, investigating, resolving and disposing of defects.</dd><dt>defect management committee</dt><dd>A cross-functional team of stakeholders who manage reported defects from initial detection to ultimate resolution (defect removal, defect deferral, or report cancellation). In some cases, the same team as the configuration control board.</dd><dt>defect report</dt><dd>Documentation of the occurrence, nature, and status of a defect.</dd><dt>defect taxonomy</dt><dd>A list of categories designed to identify and classify defects.</dd><dt>defect-based test technique</dt><dd>A test technique in which test cases are developed from what is known about a specific defect type.</dd><dt>definition-use pair</dt><dd>The association of a definition of a variable with the subsequent use of that variable.</dd><dt>demilitarized zone</dt><dd>A physical or logical subnetwork that contains and exposes an organization's external-facing services to an untrusted network, commonly the Internet.</dd><dt>denial of service</dt><dd>A security attack that is intended to overload the system with requests such that legitimate requests cannot be serviced.</dd><dt>device-based testing</dt><dd>A type of testing in which test suites are executed on physical or virtual devices.</dd><dt>driver</dt><dd>A component or tool that temporarily replaces another component and controls or calls a test item in isolation.</dd><dt>dynamic analysis</dt><dd>The process of evaluating a component or system based on its behavior during execution.</dd><dt>dynamic testing</dt><dd>Testing that involves the execution of the test item.</dd><dt>effectiveness</dt><dd>The extent to which correct and complete goals are achieved.</dd><dt>efficiency</dt><dd>The degree to which resources are expended in relation to results achieved.</dd><dt>EFQM model</dt><dd>A management framework that supports organisations in managing change and improving performance.</dd><dt>emulator</dt><dd>Software used during testing that mimics the behavior of hardware.</dd><dt>encryption</dt><dd>The process of encoding information so that only authorized parties can retrieve the original information, usually by means of a specific decryption key or process.</dd><dt>end-to-end testing</dt><dd>A test type in which business processes are tested from start to finish under production-like circumstances.</dd><dt>endurance testing</dt><dd>Testing to determine the stability of a system under a significant load over a significant period of time within the system's operational context.</dd><dt>entry criteria</dt><dd>The set of conditions for officially starting a defined task.</dd><dt>environment model</dt><dd>An abstraction of the real environment of a component or system including other components, processes, and environment conditions, in a real-time simulation.</dd><dt>epic</dt><dd>A large user story that cannot be delivered as defined within a single iteration or is large enough that it can be split into smaller user stories.</dd><dt>equivalence partition</dt><dd>A subset of the value domain of a variable within a component or system in which all values are expected to be treated the same based on the specification.</dd><dt>equivalence partitioning</dt><dd>A black-box test technique in which test conditions are equivalence partitions exercised by one representative member of each partition.</dd><dt>equivalent manual test effort</dt><dd>Effort required for running tests manually.</dd><dt>ergonomics testing</dt><dd>Testing to determine whether a component or system and its input devices are being used properly with correct posture.</dd><dt>error</dt><dd>A human action that produces an incorrect result.</dd><dt>error guessing</dt><dd>A test technique in which tests are derived on the basis of the tester's knowledge of past failures, or general knowledge of failure modes.</dd><dt>escaped defect</dt><dd>A defect that was not detected by a testing activity that is supposed to find that defect.</dd><dt>ethical hacker</dt><dd>A security tester using hacker techniques.</dd><dt>exhaustive testing</dt><dd>A test approach in which the test suite comprises all combinations of input values and preconditions.</dd><dt>exit criteria</dt><dd>The set of conditions for officially completing a defined task.</dd><dt>expected result</dt><dd>The observable predicted behavior of a test item under specified conditions based on its test basis.</dd><dt>experience-based test technique</dt><dd>A test technique based on the tester's experience, knowledge and intuition.</dd><dt>experience-based testing</dt><dd>Testing based on the tester's experience, knowledge and intuition.</dd><dt>expert usability review</dt><dd>An informal usability review in which the reviewers are experts. Experts can be usability experts or subject matter experts, or both.</dd><dt>exploratory testing</dt><dd>An approach to testing in which the testers dynamically design and execute tests based on their knowledge, exploration of the test item and the results of previous tests.</dd><dt>failed</dt><dd>The status of a test result if the actual result does not match the expected result.</dd><dt>failover</dt><dd>The backup operational mode in which the functions of a system that becomes unavailable are assumed by a secondary system.</dd><dt>failure</dt><dd>An event in which a component or system does not perform a required function within specified limits.</dd><dt>failure mode</dt><dd>The physical or functional manifestation of a failure.</dd><dt>failure mode and effect analysis</dt><dd>A systematic approach to risk identification and analysis of identifying possible modes of failure and attempting to prevent their occurrence.</dd><dt>failure rate</dt><dd>The ratio of the number of failures of a given category to a given unit of measure.</dd><dt>false-negative result</dt><dd>A test result which fails to identify the presence of a defect that is actually present in the test object.</dd><dt>false-positive result</dt><dd>A test result in which a defect is reported although no such defect actually exists in the test object.</dd><dt>fault injection</dt><dd>The process of intentionally adding a defect to a component or system to determine whether it can detect and possibly recover from it.</dd><dt>fault seeding</dt><dd>The process of intentionally adding defects to a component or system to monitor the rate of detection and removal, and to estimate the number of defects remaining.</dd><dt>fault tolerance</dt><dd>The degree to which a component or system operates as intended despite the presence of hardware or software faults.</dd><dt>fault tree analysis</dt><dd>A technique for analyzing the causes of failures that uses a hierarchical model of events and their logical relationships.</dd><dt>feature-driven development</dt><dd>An iterative and incremental software development process driven from a client-valued functionality (feature) perspective. Feature-driven development is mostly used in Agile software development.</dd><dt>field testing</dt><dd>A type of testing conducted to evaluate the system behavior under productive connectivity conditions in the field.</dd><dt>finding</dt><dd>A result of an evaluation that identifies some important issue, problem, or opportunity.</dd><dt>firewall</dt><dd>A component or set of components that controls incoming and outgoing network traffic based on predetermined security rules.</dd><dt>follow-up test case</dt><dd>A test case generated by applying a metamorphic relation to a source test case during metamorphic testing.</dd><dt>formal review</dt><dd>A type of review that follows a defined process with a formally documented output.</dd><dt>formative evaluation</dt><dd>A type of evaluation designed and used to improve the quality of a component or system, especially when it is still being designed.</dd><dt>functional appropriateness</dt><dd>The degree to which the functions facilitate the accomplishment of specified tasks and objectives.</dd><dt>functional completeness</dt><dd>The degree to which the set of functions covers all the specified tasks and user objectives.</dd><dt>functional correctness</dt><dd>The degree to which a component or system provides the correct results with the needed degree of precision.</dd><dt>functional safety</dt><dd>The absence of unreasonable risk due to hazards caused by malfunctioning behavior of Electric/Electronic(E/E) - Systems.</dd><dt>functional suitability</dt><dd>The degree to which a component or system provides functions that meet stated and implied needs when used under specified conditions.</dd><dt>functional testing</dt><dd>Testing performed to evaluate if a component or system satisfies functional requirements.</dd><dt>fuzz testing</dt><dd>A software testing technique used to discover security vulnerabilities by inputting massive amounts of random data, called fuzz, to the component or system.</dd><dt>generic test automation architecture</dt><dd>Representation of the layers, components, and interfaces of a test automation architecture, allowing for a structured and modular approach to implement test automation.</dd><dt>graphical user interface</dt><dd>A type of interface that allows users to interact with a component or system through graphical icons and visual indicators.</dd><dt>GUI testing</dt><dd>Testing performed by interacting with the software under test via the graphical user interface.</dd><dt>hacker</dt><dd>A person or organization who is actively involved in security attacks, usually with malicious intent.</dd><dt>hardware in the loop</dt><dd>Dynamic testing performed using real hardware with integrated software in a simulated environment.</dd><dt>hashing</dt><dd>Transformation of a variable length string of characters into a usually shorter fixed-length value or key. Hashed values, or hashes, are commonly used in table or database lookups. Cryptographic hash functions are used to secure data.</dd><dt>heuristic</dt><dd>A generally recognized rule of thumb that helps to achieve a goal.</dd><dt>heuristic evaluation</dt><dd>A usability review technique that evaluates a work product by using a set of heuristics.</dd><dt>high-level test case</dt><dd>A test case with abstract preconditions, input data, expected results, postconditions, and actions (where applicable).</dd><dt>human-centered design</dt><dd>An approach to design that aims to make software products more usable by focusing on the use of the software products and applying human factors, ergonomics, and usability knowledge and techniques.</dd><dt>hyperlink</dt><dd>A pointer within a web page that leads to other web pages.</dd><dt>hypothesis testing</dt><dd>The validation of a theory and its assumptions using sample data</dd><dt>impact analysis</dt><dd>The identification of all work products affected by a change, including an estimate of the resources needed to accomplish the change.</dd><dt>incremental development model</dt><dd>A type of software development lifecycle model in which the component or system is developed through a series of increments.</dd><dt>independence of testing</dt><dd>Separation of responsibilities, which encourages the accomplishment of objective testing.</dd><dt>independent test lab</dt><dd>An organization responsible to test and certify that the software, hardware, firmware, platform, and operating system follow all the jurisdictional rules for each location where the product will be used.</dd><dt>informal review</dt><dd>A type of review that does not follow a defined process and has no formally documented output.</dd><dt>information assurance</dt><dd>Measures that protect and defend information and information systems by ensuring their availability, integrity, authentication, confidentiality, and non-repudiation. These measures include providing for restoration of information systems by incorporating protection, detection, and reaction capabilities.</dd><dt>input data testing</dt><dd>A test level that focuses on the quality of the data used for training and prediction by ML models.</dd><dt>insider threat</dt><dd>A security threat originating from within the organization, often by an authorized system user.</dd><dt>insourced testing</dt><dd>Testing performed by people who are co-located with the project team but are not fellow employees.</dd><dt>inspection</dt><dd>A type of formal review that uses defined team roles and measurement to identify defects in a work product, and improve the review process and the software development process.</dd><dt>installability</dt><dd>The degree to which a component or system can be successfully installed and/or uninstalled in a specified environment.</dd><dt>integration testing</dt><dd>A test level that focuses on interactions between components or systems.</dd><dt>integrity</dt><dd>The degree to which a component or system allows only authorized access and modification to a component, a system or data.</dd><dt>interface testing</dt><dd>A type of integration testing performed to determine whether components or systems pass data and control correctly to one another.</dd><dt>internationalization</dt><dd>The process of making a system suitable for international user groups.</dd><dt>interoperability</dt><dd>The degree to which two or more components or systems can exchange information and use the information that has been exchanged.</dd><dt>intrusion detection system</dt><dd>A system which monitors activities on the 7 layers of the OSI model from network to application level, to detect violations of the security policy.</dd><dt>iterative development model</dt><dd>A type of software development lifecycle model in which the component or system is developed through a series of repeated cycles.</dd><dt>keyword-driven testing</dt><dd>A scripting technique in which test scripts contain high-level keywords and supporting files that contain low-level scripts that implement those keywords.</dd><dt>learnability</dt><dd>The degree to which a component or system can be used by specified users to achieve specified goals of learning with satisfaction and freedom from risk in a specified context of use.</dd><dt>level of intrusion</dt><dd>The level to which a test object is modified by adjusting it for testability.</dd><dt>level test plan</dt><dd>A test plan that typically addresses one test level.</dd><dt>linear scripting</dt><dd>A simple scripting technique without any control structure in the test scripts.</dd><dt>load generation</dt><dd>The process of simulating a defined set of activities at a specified load to be submitted to a component or system.</dd><dt>load generator</dt><dd>A tool that generates a load for a system under test.</dd><dt>load management</dt><dd>The control and execution of load generation, and performance monitoring and reporting of the component or system.</dd><dt>load profile</dt><dd>Documentation defining a designated number of virtual users who process a defined set of transactions in a specified time period that a component or system being tested may experience in production.</dd><dt>load testing</dt><dd>A type of performance testing conducted to evaluate the behavior of a component or system under varying loads, usually between anticipated conditions of low, typical, and peak usage.</dd><dt>localization</dt><dd>The process of making a system suitable for a specific user group.</dd><dt>low-level test case</dt><dd>A test case with concrete values for preconditions, input data, expected results, postconditions, and a detailed description of actions (where applicable).</dd><dt>maintainability</dt><dd>The degree to which a component or system can be modified by the intended maintainers.</dd><dt>maintenance</dt><dd>The process of modifying a component or system after delivery to correct defects, improve quality characteristics, or adapt to a changed environment.</dd><dt>maintenance testing</dt><dd>Testing the changes to an operational system or the impact of a changed environment to an operational system.</dd><dt>malware</dt><dd>Software that is intended to harm a system or its components.</dd><dt>malware scanning</dt><dd>Static analysis aiming to detect and remove malicious code received at an interface.</dd><dt>management review</dt><dd>A systematic evaluation of software acquisition, supply, development, operation, or maintenance process, performed by or on behalf of management that monitors progress, determines the status of plans and schedules, confirms requirements and their system allocation, or evaluates the effectiveness of management approaches to achieve fitness for purpose.</dd><dt>manufacturing-based quality</dt><dd>A view of quality measured by the degree that a product or service conforms to its intended design and requirements based on the process used.</dd><dt>master test plan</dt><dd>A test plan that is used to coordinate multiple test levels or test types.</dd><dt>math testing</dt><dd>Testing to determine the correctness of the pay table implementation, the random number generator results, and the return to player computations.</dd><dt>maturity</dt><dd>(1) The capability of an organization with respect to the effectiveness and efficiency of its processes and work practices. (2) The degree to which a component or system meets needs for reliability under normal operation.</dd><dt>MBT model</dt><dd>Any model used in model-based testing.</dd><dt>mean time between failures</dt><dd>The average time between consecutive failures of a component or system.</dd><dt>mean time to failure</dt><dd>The average time from the start of operation to a failure for a component or system.</dd><dt>mean time to repair</dt><dd>The average time a component or system will take to recover from a failure.</dd><dt>measurement</dt><dd>The process of assigning a number or category to an entity to describe an attribute of that entity.</dd><dt>memory leak</dt><dd>A memory access failure due to a defect in a program's dynamic store allocation logic that causes it to fail to release memory after it has finished using it.</dd><dt>metamorphic relation</dt><dd>A description of how a change in the test inputs from the source test case to the follow-up test case affects a change in the expected outputs from the source test case to the follow-up test case.</dd><dt>metamorphic testing</dt><dd>A test technique in which the inputs and expected results are extrapolated from a passing test case using a metamorphic relation.</dd><dt>method table</dt><dd>A table containing different test approaches, testing techniques and test types that are required depending on the Automotive Safety Integrity Level (ASIL) and on the context of the test object.</dd><dt>methodical test strategy</dt><dd>A test strategy whereby the test team uses a pre-determined set of test conditions such as a quality standard, a checklist, or a collection of generalized, logical test conditions which may relate to a particular domain, application or type of testing.</dd><dt>metric</dt><dd>A measurement scale and the method used for measurement.</dd><dt>ML functional performance</dt><dd>The degree to which an ML model meets ML functional performance criteria.</dd><dt>ML functional performance criteria</dt><dd>Criteria based on ML functional performance metrics used as a basis for model evaluation, tuning and testing.</dd><dt>ML functional performance metrics</dt><dd>A set of measures that relate to the functional correctness of an ML system.</dd><dt>ML model</dt><dd>An implementation of machine learning (ML) that generates a prediction, classification or recommendation based on input data.</dd><dt>ML model testing</dt><dd>A test level that focuses on the ability of an ML model to meet required ML functional performance criteria and non-functional criteria.</dd><dt>model coverage</dt><dd>The coverage of model elements.</dd><dt>model in the loop</dt><dd>Dynamic testing performed using a simulation model of the system in a simulated environment.</dd><dt>model-based test strategy</dt><dd>A test strategy whereby the test team derives testware from models.</dd><dt>model-based testing</dt><dd>Testing based on or involving models.</dd><dt>moderator</dt><dd>(1) The person responsible for running review meetings. (2) The person who performs a usability test session.</dd><dt>modifiability</dt><dd>The degree to which a component or system can be modified without degrading its quality.</dd><dt>modified condition/decision coverage</dt><dd>The coverage of all outcomes of the atomic conditions that independently affect the overall decision outcome.</dd><dt>modified condition/decision testing</dt><dd>A white-box test technique in which test cases are designed to exercise outcomes of atomic conditions that independently affect a decision outcome.</dd><dt>modularity</dt><dd>The degree to which a system is composed of discrete components such that a change to one component has minimal impact on other components.</dd><dt>monitoring tool</dt><dd>A software tool or hardware device that runs concurrently with the component or system under test and supervises, records and/or analyzes the behavior of the component or system.</dd><dt>multiplayer testing</dt><dd>Testing to determine if many players can simultaneously interact with the casino game world, with computer-controlled opponents, game servers, and with each other, as expected according to the game design.</dd><dt>multiple condition coverage</dt><dd>The coverage of all possible combinations of all single condition outcomes within one statement.</dd><dt>multiple condition testing</dt><dd>A white-box test technique in which test cases are designed to exercise outcome combinations of atomic conditions.</dd><dt>Myers-Briggs Type Indicator</dt><dd>An indicator of psychological preference representing the different personalities and communication styles of people.</dd><dt>N-switch coverage</dt><dd>The coverage of sequences of N+1 transitions.</dd><dt>negative testing</dt><dd>Testing a component or system in a way for which it was not intended to be used.</dd><dt>neighborhood integration testing</dt><dd>A type of integration testing in which all of the nodes that connect to a given node are the basis for the integration testing.</dd><dt>network zone</dt><dd>A sub-network with a defined level of trust. For example, the Internet or a public zone would be considered to be untrusted.</dd><dt>neuron coverage</dt><dd>The coverage of activated neurons in the neural network for a set of tests.</dd><dt>non-functional testing</dt><dd>Testing performed to evaluate that a component or system complies with non-functional requirements.</dd><dt>non-repudiation</dt><dd>The degree to which actions or events can be proven to have taken place, so that the actions or events cannot be repudiated later.</dd><dt>offline MBT</dt><dd>Model-based test approach whereby test cases are generated into a repository for future execution.</dd><dt>online MBT</dt><dd>Model-based test approach whereby test cases are generated and executed simultaneously.</dd><dt>open-source tool</dt><dd>A software tool that is available to all potential users in source code form, usually via the internet. Its users are permitted, usually under license, to study, change, improve and, at times, to distribute the software.</dd><dt>openâ€“loop-system</dt><dd>A system in which controlling action or input is independent of the output or changes in output.</dd><dt>operability</dt><dd>The degree to which a component or system has attributes that make it easy to operate and control.</dd><dt>operational acceptance testing</dt><dd>A type of acceptance testing performed to determine if operations and/or systems administration staff can accept a system.</dd><dt>operational profile</dt><dd>An actual or predicted pattern of use of the component or system.</dd><dt>operational profiling</dt><dd>The process of developing and implementing an operational profile.</dd><dt>organizational test strategy</dt><dd>A strategy that describes the generic requirements for testing and how to perform testing within an organization.</dd><dt>outsourced testing</dt><dd>Testing performed by people who are not co-located with the project team and are not fellow employees.</dd><dt>pair testing</dt><dd>A test approach in which two team members simultaneously collaborate on testing a work product.</dd><dt>pairwise integration testing</dt><dd>A type of integration testing that targets pairs of components that work together as shown in a call graph.</dd><dt>pairwise testing</dt><dd>A black-box test technique in which test cases are designed to exercise pairs of parameter-value pairs.</dd><dt>par sheet testing</dt><dd>Testing to determine that the game returns the correct mathematical results to the screen, to the players' accounts, and to the casino account.</dd><dt>pass/fail criteria</dt><dd>Decision rules used to determine whether a test item has passed or failed.</dd><dt>passed</dt><dd>The status of a test result if the actual result matches the expected result.</dd><dt>password cracking</dt><dd>A security attack recovering secret passwords stored in a computer system or transmitted over a network.</dd><dt>path</dt><dd>A sequence of consecutive edges in a directed graph.</dd><dt>path testing</dt><dd>A white-box test design technique in which test cases are designed to execute paths.</dd><dt>peer review</dt><dd>A review performed by others with the same abilities to create the work product.</dd><dt>penetration testing</dt><dd>A testing technique aiming to exploit security vulnerabilities (known or unknown) to gain unauthorized access.</dd><dt>performance efficiency</dt><dd>The degree to which a component or system uses time, resources and capacity when accomplishing its designated functions.</dd><dt>performance testing</dt><dd>Testing to determine the performance efficiency of a component or system.</dd><dt>performance testing tool</dt><dd>A test tool that generates load for a designated test item and that measures and records its performance during test execution.</dd><dt>perspective-based reading</dt><dd>A review technique in which a work product is evaluated from the perspective of different stakeholders with the purpose to derive other work products.</dd><dt>pharming</dt><dd>A security attack intended to redirect a website's traffic to a fraudulent website without the user's knowledge or consent.</dd><dt>phase containment</dt><dd>The policy of removing defects in the same phase of the software development lifecycle in which they were introduced.</dd><dt>phishing</dt><dd>An attempt to acquire personal or sensitive information by masquerading as a trustworthy entity in an electronic communication.</dd><dt>planning poker</dt><dd>A consensus-based estimation technique, mostly used to estimate effort or relative size of user stories in Agile software development. It is a variation of the Wideband Delphi method using a deck of cards with values representing the units in which the team estimates.</dd><dt>player perspective testing</dt><dd>Testing done by testers from a player's perspective to validate player satisfaction.</dd><dt>playtest</dt><dd>Ad hoc testing of a game by players to identify failures and gather feedback.</dd><dt>portability</dt><dd>The degree to which a component or system can be transferred from one hardware, software or other operational or usage environment to another.</dd><dt>post-release testing</dt><dd>A type of testing to ensure that the release is performed correctly and the application can be deployed.</dd><dt>postcondition</dt><dd>The expected state of a test item and its environment at the end of test case execution.</dd><dt>precondition</dt><dd>The required state of a test item and its environment prior to test case execution.</dd><dt>priority</dt><dd>The level of (business) importance assigned to an item, e.g., defect.</dd><dt>PRISMA</dt><dd>A systematic approach to risk-based testing that creates a product risk matrix.</dd><dt>probe effect</dt><dd>An unintended change in behavior of a component or system caused by measuring it.</dd><dt>process assessment</dt><dd>A disciplined evaluation of an organization's software processes against a reference model.</dd><dt>process-compliant test strategy</dt><dd>A test strategy whereby the test team follows a set of predefined processes, whereby the processes address such items as documentation, the proper identification and use of the test basis and test oracle(s), and the organization of the test team.</dd><dt>process-driven scripting</dt><dd>A scripting technique where scripts are structured into scenarios which represent use cases of the software under test. The scripts can be parameterized with test data.</dd><dt>product risk</dt><dd>A risk that impacts the quality of a product.</dd><dt>product-based quality</dt><dd>A view of quality measured by the degree that well-defined quality characteristics are met.</dd><dt>project risk</dt><dd>A risk that impacts project success.</dd><dt>proximity-based testing</dt><dd>A type of testing to confirm that sensors can detect nearby objects without physical contact.</dd><dt>pseudo-oracle</dt><dd>An independently derived variant of the test item used to generate results, which are compared with the results of the original test item based on the same test inputs.</dd><dt>quality</dt><dd>The degree to which a work product satisfies stated and implied needs of its stakeholders.</dd><dt>quality assistance</dt><dd>An approach to quality management that focuses on a quality culture throughout an organization.</dd><dt>quality assurance</dt><dd>Activities focused on providing confidence that quality requirements will be fulfilled.</dd><dt>quality capability</dt><dd>The abilities an organization needs to successfully accomplish its quality goals.</dd><dt>quality characteristic</dt><dd>A category of quality attributes that bears on work product quality.</dd><dt>quality coaching</dt><dd>The activities focused on helping an agile organization identify, understand, and deal with quality management, business value, flow of work, and customer collaboration.</dd><dt>quality control</dt><dd>Activities designed to evaluate the quality of a component or system.</dd><dt>quality culture</dt><dd>An organizational value system that results in an environment to establish and continually improve quality.</dd><dt>quality debt</dt><dd>The implied cost of deferred quality assurance activities.</dd><dt>quality function deployment</dt><dd>A facilitated workshop technique that helps determine critical characteristics for new product development.</dd><dt>quality management</dt><dd>The process of establishing and directing a quality policy, quality objectives, quality planning, quality control, quality assurance, and quality improvement for an organization.</dd><dt>quality risk</dt><dd>A product risk related to a quality characteristic.</dd><dt>RACI matrix</dt><dd>A matrix describing the participation by various roles in completing tasks or deliverables for a project or process. It is especially useful in clarifying roles and responsibilities. RACI is an acronym derived from the four key responsibilities most typically used: Responsible, Accountable, Consulted, and Informed.</dd><dt>ramp-down</dt><dd>A technique for decreasing the load on a system in a measurable and controlled way.</dd><dt>ramp-up</dt><dd>A technique for increasing the load on a system in a measurable and controlled way.</dd><dt>random testing</dt><dd>A black-box test technique in which input values are randomly generated based on an operational profile.</dd><dt>reactive test strategy</dt><dd>A test strategy whereby the test team waits to design and implement tests until the software is received, reacting to the actual system under test.</dd><dt>reactive testing</dt><dd>Testing that dynamically responds to the behavior of the test object and to test results being obtained.</dd><dt>reconnaissance</dt><dd>The exploration of a target area aiming to gain information that can be useful for an attack.</dd><dt>recoverability</dt><dd>The degree to which a component or system can recover the data directly affected by an interruption or a failure and re-establish the desired state of the component or system.</dd><dt>regression testing</dt><dd>A type of change-related testing to detect whether defects have been introduced or uncovered in unchanged areas of the software.</dd><dt>regression-averse test strategy</dt><dd>A test strategy whereby the test team applies various techniques to manage the risk of regression such as functional and/or non-functional regression test automation at one or more levels.</dd><dt>regulatory acceptance testing</dt><dd>A type of acceptance testing performed to determine the compliance of the test object.</dd><dt>reliability</dt><dd>The degree to which a component or system performs specified functions under specified conditions for a specified period of time.</dd><dt>reliability growth model</dt><dd>A model that shows the growth in reliability over time of a component or system as a result of the defect removal.</dd><dt>remote test lab</dt><dd>A facility that provides remote access to a test environment.</dd><dt>replaceability</dt><dd>The degree to which a component or system can replace another specified component or system for the same purpose in the same environment.</dd><dt>requirement</dt><dd>A provision that contains criteria to be fulfilled.</dd><dt>requirements-based testing</dt><dd>An approach to testing in which test conditions are based on requirements.</dd><dt>resource utilization</dt><dd>The degree to which the amounts and types of resources used by a component or system, when performing its functions, meet requirements.</dd><dt>retrospective</dt><dd>A regular event in which team members discuss results, review their practices, and identify ways to improve.</dd><dt>reusability</dt><dd>The degree to which a work product can be used in more than one system, or in building other work products.</dd><dt>review</dt><dd>A type of static testing in which a work product or process is evaluated by one or more individuals to detect defects or to provide improvements.</dd><dt>review plan</dt><dd>A document describing the approach, resources and schedule of intended review activities. It identifies, amongst others: documents and code to be reviewed, review types to be used, participants, as well as entry and exit criteria to be applied in case of formal reviews, and the rationale for their choice. It is a record of the review planning process.</dd><dt>reviewer</dt><dd>A participant in a review who identifies defects in the work product.</dd><dt>risk</dt><dd>A factor that could result in future negative consequences.</dd><dt>risk analysis</dt><dd>The overall process of risk identification and risk assessment.</dd><dt>risk assessment</dt><dd>The process to examine identified risks and determine the risk level.</dd><dt>risk control</dt><dd>The overall process of risk mitigation and risk monitoring.</dd><dt>risk identification</dt><dd>The process of finding, recognizing and describing risks.</dd><dt>risk impact</dt><dd>The damage that will be caused if the risk becomes an actual outcome or event.</dd><dt>risk level</dt><dd>The measure of a risk defined by risk impact and risk likelihood.</dd><dt>risk likelihood</dt><dd>The probability that a risk will become an actual outcome or event.</dd><dt>risk management</dt><dd>The process for handling risks.</dd><dt>risk mitigation</dt><dd>The process through which decisions are reached and protective measures are implemented for reducing or maintaining risks to specified levels.</dd><dt>risk monitoring</dt><dd>The activity that checks and reports the status of known risks to stakeholders.</dd><dt>risk-based testing</dt><dd>Testing in which the management, selection, prioritization, and use of testing activities and resources are based on corresponding risk types and risk levels.</dd><dt>role-based review</dt><dd>A review technique in which a work product is evaluated from the perspective of different stakeholders.</dd><dt>root cause</dt><dd>A source of a defect such that if it is removed, the occurrence of the defect type is decreased or removed.</dd><dt>root cause analysis</dt><dd>An analysis technique aimed at identifying the root causes of defects.</dd><dt>S.M.A.R.T. goal methodology</dt><dd>A methodology whereby objectives are defined very specifically rather than generically. SMART is an acronym derived from the attributes of the objective to be defined: Specific, Measurable, Attainable, Relevant and Timely.</dd><dt>safety integrity level</dt><dd>The level of risk reduction provided by a safety function, related to the frequency and severity of perceived hazards.</dd><dt>salting</dt><dd>A cryptographic technique that adds random data (salt) to the user data prior to hashing.</dd><dt>scalability</dt><dd>The degree to which a component or system can be adjusted for changing capacity.</dd><dt>scalability testing</dt><dd>Testing to determine the scalability of the software product.</dd><dt>scenario-based review</dt><dd>A review technique in which a work product is evaluated to determine its ability to address specific scenarios.</dd><dt>scribe</dt><dd>A person who records information at a review meeting.</dd><dt>script kiddie</dt><dd>A person who executes security attacks that have been created by other hackers rather than creating one's own attacks.</dd><dt>scripted testing</dt><dd>Testing (manual or automated) that follows a test script.</dd><dt>security</dt><dd>The degree to which a component or system protects its data and resources against unauthorized access or use and secures unobstructed access and use for its legitimate users.</dd><dt>security attack</dt><dd>An attempt to gain unauthorized access to a component or system, resources, information, or an attempt to compromise system integrity.</dd><dt>security audit</dt><dd>An audit evaluating an organization's security processes and infrastructure.</dd><dt>security policy</dt><dd>A high-level document describing the principles, approach and major objectives of the organization regarding security.</dd><dt>security procedure</dt><dd>A set of steps required to implement the security policy and the steps to be taken in response to a security incident.</dd><dt>security risk</dt><dd>A quality risk related to security.</dd><dt>security testing</dt><dd>Testing to determine the security of the software product.</dd><dt>security vulnerability</dt><dd>A weakness in the system that could allow for a successful security attack.</dd><dt>sequential development model</dt><dd>A type of software development lifecycle model in which a complete system is developed in a linear way of several discrete and successive phases with no overlap between them.</dd><dt>service virtualization</dt><dd>A technique to enable virtual delivery of services which are deployed, accessed and managed remotely.</dd><dt>session-based test management</dt><dd>A method for measuring and managing session-based testing.</dd><dt>session-based testing</dt><dd>A test approach in which test activities are planned as test sessions.</dd><dt>severity</dt><dd>The degree of impact that a defect has on the development or operation of a component or system.</dd><dt>shift left</dt><dd>An approach to performing testing and quality assurance activities as early as possible in the software development lifecycle.</dd><dt>shift right</dt><dd>An approach to testing a system continuously in production.</dd><dt>sign change coverage</dt><dd>The coverage of neurons activated with both positive and negative activation values in a neural network for a set of tests.</dd><dt>sign-sign coverage</dt><dd>The coverage achieved if by changing the sign of each neuron it can be shown to individually cause one neuron in the next layer to change sign while all other neurons in the next layer do not change sign for a set of tests.</dd><dt>simulator</dt><dd>A component or system used during testing which behaves or operates like a given component or system.</dd><dt>smoke test</dt><dd>A test suite that covers the main functionality of a component or system to determine whether it works properly before planned testing begins.</dd><dt>social engineering</dt><dd>An attempt to trick someone into revealing information (e.g., a password) that can be used to attack systems or networks.</dd><dt>software development lifecycle</dt><dd>The activities performed at each stage in software development, and how they relate to one another logically and chronologically.</dd><dt>software in the loop</dt><dd>Dynamic testing performed using real software in a simulated environment or with experimental hardware.</dd><dt>software lifecycle</dt><dd>The period of time that begins when a software product is conceived and ends when the software is no longer available for use. The software lifecycle typically includes a concept phase, requirements phase, design phase, implementation phase, test phase, installation and checkout phase, operation and maintenance phase, and sometimes, retirement phase. Note these phases may overlap or be performed iteratively.</dd><dt>software process improvement</dt><dd>A program of activities designed to improve the performance and maturity of the organization's software processes and the results of such a program.</dd><dt>software qualification test</dt><dd>Testing performed on completed, integrated software to provide evidence for compliance with software requirements.</dd><dt>Software Usability Measurement Inventory</dt><dd>A questionnaire-based usability testing tool that measures and benchmarks user experience.</dd><dt>source test case</dt><dd>A test case that passed and is used as the basis of follow-up test cases in metamorphic testing.</dd><dt>specification by example</dt><dd>A development technique in which the specification is defined by examples.</dd><dt>spike testing</dt><dd>Testing to determine the ability of a system to recover from sudden bursts of peak loads and return to a steady state.</dd><dt>SQL injection</dt><dd>A type of code injection in the structured query language (SQL).</dd><dt>standard-compliant test strategy</dt><dd>A test strategy whereby the test team follows a standard. Standards followed may be valid e.g., for a country (legislation standards), a business domain (domain standards), or internally (organizational standards).</dd><dt>state transition testing</dt><dd>A black-box test technique in which test cases are designed to exercise elements of a state transition model.</dd><dt>statement coverage</dt><dd>The coverage of executable statements.</dd><dt>statement testing</dt><dd>A white-box test technique in which test cases are designed to execute statements.</dd><dt>static analysis</dt><dd>The process of evaluating a component or system without executing it, based on its form, structure, content, or documentation.</dd><dt>static testing</dt><dd>Testing that does not involve the execution of a test item.</dd><dt>stress testing</dt><dd>A type of performance testing conducted to evaluate a system or component at or beyond the limits of its anticipated or specified workloads, or with reduced availability of resources such as access to memory or servers.</dd><dt>structural coverage</dt><dd>Coverage measures based on the internal structure of a component or system.</dd><dt>structured scripting</dt><dd>A scripting technique that builds and utilizes a library of reusable (parts of) scripts.</dd><dt>stub</dt><dd>A skeletal or special-purpose implementation of a software component, used to develop or test a component that calls or is otherwise dependent on it. It replaces a called component.</dd><dt>summative evaluation</dt><dd>A type of evaluation designed and used to gather conclusions about the quality of a component or system, especially when a substantial part of it has completed design.</dd><dt>system hardening</dt><dd>The step-by-step process of reducing the security vulnerabilities of a system by applying a security policy and different layers of protection.</dd><dt>system integration testing</dt><dd>The integration testing of systems.</dd><dt>system of systems</dt><dd>Multiple heterogeneous, distributed systems that are embedded in networks at multiple levels and in multiple interconnected domains, addressing large-scale inter-disciplinary common problems and purposes, usually without a common management structure.</dd><dt>system qualification test</dt><dd>Testing performed on the completed, integrated system of software components, hardware components, and mechanics to provide evidence for compliance with system requirements and that the complete system is ready for delivery.</dd><dt>system testing</dt><dd>A test level that focuses on verifying that a system as a whole meets specified requirements.</dd><dt>system throughput</dt><dd>The amount of data passing through a component or system in a given time period.</dd><dt>system under test</dt><dd>A type of test object that is a system.</dd><dt>System Usability Scale</dt><dd>A simple, ten-item attitude scale giving a global view of subjective assessments of usability.</dd><dt>Systematic Test and Evaluation Process</dt><dd>A structured testing methodology also used as a content-based model for improving the testing process. It does not require that improvements occur in a specific order.</dd><dt>technical review</dt><dd>A formal review by technical experts that examine the quality of a work product and identify discrepancies from specifications and standards.</dd><dt>test</dt><dd>A set of one or more test cases.</dd><dt>test adaptation layer</dt><dd>The layer in a test automation architecture which provides the necessary code to adapt test scripts on an abstract level to the various components, configuration or interfaces of the SUT.</dd><dt>test analysis</dt><dd>The activity that identifies test conditions by analyzing the test basis.</dd><dt>test approach</dt><dd>The manner of implementing testing tasks.</dd><dt>test architect</dt><dd>(1) A person who provides guidance and strategic direction for a test organization and for its relationship with other disciplines. (2) A person who defines the way testing is structured for a given system, including topics such as test tools and test data management.</dd><dt>test automation</dt><dd>The use of software to perform or support test activities.</dd><dt>test automation architecture</dt><dd>An instantiation of the generic test automation architecture to define the architecture of a test automation solution, i.e., its layers, components, services and interfaces.</dd><dt>test automation engineer</dt><dd>A person who is responsible for the design, implementation and maintenance of a test automation architecture as well as the technical evolution of the resulting test automation solution.</dd><dt>test automation framework</dt><dd>A tool that provides an environment for test automation. It usually includes a test harness and test libraries.</dd><dt>test automation manager</dt><dd>A person who is responsible for the planning and supervision of the development and evolution of a test automation solution.</dd><dt>test automation solution</dt><dd>A realization/implementation of a test automation architecture, i.e., a combination of components implementing a specific test automation assignment. The components may include commercial off-the-shelf test tools, test automation frameworks, as well as test hardware.</dd><dt>test automation strategy</dt><dd>A high-level plan to achieve long-term objectives of test automation under given boundary conditions.</dd><dt>test basis</dt><dd>The body of knowledge used as the basis for test analysis and design.</dd><dt>test case</dt><dd>A set of preconditions, inputs, actions (where applicable), expected results and postconditions, developed based on test conditions.</dd><dt>test case explosion</dt><dd>The disproportionate growth of the number of test cases with growing size of the test basis, when using a certain test design technique. Test case explosion may also happen when applying the test design technique systematically for the first time.</dd><dt>test charter</dt><dd>Documentation of the goal or objective for a test session.</dd><dt>test closure</dt><dd>During the test closure phase of a test process data is collected from completed activities to consolidate experience, testware, facts and numbers. The test closure phase consists of finalizing and archiving the testware and evaluating the test process, including preparation of a test evaluation report.</dd><dt>test completion</dt><dd>The activity that makes testware available for later use, leaves test environments in a satisfactory condition and communicates the results of testing to relevant stakeholders.</dd><dt>test completion report</dt><dd>A type of test report produced at completion milestones that provides an evaluation of the corresponding test items against exit criteria.</dd><dt>test condition</dt><dd>A testable aspect of a component or system identified as a basis for testing.</dd><dt>test control</dt><dd>The activity that develops and applies corrective actions to get a test project on track when it deviates from what was planned.</dd><dt>test cycle</dt><dd>An instance of the test process against a single identifiable version of the test object.</dd><dt>test data</dt><dd>Data needed for test execution.</dd><dt>test data preparation</dt><dd>The activity to select data from existing databases or create, generate, manipulate and edit data for testing.</dd><dt>test definition layer</dt><dd>The layer in a generic test automation architecture which supports test implementation by supporting the definition of test suites and/or test cases, e.g., by offering templates or guidelines.</dd><dt>test design</dt><dd>The activity that derives and specifies test cases from test conditions.</dd><dt>test director</dt><dd>A senior manager who manages test managers.</dd><dt>test environment</dt><dd>An environment containing hardware, instrumentation, simulators, software tools, and other support elements needed to conduct a test.</dd><dt>test estimation</dt><dd>An approximation related to various aspects of testing.</dd><dt>test execution</dt><dd>The activity that runs a test on a component or system producing actual results.</dd><dt>test execution automation</dt><dd>The use of software, e.g., capture/playback tools, to control the execution of tests, the comparison of actual results to expected results, the setting up of test preconditions, and other test control and reporting functions.</dd><dt>test execution layer</dt><dd>The layer in a generic test automation architecture which supports the execution of test suites and/or test cases.</dd><dt>test execution schedule</dt><dd>A schedule for the execution of test suites within a test cycle.</dd><dt>test execution tool</dt><dd>A test tool that executes tests against a designated test item and evaluates the outcomes against expected results and postconditions.</dd><dt>test generation layer</dt><dd>The layer in a generic test automation architecture which supports manual or automated design of test suites and/or test cases.</dd><dt>test harness</dt><dd>A collection of stubs and drivers needed to execute a test suite</dd><dt>test hook</dt><dd>A customized software interface that enables automated testing of a test object.</dd><dt>test implementation</dt><dd>The activity that prepares the testware needed for test execution based on test analysis and design.</dd><dt>test improvement plan</dt><dd>A plan for achieving organizational test process improvement objectives based on a thorough understanding of the current strengths and weaknesses of the organization's test processes and test process assets.</dd><dt>test infrastructure</dt><dd>The artifacts needed to perform testing, consisting of test environments, test tools, office environment and procedures.</dd><dt>test item</dt><dd>A part of a test object used in the test process.</dd><dt>test leader</dt><dd>On large projects, the person who reports to the test manager and is responsible for project management of a particular test level or a particular set of testing activities.</dd><dt>test level</dt><dd>A specific instantiation of a test process.</dd><dt>test log</dt><dd>A chronological record of relevant details about the execution of tests.</dd><dt>test logging</dt><dd>The activity of creating a test log.</dd><dt>test management</dt><dd>The process of planning, scheduling, estimating, monitoring, reporting, controlling, and completing test activities.</dd><dt>test management tool</dt><dd>A tool that supports test management.</dd><dt>test manager</dt><dd>The person responsible for project management of testing activities, resources, and evaluation of a test object.</dd><dt>Test Maturity Model integration</dt><dd>A five-level staged framework for test process improvement, related to the Capability Maturity Model Integration (CMMI), that describes the key elements of an effective test process.</dd><dt>test mission</dt><dd>The purpose of testing for an organization, often documented as part of the test policy.</dd><dt>test model</dt><dd>A model describing testware that is used for testing a component or a system under test.</dd><dt>test monitoring</dt><dd>The activity that checks the status of testing activities, identifies any variances from planned or expected, and reports status to stakeholders.</dd><dt>test object</dt><dd>The work product to be tested.</dd><dt>test objective</dt><dd>The purpose for testing.</dd><dt>test oracle</dt><dd>A source to determine an expected result to compare with the actual result of the system under test.</dd><dt>test plan</dt><dd>Documentation describing the test objectives to be achieved and the means and the schedule for achieving them, organized to coordinate testing activities.</dd><dt>test planning</dt><dd>The activity of establishing or updating a test plan.</dd><dt>test point analysis</dt><dd>A formula-based test estimation technique based on function point analysis.</dd><dt>test policy</dt><dd>A high-level document describing the principles, approach and major objectives of the organization regarding testing.</dd><dt>test procedure</dt><dd>A sequence of test cases in execution order, and any associated actions that may be required to set up the initial preconditions and any wrap up activities post execution.</dd><dt>test process</dt><dd>The set of interrelated activities comprising of test planning, test monitoring and control, test analysis, test design, test implementation, test execution, and test completion.</dd><dt>test process group</dt><dd>A collection of specialists who facilitate the definition, maintenance, and improvement of the test processes used by an organization.</dd><dt>test process improvement</dt><dd>A program of activities undertaken to improve the performance and maturity of the organization's test processes.</dd><dt>test process improvement manifesto</dt><dd>A statement that echoes the Agile manifesto, and defines values for improving the test process.</dd><dt>test process improver</dt><dd>A person implementing improvements in the test process based on a test improvement plan.</dd><dt>test progress report</dt><dd>A type of periodic test report that includes the progress of test activities against a baseline, risks, and alternatives requiring a decision.</dd><dt>test pyramid</dt><dd>A graphical model representing the relationship of the amount of testing per level, with more at the bottom than at the top.</dd><dt>test report</dt><dd>Documentation summarizing test activities and results.</dd><dt>test reporting</dt><dd>Collecting and analyzing data from testing activities and subsequently consolidating the data in a report to inform stakeholders.</dd><dt>test result</dt><dd>The consequence/outcome of the execution of a test.</dd><dt>test run</dt><dd>The execution of a test suite on a specific version of the test object.</dd><dt>test scope</dt><dd>A description of the test object and its features to be tested.</dd><dt>test script</dt><dd>A sequence of instructions for the execution of a test.</dd><dt>test selection criteria</dt><dd>The criteria used to guide the generation of test cases or to select test cases in order to limit the size of a test.</dd><dt>test session</dt><dd>An uninterrupted period of time spent in executing tests.</dd><dt>test specification</dt><dd>The complete documentation of the test design, test cases, and test scripts for a specific test item.</dd><dt>test step</dt><dd>A single interaction between an actor and the test object consisting of an input, an action, and an expected result.</dd><dt>test strategy</dt><dd>A description of how to perform testing to reach test objectives under given circumstances.</dd><dt>test suite</dt><dd>A set of test scripts or test procedures to be executed in a specific test run.</dd><dt>test technique</dt><dd>A procedure used to define test conditions, design test cases, and specify test data.</dd><dt>test type</dt><dd>A group of test activities based on specific test objectives aimed at specific characteristics of a component or system.</dd><dt>test-driven development</dt><dd>A software development technique in which the test cases are developed, automated and then the software is developed incrementally to pass those test cases.</dd><dt>test-first approach</dt><dd>An approach to software development in which the test cases are designed and implemented before the associated component or system is developed.</dd><dt>testability</dt><dd>The degree to which test conditions can be established for a component or system, and tests can be performed to determine whether those test conditions have been met.</dd><dt>tester</dt><dd>A person who performs testing.</dd><dt>testing</dt><dd>The process within the software development lifecycle that evaluates the quality of a component or system and related work products.</dd><dt>testing capability</dt><dd>The abilities an organization needs to successfully accomplish its desired test outcomes.</dd><dt>testing in production</dt><dd>A test type performed in the production environment with live data.</dd><dt>testing quadrants</dt><dd>A classification model of test types/test levels in four quadrants, relating them to two dimensions of test objectives: supporting the product team versus critiquing the product, and technology-facing versus business-facing.</dd><dt>testware</dt><dd>Work products produced during the test process for use in planning, designing, executing, evaluating and reporting on testing.</dd><dt>think aloud usability testing</dt><dd>A usability testing technique where test participants share their thoughts with the moderator and observers by thinking aloud while they solve usability test tasks. Think aloud is useful to understand the test participant.</dd><dt>think time</dt><dd>The amount of time required by a user to determine and execute the next action in a sequence of actions.</dd><dt>threshold coverage</dt><dd>The coverage of neurons exceeding a threshold activation value in a neural network for a set of tests.</dd><dt>time behavior</dt><dd>The degree to which a component or system can perform its required functions within required response times, processing times and throughput rates.</dd><dt>Total Quality Management</dt><dd>An organization-wide management approach to quality based on employee participation to achieve long-term success through customer satisfaction.</dd><dt>tour</dt><dd>A set of exploratory tests organized around a special focus.</dd><dt>TPI Next</dt><dd>A continuous business-driven framework for test process improvement that describes the key elements of an effective and efficient test process.</dd><dt>traceability</dt><dd>The ability to establish explicit relationships between related work products or items within work products.</dd><dt>traceability matrix</dt><dd>A two-dimensional table, which correlates two entities (e.g., requirements and test cases). The table allows tracing back and forth the links of one entity to the other, thus enabling the determination of coverage achieved and the assessment of impact of proposed changes.</dd><dt>transcendent quality</dt><dd>A view of quality based on the perception and feeling of individuals.</dd><dt>unit test framework</dt><dd>A tool that provides an environment for unit or component testing in which a component can be tested in isolation or with suitable stubs and drivers. It also provides other support for the developer, such as debugging capabilities.</dd><dt>usability</dt><dd>The degree to which a component or system can be used by specified users to achieve specified goals in a specified context of use.</dd><dt>usability evaluation</dt><dd>A process through which information about the usability of a system is gathered in order to improve the system (known as formative evaluation) or to assess the merit or worth of a system (known as summative evaluation).</dd><dt>usability lab</dt><dd>A test facility in which unintrusive observation of participant reactions and responses to software takes place.</dd><dt>usability requirement</dt><dd>A requirement on the usability of a component or system.</dd><dt>usability test participant</dt><dd>A representative user who solves typical tasks in a usability test.</dd><dt>usability test script</dt><dd>A document specifying a sequence of actions for the execution of a usability test. It is used by the moderator to keep track of briefing and pre-session interview questions, usability test tasks, and post-session interview questions.</dd><dt>usability test session</dt><dd>A test session in usability testing in which a usability test participant is executing tests, moderated by a moderator and observed by a number of observers.</dd><dt>usability test task</dt><dd>A usability test execution activity specified by the moderator that needs to be accomplished by a usability test participant within a given period of time.</dd><dt>usability testing</dt><dd>Testing to evaluate the degree to which the system can be used by specified users with effectiveness, efficiency and satisfaction in a specified context of use.</dd><dt>use case testing</dt><dd>A black-box test technique in which test cases are designed to exercise use case behaviors.</dd><dt>user acceptance testing</dt><dd>A type of acceptance testing performed to determine if intended users accept the system.</dd><dt>user error protection</dt><dd>The degree to which a component or system protects users against making errors.</dd><dt>user experience</dt><dd>A person's perceptions and responses resulting from the use or anticipated use of a software product.</dd><dt>user interface</dt><dd>All components of a system that provide information and controls for the user to accomplish specific tasks with the system.</dd><dt>user interface aesthetics</dt><dd>The degree to which a user interface enables pleasing and satisfying interaction for the user.</dd><dt>user interface guideline</dt><dd>A low-level, specific rule or recommendation for user interface design that leaves little room for interpretation so designers implement it similarly. It is often used to ensure consistency in the appearance and behavior of the user interface of the systems produced by an organization.</dd><dt>user story</dt><dd>A user or business requirement consisting of one sentence expressed in the everyday or business language which is capturing the functionality a user needs, the reason behind it, any non-functional criteria, and also including acceptance criteria.</dd><dt>user story testing</dt><dd>A black-box test technique in which test conditions are the acceptance criteria of user stories.</dd><dt>user survey</dt><dd>A usability evaluation whereby a representative sample of users are asked to report subjective evaluation into a questionnaire based on their experience in using a component or system.</dd><dt>user-agent based testing</dt><dd>A type of testing in which a test client is used to switch the user agent string and identify itself as a different client while executing test suites.</dd><dt>user-based quality</dt><dd>A view of quality measured by the degree that the needs, wants, and desires of a user are met.</dd><dt>V-model</dt><dd>A sequential software development lifecycle model describing a one-for-one relationship between major phases of software development from business requirements specification to delivery, and corresponding test levels from acceptance testing to component testing.</dd><dt>validation</dt><dd>Confirmation by examination that a work product matches a stakeholder's needs.</dd><dt>value change coverage</dt><dd>The coverage of neurons activated where their activation values differ by more than a change amount in the neural network for a set of tests.</dd><dt>value-based quality</dt><dd>A view of quality measured by the ratio of the cost to the value received from a product or service.</dd><dt>verification</dt><dd>Confirmation by examination and through provision of objective evidence that specified requirements have been fulfilled.</dd><dt>virtual test environment</dt><dd>A test environment in which one or more parts are digitally simulated.</dd><dt>virtual user</dt><dd>A simulation of activities performed according to a user operational profile.</dd><dt>visual testing</dt><dd>Testing that uses image recognition to interact with GUI objects.</dd><dt>vulnerability scanner</dt><dd>A static analyzer that is used to detect particular security vulnerabilities in the code.</dd><dt>walkthrough</dt><dd>A type of review in which an author leads members of the review through a work product and the members ask questions and make comments about possible issues.</dd><dt>Web Content Accessibility Guidelines</dt><dd>A part of a series of web accessibility guidelines published by the Web Accessibility Initiative (WAI) of the World Wide Web Consortium (W3C), the main international standards organization for the internet. They consist of a set of guidelines for making content accessible, primarily for people with disabilities.</dd><dt>Website Analysis and Measurement Inventory</dt><dd>A commercial website analysis service providing a questionnaire for measuring user experience and assessing delivery of business goals online.</dd><dt>white-box test technique</dt><dd>A test technique only based on the internal structure of a component or system.</dd><dt>white-box testing</dt><dd>Testing based on an analysis of the internal structure of the component or system.</dd><dt>Wideband Delphi</dt><dd>An expert-based test estimation technique that aims at making an accurate estimation using the collective wisdom of the team members.</dd><dt>wild pointer</dt><dd>A pointer that references a location that is out of scope for that pointer or that does not exist.</dd><dt>XiL test environment</dt><dd>A generalized term for dynamic testing in different virtual test environments.</dd>
</dl>

</body>
</html>

<template>
	// extraction template
	var result = '';
	var dictionary = {
		terms: []
	};

	document.querySelectorAll('h3').forEach(function(item) {
		const block = {
			term: item.innerText,
			description: item.nextSibling.innerText
		};
		dictionary.terms.push(block);
	})

	dictionary.terms.forEach(function(item) {
		result += `<dt>${item['term']}</dt><dd>${item['description']}</dd><br>`;
	})
</template>