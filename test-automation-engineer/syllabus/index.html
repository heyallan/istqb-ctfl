<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Syllabus - Test Automation Engineer (CT-TAE)</title>
	<meta name="description" content="This syllabus contains the body of knowledge for ISTQB Test Automation Engineer (CT-TAE)">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' width='48' height='48' viewBox='0 0 16 16'><text x='0' y='14'>ðŸ“•</text></svg>" />
	<link rel="stylesheet" href="/istqb-ctfl/css/reset.min.css">
	<link rel="stylesheet" href="/istqb-ctfl/css/grid.min.css"/>
	<link rel="stylesheet" href="/istqb-ctfl/css/properties.min.css">
	<link rel="stylesheet" href="/istqb-ctfl/css/styles.css">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;700&display=swap" rel="stylesheet">
	<script defer src="/istqb-ctfl/functions.js"></script>
	<script defer src="/istqb-ctfl/index.js"></script>
</head>
<body>

<ui-navbar></ui-navbar>

<div class="container printable padding-top-2">
	<div class="flex gap-2">
		<div class="width-4-md">
			<p class="text-uppercase font-size-small font-weight-700">Table of contents</p>
			<a class="display-block margin-bottom-0.5" href="#test-automation-engineer">Test Automation Engineer</a>
			<a class="display-block margin-bottom-0.5" href="#acronyms">Acronyms</a>
			<a class="display-block margin-bottom-0.5" href="#1-introduction-and-objectives-for-test-automation">1. Introduction and Objectives for Test Automation</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#1-1-purpose-of-test-automation">1.1 Purpose of Test Automation</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#1-2-success-factors-in-test-automation">1.2 Success Factors in Test Automation</a>
			<a class="display-block margin-bottom-0.5" href="#2-preparing-for-test-automation">2. Preparing for Test Automation</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#2-1-sut-factors-influencing-test-automation">2.1 <abbr title="System Under Test">SUT</abbr> Factors Influencing Test Automation</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#2-2-tool-evaluation-and-selection">2.2 Tool Evaluation and Selection</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#2-3-design-for-testability-and-automation">2.3 Design for Testability and Automation</a>
			<a class="display-block margin-bottom-0.5" href="#3-the-generic-test-automation-architecture">3. The Generic Test Automation Architecture</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-introduction-to-gtaa">3.1 Introduction to <abbr title="Generic Test Automation Architecture">gTAA</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-1-overview-of-the-gtaa">3.1.1 Overview of the <abbr title="Generic Test Automation Architecture">gTAA</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-2-test-generation-layer">3.1.2 Test Generation Layer</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-3-test-definition-layer">3.1.3 Test Definition Layer</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-4-test-execution-layer">3.1.4 Test Execution Layer</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-5-test-adaptation-layer">3.1.5 Test Adaptation Layer</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-6-configuration-management-of-a-tas">3.1.6 Configuration Management of a <abbr title="Test Automation Solution">TAS</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-7-project-management-of-a-tas">3.1.7 Project Management of a <abbr title="Test Automation Solution">TAS</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-1-8-tas-support-for-test-management">3.1.8 <abbr title="Test Automation Solution">TAS</abbr> Support for Test Management</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-2-taa-design">3.2 <abbr title="Test Automation Architecture">TAA</abbr> Design</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-2-1-introduction-to-taa-design">3.2.1 Introduction to <abbr title="Test Automation Architecture">TAA</abbr> Design</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-2-2-approaches-for-automating-test-cases">3.2.2 Approaches for Automating Test Cases</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-2-3-technical-considerations-of-the-sut">3.2.3 Technical considerations of the <abbr title="System Under Test">SUT</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-2-4-considerations-for-development/qa-processes">3.2.4 Considerations for Development/QA Processes</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-3-tas-development">3.3 <abbr title="Test Automation Solution">TAS</abbr> Development</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-3-1-introduction-to-tas-development">3.3.1 Introduction to <abbr title="Test Automation Solution">TAS</abbr> Development</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-3-2-compatibility-between-the-tas-and-the-sut">3.3.2 Compatibility between the <abbr title="Test Automation Solution">TAS</abbr> and the <abbr title="System Under Test">SUT</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-3-3-synchronization-between-tas-and-sut">3.3.3 Synchronization between <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-3-4-building-reuse-into-the-tas">3.3.4 Building Reuse into the <abbr title="Test Automation Solution">TAS</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#3-3-5-support-for-a-variety-of-target-systems">3.3.5 Support for a Variety of Target Systems</a>
			<a class="display-block margin-bottom-0.5" href="#4-deployment-risks-and-contingencies">4 Deployment Risks and Contingencies</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#4-1-selection-of-test-automation-approach-and-planning-of-deployment/rollout">4.1 Selection of Test Automation Approach and Planning of Deployment/Rollout</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#4-1-1-pilot-project">4.1.1 Pilot Project</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#4-1-2-deployment">4.1.2 Deployment</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#4-1-3-deployment-of-the-tas-within-the-software-lifecycle">4.1.3 Deployment of the <abbr title="Test Automation Solution">TAS</abbr> Within the Software Lifecycle</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#4-2-risk-assessment-and-mitigation-strategies">4.2 Risk Assessment and Mitigation Strategies</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#4-3-test-automation-maintenance">4.3 Test Automation Maintenance</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#4-3-1-types-of-maintenance">4.3.1 Types of Maintenance</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#4-3-2-scope-and-approach">4.3.2 Scope and Approach</a>
			<a class="display-block margin-bottom-0.5" href="#5-test-automation-reporting-and-metrics">5 Test Automation Reporting and Metrics</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#5-1-selection-of-tas-metrics">5.1 Selection of <abbr title="Test Automation Solution">TAS</abbr> Metrics</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#5-2-implementation-of-measurement">5.2 Implementation of Measurement</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#5-3-logging-of-the-tas-and-the-sut">5.3 Logging of the <abbr title="Test Automation Solution">TAS</abbr> and the <abbr title="System Under Test">SUT</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#5-4-test-automation-reporting">5.4 Test Automation Reporting</a>
			<a class="display-block margin-bottom-0.5" href="#6-transitioning-manual-testing-to-an-automated-environment">6 Transitioning Manual Testing to an Automated Environment</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#6-1-criteria-for-automation">6.1 Criteria for Automation</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#6-2-identify-steps-needed-to-implement-automation-within-regression-testing">6.2 Identify Steps Needed to Implement Automation within Regression Testing</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#6-3-factors-to-consider-when-implementing-automation-within-new-feature-testing">6.3 Factors to Consider when Implementing Automation within New Feature Testing</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#6-4-factors-to-consider-when-implementing-automation-of-confirmation-testing">6.4 Factors to Consider when Implementing Automation of Confirmation Testing</a>
			<a class="display-block margin-bottom-0.5" href="#7-verifying-the-tas">7 Verifying the <abbr title="Test Automation Solution">TAS</abbr></a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#7-1-verifying-automated-test-environment-components">7.1 Verifying Automated Test Environment Components</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#7-2-verifying-the-automated-test-suite">7.2 Verifying the Automated Test Suite</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#8-continuous-improvement">8 Continuous Improvement</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#8-1-options-for-improving-test-automation">8.1 Options for Improving Test Automation</a>
			<a class="display-block margin-bottom-0.5 margin-left-1.5" href="#8-2-planning-the-implementation-of-test-automation-improvement">8.2 Planning the Implementation of Test Automation Improvement</a>
		</div><!--/column-->

		<!--column-->
		<div class="width-8-md syllabus">
			<article id="test-automation-engineer">
				<h1><span class="underscore">Syllabus - Test Automation Engineer (CT-TAE)</span></h1>
				<p class="font-size-h2 font-family-heading font-weight-400 color-primary">This syllabus contains the body of knowledge for ISTQB Test Automation Engineer (CT-TAE)</p>
				<div class="no-print margin-bottom-1 stat-name padding-x-0.5 padding-y-0.5 font-weight-700 font-size-x-small text-uppercase bg-primary-highlight">
					This document contains extracts from the original syllabus. Published here for study purposes. Non-relevant sections have been omitted. To read the full document visit the ISTQBÂ® website.
				</div>
				<p>The Test Automation Engineer certification is focused on the design, development, and maintenance of test automation solutions, and also covers the automation of dynamic functional tests.</p>
			</article>

			<article id="acronyms">
				<h1><a class="color-inherit" href="#acronyms">Acronyms</a></h1>
				<dl class="font-size-small">
					<dt><abbr title="Command Line Interface">CLI</abbr></dt>
					<dd>Command Line Interface</dd>
					<dt><abbr title="Equivalent manual test effor">EMTE</abbr></dt>
					<dd>Equivalent Manual Test Effort</dd>
					<dt><abbr title="Generic Test Automation Architecture">gTAA</abbr></dt>
					<dd>Generic Test Automation Architecture (providing a blueprint for test automation solutions)</dd>
					<dt><abbr title="Graphical User Interface">GUI</abbr></dt>
					<dd>Graphical User Interface</dd>
					<dt><abbr title="System Under Test">SUT</abbr></dt>
					<dd>system under test, see also test object</dd>
					<dt><abbr title="Test Automation Architecture">TAA</abbr></dt>
					<dd>Test Automation Architecture (an instantiation of <abbr title="Generic Test Automation Architecture">gTAA</abbr> to define the architecture of a <abbr title="Test Automation Solution">TAS</abbr>)</dd>
					<dt><abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr></dt>
					<dd>Test Automation Engineer (the person who is responsible for the design of a <abbr title="Test Automation Architecture">TAA</abbr>, including the implementation of the resulting <abbr title="Test Automation Solution">TAS</abbr>, its maintenance and technical evolution)</dd>
					<dt><abbr title="Test Automation Framework">TAF</abbr></dt>
					<dd>Test Automation Framework (the environment required for test automation including test harnesses and artifacts such as test libraries)</dd>
					<dt><abbr title="Test Automation Manager">TAM</abbr></dt>
					<dd>Test Automation Manager (the person responsible for the planning and supervision of the development and evolution of a <abbr title="Test Automation Solution">TAS</abbr>)</dd>
					<dt><abbr title="Test Automation Solution">TAS</abbr></dt>
					<dd>Test Automation Solution (the realization/implementation of a <abbr title="Test Automation Architecture">TAA</abbr>, including test harnesses and artifacts such as test libraries)</dd>
					<dt><abbr title="User Interface">UI</abbr></dt>
					<dd>User Interface</dd>
				</dl>
			</article>

			<article id="1-introduction-and-objectives-for-test-automation">
				<h1><a class="color-inherit" href="#1-introduction-and-objectives-for-test-automation">1. Introduction and Objectives for Test Automation</a></h1>
				<h2 id="1-1-purpose-of-test-automation"><a class="color-inherit" href="#1-1-purpose-of-test-automation">1.1 Purpose of Test Automation</a></h2>
				<p class="learning-objective font-size-small color-secondary">ALTA-E-1.1.1 (K2) Explain the objectives, advantages, disadvantages and limitations of test automation</p>
				<p>In software testing, test automation (which includes automated test execution) is one or more of thefollowing tasks:</p>
				<ul>
					<li>Using purpose built software tools to control and set up test preconditions</li>
					<li>Executing tests</li>
					<li>Comparing actual outcomes to predicted outcomes</li>
				</ul>
				<p>A good practice is to separate the software used for testing from the system under test (<abbr title="System Under Test">SUT</abbr>) itself tominimize interference. There are exceptions, for example embedded systems where the test softwareneeds to be deployed to the <abbr title="System Under Test">SUT</abbr>.</p>
				<p>Test automation is expected to help run many test cases consistently and repeatedly on different versionsof the <abbr title="System Under Test">SUT</abbr> and/or environments. But test automation is more than a mechanism for running a test suitewithout human interaction. It involves a process of designing the testware, including:</p>
				<ul>
					<li>Software</li>
					<li>Documentation</li>
					<li>Test cases</li>
					<li>Test environments</li>
					<li>Test data</li>
				</ul>
				<p>Testware is necessary for the testing activities that include:</p>
				<ul>
					<li>Implementing automated test cases</li>
					<li>Monitoring and controlling the execution of automated tests</li>
					<li>Interpreting, reporting and logging the automated test results</li>
				</ul>
				<p>Test automation has different approaches for interacting with a <abbr title="System Under Test">SUT</abbr>:</p>
				<ul>
					<li>Testing through the public interfaces to classes, modules or libraries of the <abbr title="System Under Test">SUT</abbr> (API testing)</li>
					<li>Testing through the user interface of the <abbr title="System Under Test">SUT</abbr> (e.g., <abbr title="Graphical User Interface">GUI</abbr> testing or <abbr title="Command Line Interface">CLI</abbr> testing)</li>
					<li>Testing through a service or protocol</li>
				</ul>
				<p>Objectives of test automation include:</p>
				<ul>
					<li>Improving test efficiency</li>
					<li>Providing wider function coverage</li>
					<li>Reducing the total test cost</li>
					<li>Performing tests that manual testers cannot</li>
					<li>Shortening the test execution period</li>
					<li>Increasing the test frequency/reducing the time required for test cycles</li>
				</ul>
				<p>Advantages of test automation include:</p>
				<ul>
					<li>More tests can be run per build</li>
					<li>The possibility to create tests that cannot be done manually (real-time, remote, parallel tests)</li>
					<li>Tests can be more complex</li>
					<li>Tests run faster</li>
					<li>Tests are less subject to operator error</li>
					<li>More effective and efficient use of testing resources</li>
					<li>Quicker feedback regarding software quality</li>
					<li>Improved system reliability (e.g., repeatability, consistency)</li>
					<li>Improved consistency of tests</li>
				</ul>
				<p>Disadvantages of test automation include:</p>
				<ul>
					<li>Additional costs are involved</li>
					<li>Initial investment to setup <abbr title="Test Automation Solution">TAS</abbr></li>
					<li>Requires additional technologies</li>
					<li>Team needs to have development and automation skills</li>
					<li>On-going <abbr title="Test Automation Solution">TAS</abbr> maintenance requirement</li>
					<li>Can distract from testing objectives, e.g., focusing on automating tests cases at the expense of executing tests</li>
					<li>Tests can become more complex</li>
					<li>Additional errors may be introduced by automation</li>
				</ul>
				<p>Limitations of test automation include:</p>
				<ul>
					<li>Not all manual tests can be automated</li>
					<li>The automation can only check machine-interpretable results</li>
					<li>The automation can only check actual results that can be verified by an automated test oracle</li>
					<li>Not a replacement for exploratory testing</li>
				</ul>
				<h2 id="1-2-success-factors-in-test-automation"><a class="color-inherit" href="#1-2-success-factors-in-test-automation">1.2 Success Factors in Test Automation</a></h2>
				<p class="learning-objective font-size-small color-secondary">ALTA-E-1.2.1 (K2) Identify technical success factors of a test automation project</p>
				<p>The following success factors apply to test automation projects that are in operation and therefore the focus is on influences that impact on the long term success of the project. Factors influencing the success of test automation projects at the pilot stage are not considered here.</p>
				<p>Major success factors for test automation include the following:</p>
				<p><strong>Test Automation Architecture (<abbr title="Test Automation Architecture">TAA</abbr>)</strong></p>
				<p>The Test Automation Architecture (<abbr title="Test Automation Architecture">TAA</abbr>) is very closely aligned with the architecture of a software product. It should be clear which functional and non-functional requirements the architecture is to support. Typically this will be the most important requirements.</p>
				<p>Often <abbr title="Test Automation Architecture">TAA</abbr> is designed for maintainability, performance and learnability. (See ISO/IEC 25000:2014 for details of these and other non-functional characteristics.) It is helpful to involve software engineers who understand the architecture of the <abbr title="System Under Test">SUT</abbr>.</p>
				<p><strong><abbr title="System Under Test">SUT</abbr> Testability</strong></p>
				<p>The <abbr title="System Under Test">SUT</abbr> needs to be designed for testability that supports automated testing. In the case of <abbr title="Graphical User Interface">GUI</abbr> testing, this could mean that the <abbr title="System Under Test">SUT</abbr> should decouple as much as possible the <abbr title="Graphical User Interface">GUI</abbr> interaction and data from the appearance of the graphical interface. In the case of API testing, this could mean that more classes, modules or the command-line interface need to be exposed as public so that they can be tested.</p>
				<p>The testable parts of the <abbr title="System Under Test">SUT</abbr> should be targeted first. Generally, a key factor in the success of test automation lies in the ease of implementing automated test scripts. With this goal in mind, and also to provide a successful proof of concept, the Test Automation Engineer (<abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr>) needs to identify modules or components of the <abbr title="System Under Test">SUT</abbr> that are easily tested with automation and start from there.</p>
				<p><strong>Test Automation Strategy</strong></p>
				<p>A practical and consistent test automation strategy that addresses maintainability and consistency of the <abbr title="System Under Test">SUT</abbr>.</p>
				<p>It may not be possible to apply the test automation strategy in the same way to both old and new parts of the <abbr title="System Under Test">SUT</abbr>. When creating the automation strategy, consider the costs, benefits and risks of applying it to different parts of the code</p>
				<p>Consideration should be given to testing both the user interface and the API with automated test cases to check the consistency of the results.</p>
				<p><strong>Test Automation Framework (<abbr title="Test Automation Framework">TAF</abbr>)</strong></p>
				<p>A test automation framework (<abbr title="Test Automation Framework">TAF</abbr>) that is easy to use, well documented and maintainable, supports a consistent approach to automating tests.</p>
				<p>In order to establish an easy to use and maintainable <abbr title="Test Automation Framework">TAF</abbr>, the following must be done:</p>
				<ul>
					<li>Implement reporting facilities: The test reports should provide information (pass/fail/error/not run/aborted, statistical, etc.) about the quality of the <abbr title="System Under Test">SUT</abbr>. Reporting should provide the information for the involved testers, test managers, developers, project managers and other stakeholders to obtain an overview of the quality.</li>
					<li>
						Enable easy troubleshooting: In addition to the test execution and logging, the <abbr title="Test Automation Framework">TAF</abbr> has to provide an easy way to troubleshoot failing tests. The test can fail due to
						<ul>
							<li>failures found in the <abbr title="System Under Test">SUT</abbr></li>
							<li>failures found in the <abbr title="Test Automation Solution">TAS</abbr></li>
							<li>problem with the tests themselves or the test environment.</li>
						</ul>
					</li>
					<li>Address the test environment appropriately: Test tools are dependent upon consistency in the test environment. Having a dedicated test environment is necessary in automated testing. If there is no control of the test environment and test data, the setup for tests may not meet the requirements for test execution and it is likely to produce false execution results.</li>
					<li>Document the automated test cases: The goals for test automation have to be clear, e.g., which parts of application are to be tested, to what degree, and which attributes are to be tested (functional and non-functional). This must be clearly described and documented.</li>
					<li>Trace the automated test: <abbr title="Test Automation Framework">TAF</abbr> shall support tracing for the test automation engineer to trace individual steps to test cases.</li>
					<li>Enable easy maintenance: Ideally, the automated test cases should be easily maintained so that maintenance will not consume a significant part of the test automation effort. In addition, the maintenance effort needs to be in proportion to the scale of the changes made to the <abbr title="System Under Test">SUT</abbr>. To do this, the cases must be easily analyzable, changeable and expandable. Furthermore, automated testware reuse should be high to minimize the number of items requiring changes.</li>
					<li>Keep the automated tests up-to-date: when new or changed requirements cause tests or entire test suites to fail, do not disable the failed tests â€“ fix them.</li>
					<li>Plan for deployment: Make sure that test scripts can be easily deployed, changed and redeployed.</li>
					<li>Retire tests as needed: Make sure that automated test scripts can be easily retired if they are no longer useful or necessary.</li>
					<li>Monitor and restore the <abbr title="System Under Test">SUT</abbr>: In real practice, to continuously run a test case or set of test cases, the <abbr title="System Under Test">SUT</abbr> must be monitored continuously. If the <abbr title="System Under Test">SUT</abbr> encounters a fatal error (such as a crash), the <abbr title="Test Automation Framework">TAF</abbr> must have the capability to recover, skip the current case, and resume testing with the next case.</li>
				</ul>
				<p>The test automation code can be complex to maintain. It is not unusual to have as much code for testing as the code for the <abbr title="System Under Test">SUT</abbr>. This is why it is of utmost importance that the test code be maintainable. This is due to the different test tools being used, the different types of verification that are used and the different testware artifacts that have to be maintained (such as test input data, test oracles, test reports).</p>
				<p>With these maintenance considerations in mind, in addition to the important items that should be done, there are a few that should not be done, as follows:</p>
				<ul>
					<li>Do not create code that is sensitive to the interface (i.e., it would be affected by changes in the graphical interface or in non-essential parts of the API).</li>
					<li>Do not create test automation that is sensitive to data changes or has a high dependency on particular data values (e.g., test input depending on other test outputs).</li>
					<li>Do not create an automation environment that is sensitive to the context (e.g., operating system date and time, operating system localization parameters or the contents of another application). In this case, it is better to use test stubs as necessary so the environment can be controlled.</li>
				</ul>
				<p>The more success factors that are met, the more likely the test automation project will succeed. Not all factors are required, and in practice rarely are all factors met. Before starting the test automation project, it is important to analyze the chance of success for the project by considering the factors in place and the factors missing keeping risks of the chosen approach in mind as well as project context. Once the <abbr title="Test Automation Architecture">TAA</abbr> is in place, it is important to investigate which items are missing or still need work.</p>
			</article>

			<article id="2-preparing-for-test-automation">
				<h1><a class="color-inherit" href="#2-preparing-for-test-automation">2. Preparing for Test Automation</a></h1>
				<h2 id="2-1-sut-factors-influencing-test-automation"><a class="color-inherit" href="#2-1-sut-factors-influencing-test-automation">2.1 <abbr title="System Under Test">SUT</abbr> Factors Influencing Test Automation</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-2.1.1 (K4) Analyze a system under test to determine the appropriate automation solution</p>
				<p>When evaluating the context of the <abbr title="System Under Test">SUT</abbr> and its environment, factors that influence test automation need to be identified to determine an appropriate solution. These may include the following:</p>
				<ul>
					<li><strong><abbr title="System Under Test">SUT</abbr> interfaces:</strong> The automated test cases invoke actions on the <abbr title="System Under Test">SUT</abbr>. For this, the <abbr title="System Under Test">SUT</abbr> must provide interfaces via which the <abbr title="System Under Test">SUT</abbr> can be controlled. This can be done via <abbr title="User Interface">UI</abbr> controls, but also via lower-level software interfaces. In addition, some test cases may be able to interface at the communication level (e.g., using TCP/IP, USB, or proprietary messaging interfaces). The decomposition of the <abbr title="System Under Test">SUT</abbr> allows the test automation to interface with the <abbr title="System Under Test">SUT</abbr> on different test levels. It is possible to automate the tests on a specific level (e.g., component and system level), but only when the <abbr title="System Under Test">SUT</abbr> supports this adequately. For example, at the component level, there may be no user interface that can be used for testing, so different, possibly customized, software interfaces (also called test hooks) need to be available.</li>
					<li><strong>Third party software:</strong> Often the <abbr title="System Under Test">SUT</abbr> not only consists of software written in the home organization but may also include software provided by third parties. In some contexts, this third party software may need testing, and if test automation is justified, it may need a different test automation solution, such as using an API.</li>
					<li><strong>Levels of intrusion:</strong> Different test automation approaches (using different tools) have different levels of intrusion. The greater the number of changes that are required to be made to the <abbr title="System Under Test">SUT</abbr> specifically for automated testing, the higher the level of intrusion. Using dedicated software interfaces requires a high level of intrusion whereas using existing <abbr title="User Interface">UI</abbr> elements has a lower level of intrusion. Using hardware elements of the <abbr title="System Under Test">SUT</abbr> (such as keyboards, hand-switches, touchscreens, communication interfaces) have an even higher level of intrusion. The problem with higher levels of intrusion is the risk for false alarms. The <abbr title="Test Automation Solution">TAS</abbr> can exhibit failures that may be due to the level of intrusion imposed by the tests, but these are not likely to happen when the software system is being used in a real live environment. Testing with a high level of intrusion is usually a simpler solution for the test automation approach.</li>
					<li><strong>Different <abbr title="System Under Test">SUT</abbr>:</strong> architectures Different <abbr title="System Under Test">SUT</abbr> architectures may require different test automation solutions. A different approach is needed for an <abbr title="System Under Test">SUT</abbr> written in C++ using COM technology than for an <abbr title="System Under Test">SUT</abbr> written in Python. It may be possible for these different architectures to be handled by the same test automation strategy, but that requires a hybrid strategy with the ability to support them.</li>
					<li><strong>Size and complexity of the <abbr title="System Under Test">SUT</abbr>:</strong> Consider the size and complexity of the current <abbr title="System Under Test">SUT</abbr> and plans for future development. For a small and simple <abbr title="System Under Test">SUT</abbr>, a complex and ultra-flexible test automation approach may not be warranted. A simple approach may be better suited. Conversely, it may not be wise to implement a small and simple approach for a very large and complex <abbr title="System Under Test">SUT</abbr>. At times though, it is appropriate to start small and simple even for a complex <abbr title="System Under Test">SUT</abbr> but this should be a temporary approach (see Chapter 3 for more details).</li>
				</ul>
            <p>Several factors described here are known (e.g., size and complexity, available software interfaces) when the <abbr title="System Under Test">SUT</abbr> is already available, but most of the time the development of the test automation should start before the <abbr title="System Under Test">SUT</abbr> is available. When this happens several things need to be estimated or the <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> can specify the software interfaces that are needed. (see Section 2.3 for more details).</p>
            <p>Even when the <abbr title="System Under Test">SUT</abbr> does not yet exist, test automation planning can start. For example:</p>
            <ul>
                <li>When the requirements (functional or non-functional) are known, candidates for automation can be selected from those requirements together with identifying the means to test them. Planning for automation can begin for those candidates, including identifying the requirements for the automation and determining the test automation strategy.</li>
                <li>When the architecture and technical design is being developed, the design of software interfaces to support testing can be undertaken.</li>
            </ul>
				<h2 id="2-2-tool-evaluation-and-selection"><a class="color-inherit" href="#2-2-tool-evaluation-and-selection">2.2 Tool Evaluation and Selection</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-2.2.1 (K4) Analyze test automation tools for a given project and report technical findings and recommendations</p>
            <p>The primary responsibility for the tool selection and evaluation process belongs with the Test Automation Manager (<abbr title="Test Automation Manager">TAM</abbr>). However the <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> will be involved in supplying information to the <abbr title="Test Automation Manager">TAM</abbr> and conducting many of the evaluation and selection activities. The concept of the tool evaluation and selection process was introduced at the Foundation Level and more details of this process are described in the Advanced Level â€“ Test Manager Syllabus [ISTQB-AL-TM].</p>
            <p>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> will be involved throughout the tool evaluation and selection process but will have particular contributions to make to the following activities:</p>
            <ul>
                <li>Assessing organizational maturity and identification of opportunities for test tool support</li>
                <li>Assessing appropriate objectives for test tool support</li>
                <li>Identifying and collecting information on potentially suitable tools</li>
                <li>Analyzing tool information against objectives and project constraints</li>
                <li>Estimating the cost-benefit ratio based on a solid business case</li>
                <li>Making a recommendation on the appropriate tool</li>
                <li>Identifying compatibility of the tool with <abbr title="System Under Test">SUT</abbr> components</li>
            </ul>
            <p>Functional test automation tools frequently cannot meet all the expectations or the situations that are encountered by an automation project. The following is a set of examples of these types of issues (but it is definitely not a complete list):</p>
            <table>
                <thead>
                    <tr>
                        <th>Finding</th>
                        <th>Examples</th>
                        <th>Possible Solutions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>The toolâ€™s interface does not work with other tools that are already in place</td>
                        <td>
								<ul class="padding-left-1">
									<li>The test management tool has been updated and the connecting interface has changed</li>
									<li>The information from pre-sales support was wrong and not all data can be transferred to the reporting too</li>
								</ul>
							</td>
                        <td>
								<ul class="padding-left-1">
									<li>Pay attention to the release notes before any updates, and for big migrations test before migrating to production</li>
        						<li>Try to gain an onsite demonstration of the tool that uses the real <abbr title="System Under Test">SUT</abbr></li>
        						<li>Seek support from the vendor and/or user community forums</li>
								</ul>
							</td>
                    </tr>
						<tr>
							<td>
								Some <abbr title="System Under Test">SUT</abbr> dependencies are changed to ones not supported by the test tool
							</td>
							<td>
								<ul class="padding-left-1"><li>The development department has updated to the newest version of Java</li></ul>
							</td>
							<td>
								<ul class="padding-left-1"><li>Synchronize upgrades for development/test environment and the test automation tool</li></ul>
							</td>
						</tr>
						<tr>
							<td>
								Object on <abbr title="Graphical User Interface">GUI</abbr> could not be captured
							</td>
							<td>
								<ul class="padding-left-1"><li>The object is visible but the test automation tool cannot interact with it</li></ul>
							</td>
							<td>
								<ul class="padding-left-1">
									<li>Try to use only well-known technologies or objects in development</li>
									<li>Do a pilot project before buying a test automation tool</li>
									<li>Have developers define standards for objects</li>
								</ul>
							</td>
						</tr>
						<tr>
							<td>
								Tool looks very complicated
							</td>
							<td>
								<ul class="padding-left-1">
									<li>The tool has a huge feature set but only part of that will be used</li>
								</ul>
							</td>
							<td>
								<ul class="padding-left-1">
									<li>Try to find a way to limit the feature set by removing unwanted features from the tool bar</li>
									<li>Select a license to meet your needs.</li>
									<li>Try to find alternative tools that are more focused on the required functionality.</li>
								</ul>
							</td>
						</tr>
						<tr>
							<td>Conflict with other systems</td>
							<td><ul class="padding-left-1"><li>After installation of other software the test automation tool will not work anymore or vice versa</li></ul></td>
							<td><ul class="padding-left-1">
								<li>Read the release notes or technical requirements before installing.</li>
								<li>Get confirmation from the supplier that there will be no impact to other tools.</li>
								<li>Question user community forums</li>
							</ul></td>
						</tr>
						<tr>
							<td>Impact on the <abbr title="System Under Test">SUT</abbr></td>
							<td><ul class="padding-left-1"><li>During/after use of the test automation tool the <abbr title="System Under Test">SUT</abbr> is reacting differently (e.g., longer response time)</li></ul></td>
							<td><ul class="padding-left-1"><li>Use a tool that will not need to change the <abbr title="System Under Test">SUT</abbr> (e.g., installation of libraries, etc.)</li></ul></td>
						</tr>
						<tr>
							<td>Access to code</td>
							<td><ul class="padding-left-1"><li>The test automation tool will change parts of the source code</li></ul></td>
							<td><ul class="padding-left-1"><li>Use a tool that will not need to change the source code (e.g., installation of libraries, etc.)</li></ul></td>
						</tr>
						<tr>
							<td>Limited resources (mainly in embedded environments)</td>
							<td><ul class="padding-left-1"><li>The test environment has limited free resources or runs out of resources (e.g., memory)</li></ul></td>
							<td><ul class="padding-left-1">
								<li>Read release notes and discuss the environment with the tool provider to get confirmation that this will not lead to problems.</li>
								<li>Question user community forums.</li>
							</ul></td>
						</tr>
						<tr>
							<td>Updates</td>
							<td><ul class="padding-left-1">
								<li>Update will not migrate all data or corrupts existing automated test scripts, data or configurations</li>
								<li>Upgrade needs a different (better) environment</li>
							</ul></td>
							<td><ul class="padding-left-1">
								<li>Test upgrade on the test environment and get confirmation from the provider that migration will work</li>
								<li>Read update prerequisites and decide if the update is worth the effort</li>
								<li>Seek support from the user community forums</li>
							</ul></td>
						</tr>
						<tr>
							<td>Security</td>
							<td><ul class="padding-left-1"><li>Test automation tool requires information that is not available to the test automation engineer</li></ul></td>
							<td><ul class="padding-left-1"><li>Test automation engineer needs to be granted access</li></ul></td>
						</tr>
						<tr>
							<td>Incompatibility between different environments and platforms</td>
							<td><ul class="padding-left-1"><li>Test automation does not work on all environments/platforms</li></ul></td>
							<td><ul class="padding-left-1"><li>Implement automated tests to maximize tool independence thereby minimizing the cost of using multiple tools.</li></ul></td>
						</tr>
                </tbody>
            </table>
				<h2 id="2-3-design-for-testability-and-automation"><a class="color-inherit" href="#2-3-design-for-testability-and-automation">2.3 Design for Testability and Automation</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-2.3.1 (K2) Understand "design for testability" and "design for test automation" methods applicable to the <abbr title="System Under Test">SUT</abbr></p>
				<p><abbr title="System Under Test">SUT</abbr> testability (availability of software interfaces that support testing e.g., to enable control and observability of the <abbr title="System Under Test">SUT</abbr>) should be designed and implemented in parallel with the design and implementation of the other features of the <abbr title="System Under Test">SUT</abbr>. This can be done by the software architect (as testability is just one of the non-functional requirements of the system), but often this is done by, or with the involvement of, a <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr>.</p>
				<p>Design for testability consists of several parts:</p>
				<ul>
					<li>Observability: The <abbr title="System Under Test">SUT</abbr> needs to provide interfaces that give insight into the system. Test cases can then use these interfaces to check, for example, whether the expected behavior equals the actual behavior.</li>
					<li>Control(ability): The <abbr title="System Under Test">SUT</abbr> needs to provide interfaces that can be used to perform actions on the <abbr title="System Under Test">SUT</abbr>. This can be <abbr title="User Interface">UI</abbr> elements, function calls, communication elements (e.g., TCP/IP or USB protocol), electronic signals (for physical switches), etc.</li>
					<li>Clearly defined architecture: The third important part of design for testability is an architecture that provides clear and understandable interfaces giving control and visibility on all test levels.</li>
				</ul>
				<p>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> considers ways in which the <abbr title="System Under Test">SUT</abbr> can be tested, including automated testing, in an effective (testing the right areas and finding critical bugs) and efficient (without taking too much effort) way. Whenever specific software interfaces are needed, they must be specified by the <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> and implemented by the developer. It is important to define testability and, if needed, additional software interfaces early in the project, so that development work can be planned and budgeted.</p>
				<p>Some examples of software interfaces that support testing include:</p>
				<ul>
					<li>The powerful scripting capabilities of modern spreadsheets.</li>
					<li>Applying stubs or mocks to simulate software and/or hardware (e.g., electronic financial transactions, software service, dedicated server, electronic board, mechanical part) that is not yet available or is too expensive to buy, allows testing of the software in the absence of that specific interface.</li>
					<li>Software interfaces (or stubs and drivers) can be used to test error conditions. Consider a device with an internal hard disk drive (HDD). The software controlling this HDD (called a driver) should be tested for failures or wear of the HDD. Doing this by waiting for a HDD to fail is not very efficient (or reliable). Implementing software interfaces that simulate defective or slow HDDs can verify that the driver software performs correctly (e.g., provides an error message, retries).</li>
					<li>Alternative software interfaces can be used to test an <abbr title="System Under Test">SUT</abbr> when no <abbr title="User Interface">UI</abbr> is available yet (and this is often considered to be a better approach anyway). Embedded software in technical systems often needs to monitor the temperature in the device and trigger a cooling function to start when the temperature rises above a certain level. This could be tested without the hardware using a software interface to specify the temperature.</li>
					<li>State transition testing is used to evaluate the state behavior of the <abbr title="System Under Test">SUT</abbr>. A way to check whether the <abbr title="System Under Test">SUT</abbr> is in the correct state is by querying it via a customized software interface designed for this purpose (although this also includes a risk, see level of intrusion in Section 2.1).</li>
				</ul>
				<p>Design for automation should consider that:</p>
				<ul>
					<li>Compatibility with existing test tools should be established early on.</li>
					<li>The issue of test tool compatibility is critical in that it may impact the ability to automate tests of important functionality (e.g., incompatibility with a grid control prevents all tests using that control).</li>
					<li>Solutions may require development of program code and calls to APIs</li>
				</ul>
				<p>Designing for testability is of the utmost importance for a good test automation approach, and can also benefit manual test execution.</p>
			</article>

			<article id="3-the-generic-test-automation-architecture">
				<h1><a class="color-inherit" href="#3-the-generic-test-automation-architecture">3. The Generic Test Automation Architecture</a></h1>
				<h2 id="3-1-introduction-to-gtaa"><a class="color-inherit" href="#3-1-introduction-to-gtaa">3.1 Introduction to <abbr title="Generic Test Automation Architecture">gTAA</abbr></a></h2>
				<p>A test automation engineer (<abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr>) has the role of designing, developing, implementing, and maintaining test automation solutions (TASs). As each solution is developed, similar tasks need to be done, similar questions need to be answered, and similar issues need to be addressed and prioritized. These reoccurring concepts, steps, and approaches in automating testing become the basis of the generic test automation architecture, called <abbr title="Generic Test Automation Architecture">gTAA</abbr> in short.</p>
				<p>The <abbr title="Generic Test Automation Architecture">gTAA</abbr> presents the layers, components, and interfaces of a <abbr title="Generic Test Automation Architecture">gTAA</abbr>, which are then further redefined into the concrete <abbr title="Test Automation Architecture">TAA</abbr> for a particular <abbr title="Test Automation Solution">TAS</abbr>. It allows for a structured and modular approach to building a test automation solution by:</p>
				<ul>
					<li>Defining the concept space, layers, services, and interfaces of a <abbr title="Test Automation Solution">TAS</abbr> to enable the realization of TASs by in-house as well as by externally developed components</li>
					<li>Supporting simplified components for the effective and efficient development of test automation</li>
					<li>Re-using test automation components for different or evolving TASs for software product lines and families and across software technologies and tools</li>
					<li>Easing the maintenance and evolution of TASs</li>
					<li>Defining the essential features for a user of a <abbr title="Test Automation Solution">TAS</abbr></li>
				</ul>
				<p>A <abbr title="Test Automation Solution">TAS</abbr> consists of both the test environment (and its artifacts) and the test suites (a set of test cases including test data). A test automation framework (<abbr title="Test Automation Framework">TAF</abbr>) can be used to realize a <abbr title="Test Automation Solution">TAS</abbr>. It provides support for the realization of the test environment and provides tools, test harnesses, or supporting libraries.</p>
				<p>It is recommended that the <abbr title="Test Automation Architecture">TAA</abbr> of a <abbr title="Test Automation Solution">TAS</abbr> complies with the following principles that support easy development, evolution, and maintenance of the <abbr title="Test Automation Solution">TAS</abbr>:</p>
				<ul>
					<li>Single responsibility: Every <abbr title="Test Automation Solution">TAS</abbr> component must have a single responsibility, and that responsibility must be encapsulated entirely in the component. In other words, every component of a <abbr title="Test Automation Solution">TAS</abbr> should be in charge of exactly one thing, e.g., generating keywords or data, creating test scenarios, executing test cases, logging results, generating execution reports.</li>
					<li>Extension (see e.g., open/closed principle by B. Myer): Every <abbr title="Test Automation Solution">TAS</abbr> component must be open for extension, but closed for modification. This principle means that it should be possible to modify or enrich the behavior of the components without breaking the backward compatible functionality.</li>
					<li>Replacement (see e.g., substitution principle by B. Liskov): Every <abbr title="Test Automation Solution">TAS</abbr> component must be replaceable without affecting the overall behavior of the <abbr title="Test Automation Solution">TAS</abbr>. The component can be replaced by one or more substituting components but the exhibited behavior must be the same.</li>
					<li>Component segregation (see e.g., interfaces segregation principle by R.C. Martin): It is better to have more specific components than a general, multi-purpose component. This makes substitution and maintenance easier by eliminating unnecessary dependencies.</li>
					<li>Dependency inversion: The components of a <abbr title="Test Automation Solution">TAS</abbr> must depend on abstractions rather than on lowlevel details. In other words, the components should not depend on specific automated test scenarios.</li>
				</ul>
				<p>Typically, a <abbr title="Test Automation Solution">TAS</abbr> based on the <abbr title="Generic Test Automation Architecture">gTAA</abbr> will be implemented by a set of tools, their plugins, and/or components. It is important to note that the <abbr title="Generic Test Automation Architecture">gTAA</abbr> is vendor-neutral: it does not predefine any concrete method, technology, or tool for the realization of a <abbr title="Test Automation Solution">TAS</abbr>. The <abbr title="Generic Test Automation Architecture">gTAA</abbr> can be implemented by any software engineering approach, e.g., structured, object-oriented, service-oriented, model-driven, as well as by any software technologies and tools. In fact, a <abbr title="Test Automation Solution">TAS</abbr> is often implemented using off-the-shelf tools, but will typically need additional <abbr title="System Under Test">SUT</abbr> specific additions and/or adaptations.</p>
				<p>Other guidelines and reference models relating to TASs are software engineering standards for the selected <abbr title="Software Development Lifecycle">SDLC</abbr> (Software Development Lifecycle), programming technologies, formatting standards, etc. It is not in the scope of this syllabus to teach software engineering in general, however, a <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> is expected to have skills, experience, and expertise in software engineering.</p>
				<p>Furthermore, a <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to be aware of industry coding and documentation standards and best practices to make use of them while developing a <abbr title="Test Automation Solution">TAS</abbr>. These practices can increase maintainability, reliability, and security of the <abbr title="Test Automation Solution">TAS</abbr>. Such standards are typically domain-specific. Popular standards include:</p>
				<ul>
					<li>MISRA for C or C++</li>
					<li>JSF coding standard for C++</li>
					<li>AUTOSAR rules for MathWorks Matlab/SimulinkÂ®</li>
				</ul>
				<h3 id="3-1-1-overview-of-the-gtaa"><a class="color-inherit" href="#3-1-1-overview-of-the-gtaa">3.1.1 Overview of the <abbr title="Generic Test Automation Architecture">gTAA</abbr></a></h3>
				<p class="learning-objective color-secondary">ALTA-E-3.1.1 (K2) Explain the structure of the <abbr title="Generic Test Automation Architecture">gTAA</abbr></p>
				<p>The <abbr title="Generic Test Automation Architecture">gTAA</abbr> is structured into horizontal layers for the following:</p>
				<ul>
					<li>Test generation</li>
					<li>Test definition</li>
					<li>Test execution</li>
					<li>Test adaptation</li>
				</ul>
				<p>The <abbr title="Generic Test Automation Architecture">gTAA</abbr> (see Figure 1: The Generic Test Automation Architecture) encompasses the following:</p>
				<ul>
					<li>The Test Generation Layer that supports the manual or automated design of test cases. It provides the means for designing test cases.</li>
					<li>The Test Definition Layer that supports the definition and implementation of test suites and/or test cases. It separates the test definition from the <abbr title="System Under Test">SUT</abbr> and/or test system technologies and tools. It contains means to define high-level and low-level tests, which are handled in the test data, test cases, test procedures, and test library components or combinations thereof.</li>
					<li>The Test Execution Layer that supports the execution of test cases and test logging. It provides a test execution tool to execute the selected tests automatically and a logging and reporting component.</li>
					<li>The Test Adaptation Layer which provides the necessary code to adapt the automated tests for the various components or interfaces of the <abbr title="System Under Test">SUT</abbr>. It provides different adaptors for connecting to the <abbr title="System Under Test">SUT</abbr> via APIs, protocols, services, and others.</li>
					<li>It also has interfaces for project management, configuration management and test management in relation to test automation. For example, the interface between test management and test adaptation layer copes with the selection and configuration of the appropriate adaptors in relation to the chosen test configuration.</li>
				</ul>
				<p>The interfaces between the <abbr title="Generic Test Automation Architecture">gTAA</abbr> layers and their components are typically specific and, therefore, not further elaborated here.</p>
				<p>It is important to understand that these layers can be present or absent in any given <abbr title="Test Automation Solution">TAS</abbr>. For example:</p>
				<ul>
					<li>If the test execution is to be automated, the test execution and the test adaptation layers need to be utilized. They do not need to be separated and could be realized together, e.g., in unit test frameworks.</li>
					<li>If the test definition is to be automated, the test definition layer is required.</li>
					<li>If the test generation is to be automated, the test generation layer is required.</li>
				</ul>
				<p>Most often, one would start with the implementation of a <abbr title="Test Automation Solution">TAS</abbr> from bottom to top, but other approaches such as the automated test generation for manual tests can be useful as well. In general it is advised to implement the <abbr title="Test Automation Solution">TAS</abbr> in incremental steps (e.g., in sprints) in order to use the <abbr title="Test Automation Solution">TAS</abbr> as soon as possible and to prove the added value of the <abbr title="Test Automation Solution">TAS</abbr>. Also, proofs of concept are recommended as part of test automation project.</p>
				<p>Any test automation project needs to be understood, set up, and managed as a software development project and requires dedicated project management. The project management for the <abbr title="Test Automation Framework">TAF</abbr> development (i.e., test automation support for a whole company, product families or product lines) can be separated from the project management for the <abbr title="Test Automation Solution">TAS</abbr> (i.e., test automation for a concrete product).</p>
				<figure>
					<img loading="lazy" width="1632" height="1068" src="../assets/generic-test-automation-architecture.png" alt="The Generic Test Automation Architecture">
					<figcaption>Figure 1: The Generic Test Automation Architecture</figcaption>
				</figure>
				<h3 id="3-1-2-test-generation-layer"><a class="color-inherit" href="#3-1-2-test-generation-layer">3.1.2 Test Generation Layer</a></h3>
				<p>The test generation layer consists of tool support for the following:</p>
				<ul>
					<li>Manually designing test cases</li>
					<li>Developing, capturing, or deriving test data</li>
					<li>Automatically generating test cases from models that define the <abbr title="System Under Test">SUT</abbr> and/or its environment (i.e., automated model-based testing)</li>
				</ul>
				<p>The components in this layer are used to:</p>
				<ul>
					<li>Edit and navigate test suite structures</li>
					<li>Relate test cases to test objectives or <abbr title="System Under Test">SUT</abbr> requirements</li>
					<li>Document the test design</li>
				</ul>
				<p>For automated test generation the following capabilities may also be included:</p>
				<ul>
					<li>Ability to model the <abbr title="System Under Test">SUT</abbr>, its environment, and/or the test system</li>
					<li>Ability to define test directives and to configure/parameterize test generation algorithms</li>
					<li>Ability to trace the generated tests back to the model (elements)</li>
				</ul>
				<h3 id="3-1-3-test-definition-layer"><a class="color-inherit" href="#3-1-3-test-definition-layer">3.1.3 Test Definition Layer</a></h3>
				<p>The test definition layer consists of tool support for the following:</p>
				<ul>
					<li>Specifying test cases (at a high and/or low level)</li>
					<li>Defining test data for low-level test cases</li>
					<li>Specifying test procedures for a test case or a set of test cases</li>
					<li>Defining test scripts for the execution of the test cases</li>
					<li>Providing access to test libraries as needed (for example in keyword-driven approaches)</li>
				</ul>
				<p>The components in this layer are used to:</p>
				<ul>
					<li>Partition/constrain, parameterize or instantiate test data</li>
					<li>Specify test sequences or fully-fledged test behaviors (including control statements and expressions), to parameterize and/or to group them</li>
					<li>Document the test data, test cases and/or test procedures</li>
				</ul>
				<h3 id="3-1-4-test-execution-layer"><a class="color-inherit" href="#3-1-4-test-execution-layer">3.1.4 Test Execution Layer</a></h3>
				<p>The test execution layer consists of tool support for the following:</p>
				<ul>
					<li>Executing test cases automatically</li>
					<li>Logging the test case executions</li>
					<li>Reporting the test results</li>
				</ul>
				<p>The test execution layer may consist of components that provide the following capabilities:</p>
				<ul>
					<li>Set up and tear down the <abbr title="System Under Test">SUT</abbr> for test execution</li>
					<li>Set up and tear down test suites (i.e., set of test cases including test data)</li>
					<li>Configure and parameterize the test setup</li>
					<li>Interpret both test data and test cases and transform them into executable scripts</li>
					<li>Instrument the test system and/or the <abbr title="System Under Test">SUT</abbr> for (filtered) logging of test execution and/or for fault injection</li>
					<li>Analyze the <abbr title="System Under Test">SUT</abbr> responses during test execution to steer subsequent test runs</li>
					<li>Validate the <abbr title="System Under Test">SUT</abbr> responses (comparison of expected and actual results) for automated test case execution results</li>
					<li>Control the automated test execution in time</li>
				</ul>
				<h3 id="3-1-5-test-adaptation-layer"><a class="color-inherit" href="#3-1-5-test-adaptation-layer">3.1.5 Test Adaptation Layer</a></h3>
				<p>The test adaptation layer consists of tool support for the following:</p>
				<ul>
					<li>Controlling the test harness</li>
					<li>Interacting with the <abbr title="System Under Test">SUT</abbr></li>
					<li>Monitoring the <abbr title="System Under Test">SUT</abbr></li>
					<li>Simulating or emulating the <abbr title="System Under Test">SUT</abbr> environment</li>
				</ul>
				<p>The test adaptation layer provides the following functionality:</p>
				<ul>
					<li>Mediating between the technology-neutral test definitions and the specific technology requirements of the <abbr title="System Under Test">SUT</abbr> and the test devices</li>
					<li>Applying different technology-specific adaptors to interact with the <abbr title="System Under Test">SUT</abbr></li>
					<li>Distributing the test execution across multiple test devices/test interfaces or executing tests locally</li>
				</ul>
				<h3 id="3-1-6-configuration-management-of-a-tas"><a class="color-inherit" href="#3-1-6-configuration-management-of-a-tas">3.1.6 Configuration Management of a <abbr title="Test Automation Solution">TAS</abbr></a></h3>
				<p>Normally, a <abbr title="Test Automation Solution">TAS</abbr> is being developed in various iterations/versions and needs to be compatible with the iterations/versions of the <abbr title="System Under Test">SUT</abbr>. The configuration management of a <abbr title="Test Automation Solution">TAS</abbr> may need to include:</p>
				<ul>
					<li>Test models</li>
					<li>Test definitions/specifications including test data, test cases and libraries</li>
					<li>Test scripts</li>
					<li>Test execution engines and supplementary tools and components</li>
					<li>Test adaptors for the <abbr title="System Under Test">SUT</abbr></li>
					<li>Simulators and emulators for the <abbr title="System Under Test">SUT</abbr> environment</li>
					<li>Test results and test reports</li>
				</ul>
				<p>These items constitute the testware and must be at the correct version to match the version of the <abbr title="System Under Test">SUT</abbr>. In some situations it might be necessary to revert to previous versions of the <abbr title="Test Automation Solution">TAS</abbr>, e.g., in case field issues need to be reproduced with older <abbr title="System Under Test">SUT</abbr> versions. Good configuration management enables this capability.</p>
				<h3 id="3-1-7-project-management-of-a-tas"><a class="color-inherit" href="#3-1-7-project-management-of-a-tas">3.1.7 Project Management of a <abbr title="Test Automation Solution">TAS</abbr></a></h3>
				<p>As any test automation project is a software project, it requires the same project management as any other software project. A <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to perform the tasks for all phases of the established <abbr title="Software Development Lifecycle"><abbr title="Software Development Lifecycle">SDLC</abbr></abbr> methodology when developing the <abbr title="Test Automation Solution">TAS</abbr>. Also, a <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to understand that the development environment of the <abbr title="Test Automation Solution">TAS</abbr> should be designed such that status information (metrics) can be extracted easily or automatically reported to the project management of the <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<h3 id="3-1-8-tas-support-for-test-management"><a class="color-inherit" href="#3-1-8-tas-support-for-test-management">3.1.8 <abbr title="Test Automation Solution">TAS</abbr> Support for Test Management</a></h3>
				<p>A <abbr title="Test Automation Solution">TAS</abbr> must support the test management for the <abbr title="System Under Test">SUT</abbr>. Test reports including test logs and test results need to be extracted easily or automatically provided to the test management (people or system) of the <abbr title="System Under Test">SUT</abbr>.</p>
				<h2 id="3-2-taa-design"><a class="color-inherit" href="#3-2-taa-design">3.2 <abbr title="Test Automation Architecture">TAA</abbr> Design</a></h2>
				<h3 id="3-2-1-introduction-to-taa-design"><a class="color-inherit" href="#3-2-1-introduction-to-taa-design">3.2.1 Introduction to <abbr title="Test Automation Architecture">TAA</abbr> Design</a></h3>
				<p class="learning-objective color-secondary">ALTA-E-3.2.1 (K4) Design the appropriate <abbr title="Test Automation Architecture">TAA</abbr> for a given project</p>
				<p>There are a number of principal activities required to design a <abbr title="Test Automation Architecture">TAA</abbr>, which can be ordered according to the needs of the test automation project or organization. These activities are discussed in the sections below. More or fewer activities may be required depending on the complexity of the <abbr title="Test Automation Architecture">TAA</abbr>.</p>
				<p><strong>Capture requirements needed to define an appropriate <abbr title="Test Automation Architecture">TAA</abbr></strong></p>
				<p>The requirements for a test automation approach need to consider the following:</p>
				<ul>
					<li>Which activity or phase of the test process should be automated, e.g., test management, test design, test generation, or test execution. Note that test automation refines the fundamental test process by inserting test generation between test design and test implementation.</li>
					<li>Which test level should be supported, e.g., component level, integration level, system level</li>
					<li>Which type of test should be supported, e.g., functional testing, conformance testing, interoperability testing</li>
					<li>Which test role should be supported, e.g., test executor, test analyst, test architect, test manager</li>
					<li>Which software product, software product line, software product family should be supported, e.g., to define the span and lifetime of the implemented <abbr title="Test Automation Solution">TAS</abbr></li>
					<li>Which <abbr title="System Under Test">SUT</abbr> technologies should be supported, e.g., to define the <abbr title="Test Automation Solution">TAS</abbr> in view of compatibility to the <abbr title="System Under Test">SUT</abbr> technologies</li>
				</ul>
				<p><strong>Compare and contrast different design/architecture approaches</strong></p>
				<p>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to analyze the pros and cons of different approaches when designing selected layers of the <abbr title="Test Automation Architecture">TAA</abbr>. These include but are not limited to:</p>
				<p><strong>Considerations for the test generation layer:</strong></p>
				<ul>
					<li>Selection of manual or automated test generation</li>
					<li>Selection of for example requirements-based, data-based, scenario-based or behavior-based test generation</li>
					<li>Selection of test generation strategies (e.g., model coverage such as classification trees for data-based approaches, use case/exception case coverage for scenario-based approaches, transition/state/path coverage for behavior-based approaches, etc.)</li>
					<li>Choosing of the test selection strategy. In practice, full combinatorial test generation is infeasible as it may lead to test case explosion. Therefore, practical coverage criteria, weights, risk assessments, etc. should be used to guide the test generation and subsequent test selection.</li>
				</ul>
				<p><strong>Considerations for the test definition layer:</strong></p>
				<ul>
					<li>Selection of data-driven, keyword-driven, pattern-based or model-driven test definition</li>
					<li>Selection of notation for test definition (e.g., tables, state-based notation, stochastic notation, dataflow notation, business process notation, scenario-based notation, etc. by use of spreadsheets, domain-specific test languages, the Testing and Test Control Notation (TTCN- 3), the UML Testing Profile (UTP), etc.)</li>
					<li>Selection of style guides and guidelines for the definition of high quality tests</li>
					<li>Selection of test case repositories (spreadsheets, databases, files, etc.)</li>
				</ul>
				<p><strong>Considerations for the test execution layer:</strong></p>
				<ul>
					<li>Selection of the test execution tool</li>
					<li>Selection of interpretation (by use of a virtual machine) or compilation approach for implementing test procedures â€“ this choice typically depends on the chosen test execution tool</li>
					<li>Selection of the implementation technology for implementing test procedures (imperative, such as C; functional, such as Haskell or Erlang; object-oriented, such as C++, C#, Java; scripting, such as Python or Ruby, or a tool-specific technology) â€“ this choice is typically dependent on the chosen test execution tool</li>
					<li>Selection of helper libraries to ease test execution (e.g., test device libraries, encoding/decoding libraries, etc.)</li>
				</ul>
				<p><strong>Considerations for the test adaptation layer:</strong></p>
				<ul>
					<li>Selection of test interfaces to the <abbr title="System Under Test">SUT</abbr></li>
					<li>Selection of tools to stimulate and observe the test interfaces</li>
					<li>Selection of tools to monitor the <abbr title="System Under Test">SUT</abbr> during test execution</li>
					<li>Selection of tools to trace test execution (e.g., including the timing of the test execution)</li>
				</ul>
				<p><strong>Identify areas where abstraction can deliver benefits</strong></p>
				<p>Abstraction in a <abbr title="Test Automation Architecture">TAA</abbr> enables technology independence in that the same test suite can be used in different test environments and on different target technologies. The portability of test artifacts is increased. In addition, vendor-neutrality is assured which avoids lock-in effects for a <abbr title="Test Automation Solution">TAS</abbr>. Abstraction also improves maintainability and adaptability to new or evolving <abbr title="System Under Test">SUT</abbr> technologies. Furthermore, abstraction helps to make a <abbr title="Test Automation Architecture">TAA</abbr> (and its instantiations by TASs) more accessible to non-technicians as test suites can be documented (including graphical means) and explained at a higher level, which improves readability and understandability.</p>
				<p>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to discuss with the stakeholders in software development, quality assurance, and testing which level of abstraction to use in which area of the <abbr title="Test Automation Solution">TAS</abbr>. For example, which interfaces of the test adaptation and/or test execution layer need to be externalized, formally defined, and kept stable throughout the <abbr title="Test Automation Architecture">TAA</abbr> lifetime? It also needs to be discussed if an abstract test definition is being used or if the <abbr title="Test Automation Architecture">TAA</abbr> uses a test execution layer with test scripts only. Likewise, it needs to be understood if test generation is abstracted by use of test models and model-based testing approaches. The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to be aware that there are trade-offs between sophisticated and straightforward implementations of a <abbr title="Test Automation Architecture">TAA</abbr> with respect to overall functionality, maintainability, and expandability. A decision on which abstraction to use in a <abbr title="Test Automation Architecture">TAA</abbr> needs to take into account these trade-offs.</p>
				<p>The more abstraction is used for a <abbr title="Test Automation Architecture">TAA</abbr>, the more flexible it is with respect to further evolution or transitioning to new approaches or technologies. This comes at the cost of larger initial investments (e.g., more complex test automation architecture and tools, higher skill set requirements, bigger learning curves), which delays the initial breakeven but can pay off in the long run. It may also lead to lower performance of the <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<p>While the detailed ROI (Return on Investment) considerations are the responsibility of the <abbr title="Test Automation Manager">TAM</abbr>, the <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to provide inputs to the ROI analysis by providing technical evaluations and comparisons of different test automation architectures and approaches with respect to timing, costs, efforts, and benefits.</p>
				<p><strong>Understand <abbr title="System Under Test">SUT</abbr> technologies and how these interconnect with the <abbr title="Test Automation Solution">TAS</abbr></strong></p>
				<p>The access to the test interfaces of the <abbr title="System Under Test">SUT</abbr> is central to any automated test execution. The access can be available at the following levels:</p>
				<ul>
					<li>Software level, e.g., <abbr title="System Under Test">SUT</abbr> and test software are linked together</li>
					<li>API level, e.g., the <abbr title="Test Automation Solution">TAS</abbr> invokes the functions/operations/methods provided at a (remote) application programming interface</li>
					<li>Protocol level, e.g., the <abbr title="Test Automation Solution">TAS</abbr> interacts with the <abbr title="System Under Test">SUT</abbr> via HTTP, TCP, etc.</li>
					<li>Service level, e.g., the <abbr title="Test Automation Solution">TAS</abbr> interacts with the <abbr title="System Under Test">SUT</abbr> services via web services, RESTful services, etc.</li>
				</ul>
				<p>In addition, the <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to decide about the paradigm of interaction of the <abbr title="Test Automation Architecture">TAA</abbr> to be used for the interaction between the <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr>, whenever the <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> are separated by APIs, protocols or services. These paradigms include the following:</p>
				<ul>
					<li>Event-driven paradigm, which drives the interaction via events being exchanged on an event bus</li>
					<li>Client-server paradigm, which drives the interaction via service invocation from service requestors to service provider</li>
					<li>Peer-to-peer paradigm, which drives the interaction via service invocation from either peer</li>
				</ul>
				<p>Often the paradigm choice depends on the <abbr title="System Under Test">SUT</abbr> architecture and may have implications on the <abbr title="System Under Test">SUT</abbr> architecture. The interconnection between the <abbr title="System Under Test">SUT</abbr> and the <abbr title="Test Automation Architecture">TAA</abbr> needs to be carefully analyzed and designed in order to select a future-safe architecture between the two systems.</p>
				<p><strong>Understand the <abbr title="System Under Test">SUT</abbr> environment</strong></p>
				<p>An <abbr title="System Under Test">SUT</abbr> can be standalone software or software that works only in relation to other software (e.g., systems of systems), hardware (e.g., embedded systems), or environmental components (e.g., cyber-physical systems). A <abbr title="Test Automation Solution">TAS</abbr> simulates or emulates the <abbr title="System Under Test">SUT</abbr> environment as part of an automated test setup.</p>
				<p>Examples of test environments and sample uses include the following:</p>
				<ul>
					<li>A computer with both the <abbr title="System Under Test">SUT</abbr> and the <abbr title="Test Automation Solution">TAS</abbr> â€“ useful for testing a software application</li>
					<li>Individual networked computers for an <abbr title="System Under Test">SUT</abbr> and <abbr title="Test Automation Solution">TAS</abbr> respectively â€“ useful for testing server software</li>
					<li>Additional test devices to stimulate and observe the technical interfaces of an <abbr title="System Under Test">SUT</abbr> â€“ useful for testing the software for example on a set-top box</li>
					<li>Networked test devices to emulate the operational environment of the <abbr title="System Under Test">SUT</abbr> â€“ useful for testing the software of a network router</li>
					<li>Simulators to simulate the physical environment of the <abbr title="System Under Test">SUT</abbr> â€“ useful for testing the software of an embedded control unit</li>
				</ul>
				<p><strong>Time and complexity for a given testware architecture implementation</strong></p>
				<p>While the effort estimation for a <abbr title="Test Automation Solution">TAS</abbr> project is the responsibility of a <abbr title="Test Automation Manager">TAM</abbr>, a <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> needs to support a <abbr title="Test Automation Manager">TAM</abbr> in this by providing good estimates for the time and complexity of a <abbr title="Test Automation Architecture">TAA</abbr> design. Methods for estimations and examples include the following:</p>
				<ul>
					<li>Analogy-based estimation such as such as functions points, three-point estimation, wideband delphi, and expert estimation</li>
					<li>Estimation by use of work breakdown structures such as those found in management software or project templates</li>
					<li>Parametric estimation such as Constructive Cost Model (COCOMO)</li>
					<li>Size-based estimations such as Function Point Analysis, Story Point Analysis, or Use Case Analysis</li>
					<li>Group estimations such as Planning Poker</li>
				</ul>
				<p><strong>Ease of use for a given testware architecture implementation</strong></p>
				<p>In addition to the functionality of the <abbr title="Test Automation Solution">TAS</abbr>, its compatibility with the <abbr title="System Under Test">SUT</abbr>, its long-term stability and evolvability, its effort requirements, and ROI considerations, a <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> has the specific responsibility to address usability issues for a <abbr title="Test Automation Solution">TAS</abbr>. This includes, but is not limited to:</p>
				<ul>
					<li>Tester-oriented design</li>
					<li>Ease of use of the <abbr title="Test Automation Solution">TAS</abbr></li>
					<li><abbr title="Test Automation Solution">TAS</abbr> support for other roles in the software development, quality assurance, and project management</li>
					<li>Effective organization, navigation, and search in/with the <abbr title="Test Automation Solution">TAS</abbr></li>
					<li>Useful documentation, manuals, and help text for the <abbr title="Test Automation Solution">TAS</abbr></li>
					<li>Practical reporting by and about the <abbr title="Test Automation Solution">TAS</abbr></li>
					<li>Iterative designs to address <abbr title="Test Automation Solution">TAS</abbr> feedback and empirical insights</li>
				</ul>
				<h3 id="3-2-2-approaches-for-automating-test-cases"><a class="color-inherit" href="#3-2-2-approaches-for-automating-test-cases">3.2.2 Approaches for Automating Test Cases</a></h3>
				<p class="learining-objective color-secondary">ALTA-E-3.2.2 (K2) Explain the role that layers play within a <abbr title="Test Automation Architecture">TAA</abbr></p>
				<p>Test cases need to be translated into sequences of actions which are executed against an <abbr title="System Under Test">SUT</abbr>. That sequence of actions can be documented in a test procedure and/or can be implemented in a test script. Besides actions, the automated test cases should also define test data for the interaction with the <abbr title="System Under Test">SUT</abbr> and include verification steps to verify that the expected result was achieved by the <abbr title="System Under Test">SUT</abbr>. A number of approaches can be used to create the sequence of actions:</p>
				<ol>
					<li>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> implements test cases directly into automated test scripts. This option is the least recommended as it lacks abstraction and increases the maintenance load.</li>
					<li>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> designs test procedures, and transforms them into automated test scripts. This option has abstraction but lacks automation to generate the test scripts.</li>
					<li>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> uses a tool to translate test procedures into automated test scripts. This option combines both abstraction and automated script generation.</li>
					<li>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> uses a tool that generates automated test procedures and/or translates the test scripts directly from models. This option has the highest degree of automation.</li>
				</ol>
				<p>Note that the options are heavily dependent on the context of the project. It may also be efficient to start test automation by applying one of the less advanced options, as these are typically easier to implement. This can provide added value at short term although it will result in a less maintainable solution.</p>
				<p>Well-established approaches for automating test cases include:</p>
				<ul>
					<li>Capture/playback approach, which can be used for option 1</li>
					<li>Structured scripting approach, data-driven approach, and keyword-driven approach, which can be used for option 2 or 3</li>
					<li>Model-based testing (including the process-driven approach), which can be used for option 4</li>
				</ul>
				<p>These approaches are explained subsequently in terms of principal concepts and pros and cons.</p>
				<p><strong>Capture/playback approach</strong></p>
				<p><em>Principal concept</em></p>
				<ul>
					<li>In capture/playback approaches, tools are used to capture interactions with the <abbr title="System Under Test">SUT</abbr> while performing the sequence of actions as defined by a test procedure. Inputs are captured; outputs may also be recorded for later checks. During the replay of events, there are various manual and automated output checking possibilities:</li>
					<li><strong>Manual:</strong> the tester has to watch the <abbr title="System Under Test">SUT</abbr> outputs for anomalies</li>
					<li><strong>Complete:</strong> all system outputs that were recorded during capture must be reproduced by the <abbr title="System Under Test">SUT</abbr></li>
					<li><strong>Exact:</strong> all system outputs that were recorded during capture must be reproduced by the <abbr title="System Under Test">SUT</abbr> to the level of detail of the recording</li>
					<li><strong>Checkpoints:</strong> only selected system outputs are checked at certain points for specified values</li>
				</ul>
				<p><em>Pros</em></p>
				<ul>
					<li>The capture/playback approach can be used for <abbr title="System Under Test">SUT</abbr>s on the <abbr title="Graphical User Interface">GUI</abbr> and/or API level. Initially, it is easy to setup and use.</li>
				</ul>
				<p><em>Cons</em></p>
				<ul>
					<li>Capture/playback scripts are hard to maintain and evolve because the captured <abbr title="System Under Test">SUT</abbr> execution depends strongly on the <abbr title="System Under Test">SUT</abbr> version from which the capture has been taken. For example, when recording at the <abbr title="Graphical User Interface">GUI</abbr> level, changes in the <abbr title="Graphical User Interface">GUI</abbr> layout may impact the test script, even if it is only a change in the positioning of a <abbr title="Graphical User Interface">GUI</abbr> element. Therefore, capture/replay approaches remain vulnerable to changes.</li>
					<li>Implementation of the test cases (scripts) can only start when the <abbr title="System Under Test">SUT</abbr> is available.</li>
				</ul>
				<p><strong>Linear scripting</strong></p>
				<p><em>Principal concept</em></p>
				<ul>
					<li>As with all scripting techniques, linear scripting starts with some manual test procedures. Note though that these may not be written documents â€“ the knowledge about what tests to run and how to run them may be â€˜knownâ€™ by one or more Test Analysts.</li>
					<li>Each test is run manually while the test tool records the sequence of actions and in some cases captures the visible output from the <abbr title="System Under Test">SUT</abbr> to the screen. This generally results in one (typically large) script for each test procedure. Recorded scripts may be edited to improve readability (e.g., by adding comments to explain what is happening at key points) or add further checks using the scripting language of the tool.</li>
					<li>The scripts can then be replayed by the tool, causing the tool to repeat the same actions taken by the tester when the script was recorded. Although this can be used to automate <abbr title="Graphical User Interface">GUI</abbr> tests, it is not a good technique to use where large numbers of tests are to be automated and they are required for many releases of the software. This is because of the high maintenance cost that is typically caused by changes to the <abbr title="System Under Test">SUT</abbr> (each change in the <abbr title="System Under Test">SUT</abbr> may necessitate many changes to the recorded scripts).</li>
				</ul>
				<p><em>Pros</em></p>
				<ul>
					<li>The advantages of linear scripts focus on the fact that there is little or no preparation work required before you can start automating. Once you have learned to use the tool it is simply a matter of recording a manual test and replaying it (although the recording part of this may require additional interaction with the test tool to request that comparisons of actual with expected output occurs to verify the software is working correctly). Programming skills are not required but are usually helpful.</li>
				</ul>
				<p><em>Cons</em></p>
				<ul>
					<li>The disadvantages of linear scripts are numerous. The amount of effort required to automate any given test procedure will be mostly dependent on the size (number of steps or actions) required to perform it. Thus, the 1000th test procedure to be automated will take a similarly proportional amount of effort as the 100th test procedure. In other words, there is not much scope for decreasing the cost of building new automated tests.</li>
					<li>Furthermore, if there were a second script that performed a similar test albeit with different input values, that script would contain the same sequence of instructions as the first script; only the information included with the instructions (known as the instruction arguments or parameters) would differ. If there were several tests (and hence scripts) these would all contain the same sequence of instructions, all of which would need to be maintained whenever the software changed in a way that affected the scripts.</li>
					<li>Because the scripts are in a programming language, rather than a natural language, non- programmers may find them difficult to understand. Some test tools use proprietary languages (unique to the tool) so it takes time to learn the language and become proficient with it.</li>
					<li>Recorded scripts contain only general statements in the comments, if any at all. Long scripts in particular are best annotated with comments to explain what is going on at each step of the test. This makes maintenance easier. Scripts can soon become very large (containing many instructions) when the test comprises many steps.</li>
					<li>The scripts are non-modular and difficult to maintain. Linear scripting does not follow common software reusability and modularity paradigms and is tightly coupled with the tool being used.</li>
				</ul>
				<p><strong>Structured scripting</strong></p>
				<p><em>Principal concept</em></p>
				<ul><li>The major difference between the structured scripting technique and the linear scripting technique is the introduction of a script library. This contains reusable scripts that perform sequences of instructions that are commonly required across a number of tests. Good examples of such scripts are those that interface, e.g., to the operations of <abbr title="System Under Test">SUT</abbr> interfaces.</li></ul>
				<p><em>Pros</em></p>
				<ul>
					<li>Benefits of this approach include a significant reduction in the maintenance changes required and the reduced cost of automating new tests (because they can use scripts that already exist rather than having to create them all from scratch).</li>
					<li>The advantages of structured scripting are largely attained through the reuse of scripts. More tests can be automated without having to create the volume of scripts that a linear scripting approach would require. This has a direct impact on the build and maintenance costs. The second and subsequent tests will not take as much effort to automate because some of the scripts created to implement the first test can be reused again.</li>
				</ul>
				<p><em>Cons</em></p>
				<ul><li>The initial effort to create the shared scripts can be seen as a disadvantage but this initial investment should pay big dividends if approached properly. Programming skills will be required to create all the scripts as simple recording alone will not be sufficient. The script library must be well managed, i.e., the scripts should be documented and it should be easy for Technical Test Analysts to find the required scripts (so a sensible naming convention will help here).</li></ul>
				<p><strong>Data-driven testing</strong></p>
				<p><em>Principal concept</em></p>
				<ul>
					<li>The data-driven scripting technique builds on the structured scripting technique. The most significant difference is how the test inputs are handled. The inputs are extracted from the scripts and put into one or more separate files (typically called data files).</li>
					<li>This means the main test script can be reused to implement a number of tests (rather than just a single test). Typically the â€˜reusableâ€™ main test script is called a â€˜controlâ€™ script. The control script contains the sequence of instructions necessary to perform the tests but reads the input data from a data file. One control test may be used for many tests but it is usually insufficient to automate a wide range of tests. Thus, a number of control scripts will be required but that is only a fraction of the number of tests that are automated.</li>
				</ul>
				<p><em>Pros</em></p>
				<ul>
					<li>The cost of adding new automated tests can be significantly reduced by this scripting technique. This technique is used to automate many variations of a useful test, giving deeper testing in a specific area and may increase test coverage.</li>
					<li>Having the tests â€˜describedâ€™ by the data files means that Test Analysts can specify â€˜automatedâ€™ tests simply by populating one or more data files. This gives Test Analysts more freedom to specify automated tests without as much dependency on the Technical Test Analysts (who may be a scarce resource).</li>
				</ul>
				<p><em>Cons</em></p>
				<ul>
					<li>The need to manage data files and make sure they are readable by <abbr title="Test Automation Solution">TAS</abbr> is a disadvantage but can be approached properly.</li>
					<li>Also, important negative test cases may be missed. Negative tests are a combination of test procedures and test data. In an approach targeting test data mainly, "negative test procedures" may be missed.</li>
				</ul>
				<p><strong>Keyword-driven testing</strong></p>
				<p><em>Principal concept</em></p>
				<ul>
					<li>The keyword-driven scripting technique builds on the data-driven scripting technique. There are two main differences: (1) the data files are now called â€˜test definitionâ€™ files or something similar (e.g., action word files); and (2) there is only one control script.</li>
					<li>A test definition file contains a description of the tests in a way that should be easier for Test Analysts to understand (easier than the equivalent data file). It will usually contain data as does the data files but keyword files also contain high level instructions (the keywords, or â€˜action wordsâ€™).</li>
					<li>The keywords should be chosen to be meaningful to the Test Analyst, the tests being described and the application being tested. These are mostly (but not exclusively) used to represent high level business interactions with a system (e.g., â€œplace orderâ€). Each keyword represents a number of detailed interactions with the system under test. Sequences of keywords (including the relevant test data) are used to specify the test cases. Special keywords can be used for verification steps, or keywords can contain both the actions and the verification steps.</li>
					<li>The scope of responsibility for Test Analysts includes creating and maintaining the keyword files. This means that once the supporting scripts are implemented, Test Analysts can add â€˜automatedâ€™ tests simply by specifying them in a keyword file (as with data-driven scripting).</li>
				</ul>
				<p><em>Pros</em></p>
				<ul>
					<li>Once the controlling script and supporting scripts for the keywords have been written, the cost of adding new automated tests will be much reduced by this scripting technique.</li>
					<li>Having the tests â€˜describedâ€™ by the keyword files means that Test Analysts can specify â€˜automatedâ€™ tests simply by describing the tests using the keywords and associated data. This gives Test Analysts more freedom to specify automated tests without as much dependency on the Technical Test Analysts (who may be a scarce resource). The benefit of the keyword-driven approach over the data-driven approach in this regard is the use of the keywords. Each keyword should represent a sequence of detailed actions that produce some meaningful result. For example, â€˜create accountâ€™, â€˜place orderâ€™, â€˜check order statusâ€™ are all possible actions for an online shopping application that each involve a number of detailed steps. When one Test Analyst describes a system test to another Test Analyst, they are likely to speak in terms of these high level actions, not the detailed steps. The aim of the keyword-driven approach then is to implement these high level actions and allow tests to be defined in terms of the high level actions without reference to the detailed steps.</li>
					<li>These test cases are easier to maintain, read and write as the complexity can be hidden in the keywords (or in the libraries, in case of a structured scripting approach). The keywords can offer an abstraction from the complexities of the interfaces of the <abbr title="System Under Test">SUT</abbr>.</li>
				</ul>
				<p><em>Cons</em></p>
				<ul>
					<li>Implementing the keywords remains a big task for test automation engineers, particularly if using a tool that offers no support for this scripting technique. For small systems it may be too much overhead to implement and the costs would outweigh the benefits</li>
					<li>Care needs to be taken to ensure that the correct keywords are implemented. Good keywords will be used often with many different tests whereas poor keywords are likely to be used just once or only a few times</li>
				</ul>
				<p><strong>Process-driven scripting</strong></p>
				<p><em>Principal concept</em></p>
				<ul>
					<li>The process-driven approach builds on the keyword-driven scripting technique with the difference that scenarios â€“ representing uses cases of the <abbr title="System Under Test">SUT</abbr> and variants thereof â€“ constitute the scripts which are parameterized with test data or combined into higher-level test definitions.</li>
					<li>Such test definitions are easier to cope with as the logical relation between actions, e.g., â€˜check order statusâ€™ after â€˜place orderâ€™ in feature testing or â€˜check order statusâ€™ without previous â€˜place orderâ€™ in robustness testing, can be determined.</li>
				</ul>
				<p><em>Pros</em></p>
				<ul>
					<li>The use of process-like, scenario-based definition of test cases allows the test procedures to be defined from a workflow perspective. The aim of the process-driven approach is to implement these high-level workflows by using test libraries that represent the detailed test steps (see also keyword- driven approach).</li>
				</ul>
				<p><em>Cons</em></p>
				<ul>
					<li>Processes of an <abbr title="System Under Test">SUT</abbr> may not be easy to comprehend by a Technical Test Analyst â€“ and so is the implementation of the process-oriented scripts, particularly if no business process logic is supported by the tool.</li>
					<li>Care needs also to be taken to ensure that the correct processes, by use of correct keywords, are implemented. Good processes will be referenced by other processes and result in many relevant tests whereas poor processes will not pay off in terms of relevance, error-detection capability, etc.</li>
				</ul>
				<p><strong>Model-based testing</strong></p>
				<p><em>Principal concept</em></p>
				<ul>
					<li>Model-based testing refers to the automated generation of test cases (see also the Model-Based Tester Syllabus by ISTQB) â€“ as opposed to the automated execution of test cases â€“ by use of capture/playback, linear scripting, structured scripting, data-driven scripting or process-driven scripting. Model-based testing uses (semi-) formal models which abstract from the scripting technologies of the <abbr title="Test Automation Architecture">TAA</abbr>. Different test generation methods can be used to derive tests for any of the scripting frameworks discussed before.</li>
				</ul>
				<p><em>Pros</em></p>
				<ul>
					<li>Model-based testing allows by abstraction to concentrate on the essence of testing (in terms of business logic, data, scenarios, configurations, etc. to be tested). It also allows generating tests for different target systems and targeted technologies, so that the models used for test generation constitute a future-safe representation of testware which can be reused and maintained as the technology evolves.</li>
					<li>In case of changes in the requirements, the test model has to be adapted only; a complete set of test cases is generated automatically. Test case design techniques are incorporated in the test case generators.</li>
				</ul>
				<p><em>Cons</em></p>
				<ul><li>Modeling expertise is required to run a model-based testing approach effectively. The task of modeling by abstracting an <abbr title="System Under Test">SUT</abbr>â€™s interfaces, data and/or behavior can be difficult. In addition, modeling and model-based testing tools are not yet main stream, but are maturing. Model-based testing approaches require adjustments in the test processes. For example, the role of test designer needs to be established. In addition, the models used for test generation constitute major artifacts for quality assurance of an <abbr title="System Under Test">SUT</abbr> and need to be quality assured and maintained as well.</li></ul>

				<h3 id="3-2-3-technical-considerations-of-the-sut"><a class="color-inherit" href="#3-2-3-technical-considerations-of-the-sut">3.2.3 Technical considerations of the <abbr title="System Under Test">SUT</abbr></a></h3>
				<p class="learning-objective color-secondary">ALTA-E-3.2.3 (K2) Understand design considerations for a <abbr title="Test Automation Architecture">TAA</abbr></p>
				<p>In addition, technical aspects of an <abbr title="System Under Test">SUT</abbr> should be considered when designing a <abbr title="Test Automation Architecture">TAA</abbr>. Some of these are discussed below although this is not a complete list but should serve as a sample of the important aspects.</p>
				<p><strong>Interfaces of the <abbr title="System Under Test">SUT</abbr></strong></p>
				<p>An <abbr title="System Under Test">SUT</abbr> has internal interfaces (inside the system) and external interfaces (to the system environment and its users or by exposed components). A <abbr title="Test Automation Architecture">TAA</abbr> needs to be able to control and/or observe all those interfaces of the <abbr title="System Under Test">SUT</abbr> which are potentially affected by the test procedures (i.e., interfaces need to be testable). In addition, there may also be the need to log the interactions between the <abbr title="System Under Test">SUT</abbr> and the <abbr title="Test Automation Solution">TAS</abbr> with different levels of detail, typically including time stamps.</p>
				<p>Test focus (e.g., a test) is needed at the beginning of the project (or continuously in agile environments) during architecture definition to verify the availability of the necessary test interfaces or test facilities required for the <abbr title="System Under Test">SUT</abbr> to be testable (design for testability).</p>
				<p><strong><abbr title="System Under Test">SUT</abbr> data</strong></p>
				<p>An <abbr title="System Under Test">SUT</abbr> uses configuration data to control its instantiation, configuration, administration, etc. Furthermore, it uses user data which it processes. An <abbr title="System Under Test">SUT</abbr> also may use external data from other systems to complete its tasks. Depending on the test procedures for an <abbr title="System Under Test">SUT</abbr>, all these types of data need to be definable, configurable and capable of instantiation by the <abbr title="Test Automation Architecture">TAA</abbr>. The specific way of coping with the <abbr title="System Under Test">SUT</abbr> data is decided in the <abbr title="Test Automation Architecture">TAA</abbr> design. Depending on the approach, data may be handled as parameters, test data sheets, test databases, real data, etc.</p>
				<p><strong><abbr title="System Under Test">SUT</abbr> configurations</strong></p>
				<p>An <abbr title="System Under Test">SUT</abbr> may be deployed in different configurations, for example on different operating systems, on different target devices, or with different language settings. Depending on the test procedures, different <abbr title="System Under Test">SUT</abbr> configurations may have to be addressed by the <abbr title="Test Automation Architecture">TAA</abbr>. The test procedures may require different test setups (in a lab) or virtual test setups (in the cloud) of the <abbr title="Test Automation Architecture">TAA</abbr> in combination with a given <abbr title="System Under Test">SUT</abbr> configuration. It may also require adding simulators and/or emulators of selected <abbr title="System Under Test">SUT</abbr> components for selected <abbr title="System Under Test">SUT</abbr> aspects.</p>
				<p><strong><abbr title="System Under Test">SUT</abbr> standards and legal settings</strong></p>
				<p>In addition to the technical aspects of an <abbr title="System Under Test">SUT</abbr>, the <abbr title="Test Automation Architecture">TAA</abbr> design may need to respect legal and/or standards requirements so as to design the <abbr title="Test Automation Architecture">TAA</abbr> in a compatible manner. Examples include privacy requirements for the test data or confidentiality requirements that impact the logging and reporting capabilities of the <abbr title="Test Automation Architecture">TAA</abbr>.</p>
				<p><strong>Tools and tool environments used to develop the <abbr title="System Under Test">SUT</abbr></strong></p>
				<p>Along with the development of an <abbr title="System Under Test">SUT</abbr>, different tools may be used for the requirements engineering, design and modeling, coding, integration and deployment of the <abbr title="System Under Test">SUT</abbr>. The <abbr title="Test Automation Architecture">TAA</abbr> together with its own tools should take the <abbr title="System Under Test">SUT</abbr> tool landscape into account in order to enable tool compatibility, traceability and/or reuse of artifacts.</p>
				<p><strong>Test interfaces in the software product</strong></p>
				<p>It is strongly recommended not to remove all the test interfaces prior to the product release. In most cases, these interfaces can be left in the <abbr title="System Under Test">SUT</abbr> without causing issues with the final product. When left in place, the interfaces can be used by service and support engineers for problem diagnosis as well as for testing maintenance releases. It is important to verify that the interfaces will pose no security risks. If necessary, developers usually can disable these test interfaces such that they cannot be used outside the development department.</p>
				<h3 id="3-2-4-considerations-for-development/qa-processes"><a class="color-inherit" href="#3-2-4-considerations-for-development/qa-processes">3.2.4 Considerations for Development/QA Processes</a></h3>
				<p class="learning-objective color-secondary">ALTA-E-3.2.4 (K4) Analyze factors of implementation, use, and maintenance requirements for a given <abbr title="Test Automation Solution">TAS</abbr></p>
				<p>The aspects of the development and quality assurance processes of an <abbr title="System Under Test">SUT</abbr> should be considered when designing a <abbr title="Test Automation Architecture">TAA</abbr>. Some of these are discussed below although this is not a complete list but should serve as a sample of the important aspects.</p>
				<p><strong>Test execution control requirements</strong></p>
				<p>Depending on the level of automation required by the <abbr title="Test Automation Architecture">TAA</abbr>, interactive test execution, batch mode test execution or fully automated test execution may need to be supported by the <abbr title="Test Automation Architecture">TAA</abbr>.</p>
				<p><strong>Reporting requirements</strong></p>
				<p>Depending on the reporting requirements including types of reports and their structures, the <abbr title="Test Automation Architecture">TAA</abbr> needs to be able to support fixed, parameterized or defined test reports in different formats and layouts.</p>
				<p><strong>Role and access rights</strong></p>
				<p>Depending on the security requirements, the <abbr title="Test Automation Architecture">TAA</abbr> may be required to provide a role and access rights system.</p>
				<p><strong>Established tool landscape</strong></p>
				<p><abbr title="System Under Test">SUT</abbr> project management, test management, code and test repository, defect tracking, incident management, risk analysis, etc., may all be supported by tools composing the established tool landscape. The <abbr title="Test Automation Architecture">TAA</abbr> is also supported by a tool or tool set which needs to seamlessly integrate with the other tools in the landscape. Also, test scripts should be stored and versioned like <abbr title="System Under Test">SUT</abbr> code so that revisions follow the same process for both.</p>
				<h2 id="3-3-tas-development"><a class="color-inherit" href="#3-3-tas-development">3.3 <abbr title="Test Automation Solution">TAS</abbr> Development</a></h2>
				<h3 id="3-3-1-introduction-to-tas-development"><a class="color-inherit" href="#3-3-1-introduction-to-tas-development">3.3.1 Introduction to <abbr title="Test Automation Solution">TAS</abbr> Development</a></h3>
				<p class="learning-objective color-secondary">ALTA-E-3.3.2 (K2) Explain the factors to be considered when identifying reusability of components</p>
				<p>Development of a <abbr title="Test Automation Solution">TAS</abbr> is comparable to other software development projects. It can follow the same procedures and processes including peer reviews by developers and testers. Specific to a <abbr title="Test Automation Solution">TAS</abbr> are its compatibility and synchronization with the <abbr title="System Under Test">SUT</abbr>. These require consideration in the <abbr title="Test Automation Architecture">TAA</abbr> design (see Section 3.2) and in the <abbr title="Test Automation Solution">TAS</abbr> development. Also, the <abbr title="System Under Test">SUT</abbr> is impacted by the test strategy, e.g., having to make test interfaces available to the <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<p>This section uses the software development lifecycle (<abbr title="Software Development Lifecycle">SDLC</abbr>) for explaining the <abbr title="Test Automation Solution">TAS</abbr> development process and the process-related aspects of compatibility and synchronization to the <abbr title="System Under Test">SUT</abbr>. These aspects are likewise important for any other development process that has been chosen or is in place for the <abbr title="System Under Test">SUT</abbr> and/or <abbr title="Test Automation Solution">TAS</abbr> development â€“ they need to be adapted accordingly</p>
				<p>The basic <abbr title="Software Development Lifecycle">SDLC</abbr> for <abbr title="Test Automation Solution">TAS</abbr> is shown in Figure 2.</p>
				<figure>
					<img loading="lazy" width="1630" height="530" src="../assets/basic-sdlc-for-tas.png" alt="Basic software development life cycle for test automation strategy">
					<figcaption>Figure 2: Basic <abbr title="Software Development Lifecycle">SDLC</abbr> for <abbr title="Test Automation Solution">TAS</abbr></figcaption>
				</figure>
				<p>The set of requirements for a <abbr title="Test Automation Solution">TAS</abbr> needs to be analyzed and collected (see Figure 2). The requirements guide the design of the <abbr title="Test Automation Solution">TAS</abbr> as defined by its <abbr title="Test Automation Architecture">TAA</abbr> (see Section 3.2). The design is turned into software by software engineering approaches. Please note that a <abbr title="Test Automation Solution">TAS</abbr> may also use dedicated test device hardware, which is outside of consideration for this syllabus. Like any other software, a <abbr title="Test Automation Solution">TAS</abbr> needs to be tested. This is typically done by basic capability tests for the <abbr title="Test Automation Solution">TAS</abbr> which are followed by an interplay between the <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr>. After deployment and use of a <abbr title="Test Automation Solution">TAS</abbr>, often a <abbr title="Test Automation Solution">TAS</abbr> evolution is needed to add more test capability, change tests or to update the <abbr title="Test Automation Solution">TAS</abbr> to match the changing <abbr title="System Under Test">SUT</abbr>. The <abbr title="Test Automation Solution">TAS</abbr> evolution requires a new round of <abbr title="Test Automation Solution">TAS</abbr> development according to the <abbr title="Software Development Lifecycle">SDLC</abbr>.</p>
				<p>Please also note that the <abbr title="Software Development Lifecycle">SDLC</abbr> does not show the backup, archiving and teardown of a <abbr title="Test Automation Solution">TAS</abbr>. As with the <abbr title="Test Automation Solution">TAS</abbr> development, these procedures should follow established methods in an organization.</p>
				<p class="learning-objective color-secondary">ALTA-E-3.3.1 (K3) Apply components of the generic <abbr title="Test Automation Architecture">TAA</abbr> (gTAA) to construct a purpose-built <abbr title="Test Automation Architecture">TAA</abbr></p>
				<h3 id="3-3-2-compatibility-between-the-tas-and-the-sut"><a class="color-inherit" href="#3-3-2-compatibility-between-the-tas-and-the-sut">3.3.2 Compatibility between the <abbr title="Test Automation Solution">TAS</abbr> and the <abbr title="System Under Test">SUT</abbr></a></h3>
				<p><strong>Process compatibility</strong></p>
				<p>Testing of an <abbr title="System Under Test">SUT</abbr> should be synchronized with its development â€“ and, in the case of test automation, synchronized with the <abbr title="Test Automation Solution">TAS</abbr> development. Therefore, it is advantageous to coordinate the processes for <abbr title="System Under Test">SUT</abbr> development, <abbr title="Test Automation Solution">TAS</abbr> development and for testing. A large gain can be achieved when the <abbr title="System Under Test">SUT</abbr> and <abbr title="Test Automation Solution">TAS</abbr> development are compatible in terms of process structure, process management and tool support.</p>
				<p><strong>Team compatibility</strong></p>
				<p>Team compatibility is another aspect of compatibility between <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> development. If a compatible mindset is used to approach and manage the <abbr title="Test Automation Solution">TAS</abbr> and the <abbr title="System Under Test">SUT</abbr> development, both teams will benefit by reviewing each otherâ€™s requirements, designs and/or development artifacts, by discussing issues, and by finding compatible solutions. Team compatibility also helps in the communication and interaction with each other.</p>
				<p><strong>Technology compatibility</strong></p>
				<p>Furthermore, technology compatibility between the <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> should be considered. It is beneficial to design and implement a seamless interplay between the <abbr title="Test Automation Solution">TAS</abbr> and the <abbr title="System Under Test">SUT</abbr> right from the beginning. Even if that is not possible (e.g., because technical solutions are not available for either the <abbr title="Test Automation Solution">TAS</abbr> or <abbr title="System Under Test">SUT</abbr>), a seamless interplay by use of adapters, wrappers or other forms of intermediaries may be possible.</p>
				<p><strong>Tool compatibility</strong></p>
				<p>Tool compatibility between <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> management, development, and quality assurance needs to be considered. For example, if the same tools for requirements management and/or issues management are used, the exchange of information and the coordination of <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> development will be easier.</p>
				<h3 id="3-3-3-synchronization-between-tas-and-sut"><a class="color-inherit" href="#3-3-3-synchronization-between-tas-and-sut">3.3.3 Synchronization between <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr></a></h3>
				<p><strong>Synchronization of requirements</strong></p>
				<p>After requirements elicitation, both <abbr title="System Under Test">SUT</abbr> and <abbr title="Test Automation Solution">TAS</abbr> requirements are to be developed. <abbr title="Test Automation Solution">TAS</abbr> requirements can be grouped into two main groups of requirements: (1) requirements that address the development of the <abbr title="Test Automation Solution">TAS</abbr> as a software-based system, such as requirements for the <abbr title="Test Automation Solution">TAS</abbr> features for test design, test specification, test result analysis, etc. and (2) requirements that address the testing of the <abbr title="System Under Test">SUT</abbr> by means of the <abbr title="Test Automation Solution">TAS</abbr>. These so called testing requirements correspond to the <abbr title="System Under Test">SUT</abbr> requirements and reflect all those <abbr title="System Under Test">SUT</abbr> features and properties which are to be tested by the <abbr title="Test Automation Solution">TAS</abbr>. Whenever the <abbr title="System Under Test">SUT</abbr> or the <abbr title="Test Automation Solution">TAS</abbr> requirements are updated, it is important to verify the consistency between the two and to check that all <abbr title="System Under Test">SUT</abbr> requirements that are to be tested by the <abbr title="Test Automation Solution">TAS</abbr> have defined test requirements.</p>
				<p><strong>Synchronization of development phases</strong></p>
				<p>In order to have the <abbr title="Test Automation Solution">TAS</abbr> ready when needed for testing the <abbr title="System Under Test">SUT</abbr>, the development phases need to be coordinated. It is most efficient when the <abbr title="System Under Test">SUT</abbr> and <abbr title="Test Automation Solution">TAS</abbr> requirements, designs, specifications, and implementations are synchronized.</p>
				<p><strong>Synchronization of defect tracking</strong></p>
				<p>Defects can relate to the <abbr title="System Under Test">SUT</abbr>, to the <abbr title="Test Automation Solution">TAS</abbr> or to the requirements/designs/specifications. Because of the relationship between the two projects, whenever a defect is corrected within one, the corrective action may impact the other. Defect tracking and confirmation testing have to address both the <abbr title="Test Automation Solution">TAS</abbr> and the <abbr title="System Under Test">SUT</abbr>.</p>
				<p><strong>Synchronization of <abbr title="System Under Test">SUT</abbr> and <abbr title="Test Automation Solution">TAS</abbr> evolution</strong></p>
				<p>Both the <abbr title="System Under Test">SUT</abbr> and the <abbr title="Test Automation Solution">TAS</abbr> can evolve to accommodate new features or disable features, to correct defects, or to address changes in their environment (including changes to the <abbr title="System Under Test">SUT</abbr> and <abbr title="Test Automation Solution">TAS</abbr> respectively as one is an environment component for the other). Any change applied to an <abbr title="System Under Test">SUT</abbr> or to a <abbr title="Test Automation Solution">TAS</abbr> may impact the other so the management of these changes should address both the <abbr title="System Under Test">SUT</abbr> and <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<p>Two synchronization approaches between the <abbr title="System Under Test">SUT</abbr> and <abbr title="Test Automation Solution">TAS</abbr> development processes are depicted in Figure 3 and Figure 4.</p>
				<p>Figure 3 shows an approach where the two <abbr title="Software Development Lifecycle">SDLC</abbr> processes for the <abbr title="System Under Test">SUT</abbr> and the <abbr title="Test Automation Solution">TAS</abbr> are mainly synchronized in two phases: (1) the <abbr title="Test Automation Solution">TAS</abbr> analysis is based on the <abbr title="System Under Test">SUT</abbr> design, which itself is based on the <abbr title="System Under Test">SUT</abbr> analysis and (2) the testing of the <abbr title="System Under Test">SUT</abbr> makes use of the deployed <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<figure>
					<img loading="lazy" width="1630" height="636" src="../assets/synchronization-example-1-of-tas-and-sut-development-processes.png" alt="Test automation solution and system under test development processes">
					<figcaption>Figure 3: Synchronization example 1 of <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> development processes</figcaption>
				</figure>
				<p>Figure 4 shows a hybrid approach with both manual and automated testing. Whenever manual tests are used before the tests are automated or whenever manual and automated tests are used together, the <abbr title="Test Automation Solution">TAS</abbr> analysis should be based both on the <abbr title="System Under Test">SUT</abbr> design and the manual tests. In this way, the <abbr title="Test Automation Solution">TAS</abbr> is synchronized with both. The second major synchronization point for such an approach is as before: the <abbr title="System Under Test">SUT</abbr> testing requires deployed tests, which in the case of manual tests could just be the manual test procedures to be followed.</p>
				<figure>
					<img loading="lazy" width="1630" height="1086" src="../assets/synchronization-example-2-of-tas-and-sut-development-processes.png" alt="Synchronization example 2 of Test Automation Solution and System Under Test development processes">
					<figcaption>Figure 4: Synchronization example 2 of <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> development processes</figcaption>
				</figure>
				<h3 id="3-3-4-building-reuse-into-the-tas"><a class="color-inherit" href="#3-3-4-building-reuse-into-the-tas">3.3.4 Building Reuse into the <abbr title="Test Automation Solution">TAS</abbr></a></h3>
				<p>Reuse of a <abbr title="Test Automation Solution">TAS</abbr> refers to the reuse of <abbr title="Test Automation Solution">TAS</abbr> artifacts (from any level of its architecture) across product lines, product frameworks, product domains and/or project families. Requirements for reuse result from the relevance of <abbr title="Test Automation Solution">TAS</abbr> artifacts for the other product variants, products and/or projects. Reusable <abbr title="Test Automation Solution">TAS</abbr> artifacts can include:</p>
				<ul>
					<li>(Parts of) test models of test goals, test scenarios, test components or test data</li>
					<li>(Parts of) test cases, test data, test procedures or test libraries themselves</li>
					<li>The test engine and/or test report framework</li>
					<li>The adaptors to the <abbr title="System Under Test">SUT</abbr> components and/or interfaces</li>
				</ul>
				<p>While reuse aspects are already settled when the <abbr title="Test Automation Architecture">TAA</abbr> is defined, the <abbr title="Test Automation Solution">TAS</abbr> can help increase the ability for reuse by:</p>
				<ul>
					<li>Following the <abbr title="Test Automation Architecture">TAA</abbr> or revising and updating it whenever needed</li>
					<li>Documenting the <abbr title="Test Automation Solution">TAS</abbr> artifacts so that they are easily understood and can be incorporated into new contexts</li>
					<li>Ensuring the correctness of any <abbr title="Test Automation Solution">TAS</abbr> artifact so that the usage in new contexts is supported by its high quality</li>
				</ul>
				<p>It is important to note that while design for reuse is mainly a matter for the <abbr title="Test Automation Architecture">TAA</abbr>, the maintenance and improvements for reuse are a concern throughout the <abbr title="Test Automation Solution">TAS</abbr> lifecycle. It requires continuous consideration and effort to make reuse happen, to measure and demonstrate the added value of reuse, and to evangelize others to reuse existing TASs.</p>
				<h3 id="3-3-5-support-for-a-variety-of-target-systems"><a class="color-inherit" href="#3-3-5-support-for-a-variety-of-target-systems">3.3.5 Support for a Variety of Target Systems</a></h3>
				<p><abbr title="Test Automation Solution">TAS</abbr> support for a variety of target systems refers to the ability of a <abbr title="Test Automation Solution">TAS</abbr> to test different configurations of a software product. Different configurations refer to any of the following:</p>
				<ul>
					<li>Number and interconnection of <abbr title="System Under Test">SUT</abbr> components</li>
					<li>Environments (both software and hardware) on which the <abbr title="System Under Test">SUT</abbr> components run</li>
					<li>Technologies, programming languages or operating systems used to implement the <abbr title="System Under Test">SUT</abbr> components</li>
					<li>Libraries and packages the <abbr title="System Under Test">SUT</abbr> components are using</li>
					<li>Tools used to implement the <abbr title="System Under Test">SUT</abbr> components</li>
				</ul>
				<p>While the first four aspects impact the <abbr title="Test Automation Solution">TAS</abbr> on any test level, the last one applies mainly to component-level and integration-level testing.</p>
				<p>The ability of a <abbr title="Test Automation Solution">TAS</abbr> to test different software product configurations is determined when the <abbr title="Test Automation Architecture">TAA</abbr> is defined. However, the <abbr title="Test Automation Solution">TAS</abbr> has to implement the ability to handle the technical variance and has to enable the management of the <abbr title="Test Automation Solution">TAS</abbr> features and components needed for different configurations of a software product.</p>
				<p>The handling of the <abbr title="Test Automation Solution">TAS</abbr> variety in relation to the variety of the software product can be dealt with differently:</p>
				<ul>
					<li>Version/configuration management for the <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> can be used to provide the respective versions and configurations of the <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> that fit to each other</li>
					<li><abbr title="Test Automation Solution">TAS</abbr> parameterization can be used to adjust a <abbr title="Test Automation Solution">TAS</abbr> to an <abbr title="System Under Test">SUT</abbr> configuration</li>
				</ul>
				<p>It is important to note that while design for <abbr title="Test Automation Solution">TAS</abbr> variability is mainly a matter for the <abbr title="Test Automation Architecture">TAA</abbr>, the maintenance of and improvements for variability are a concern throughout the <abbr title="Test Automation Solution">TAS</abbr> life cycle. It requires continuous consideration and efforts to revise, add and even remove options and forms of variability.</p>
			</article>

			<article id="4-deployment-risks-and-contingencies">
				<h1><a class="color-inherit" href="#4-deployment-risks-and-contingencies">4 Deployment Risks and Contingencies</a></h1>
				<p>There are two main activities involved in the implementation and rollout of a <abbr title="Test Automation Solution">TAS</abbr>: pilot and deployment. The steps that comprise these two activities will vary depending on the type of <abbr title="Test Automation Solution">TAS</abbr> and the specific situation.</p>
				<p>For the pilot, at least the following steps should be considered:</p>
				<ul>
					<li>Identify a suitable project</li>
					<li>Plan the pilot</li>
					<li>Conduct the pilot</li>
					<li>Evaluate the pilot</li>
				</ul>
				<p>For deployment, at least the following steps should be considered:</p>
				<ul>
					<li>Identify initial target project(s)</li>
					<li>Deploy the <abbr title="Test Automation Solution">TAS</abbr> in the selected projects</li>
					<li>Monitor and evaluate the <abbr title="Test Automation Solution">TAS</abbr> in projects after a pre-defined period</li>
					<li>Rollout to the rest of the organization/projects</li>
				</ul>
				<h2 id="4-1-selection-of-test-automation-approach-and-planning-of-deployment/rollout"><a class="color-inherit" href="#4-1-selection-of-test-automation-approach-and-planning-of-deployment/rollout">4.1 Selection of Test Automation Approach and Planning of Deployment/Rollout</a></h2>
				<h3 id="4-1-1-pilot-project"><a class="color-inherit" href="#4-1-1-pilot-project">4.1.1 Pilot Project</a></h3>
				<p class="learning-objective color-secondary">ALTA-E-4.1.1 (K3) Apply guidelines that support effective test tool pilot and deployment activities</p>
				<p>Tool implementation typically starts with a pilot project. The aim of the pilot project is to ensure that the <abbr title="Test Automation Solution">TAS</abbr> can be used to achieve the planned benefits. Objectives of the pilot project include:</p>
				<ul>
					<li>Learn more detail about the <abbr title="Test Automation Solution">TAS</abbr>.</li>
					<li>See how the <abbr title="Test Automation Solution">TAS</abbr> fits with existing processes, procedures and tools; identify how they might need to change. (It is usually preferred to modify the <abbr title="Test Automation Solution">TAS</abbr> so it fits the existing processes/procedures. If these need to be adjusted to â€œsupport the <abbr title="Test Automation Solution">TAS</abbr>â€, this should at least be an improvement to the processes themselves).</li>
					<li>Design the automation interface to match the needs of the testers.</li>
					<li>Decide on standard ways of using, managing, storing and maintaining the <abbr title="Test Automation Solution">TAS</abbr> and the test assets including integration with configuration management and change management (e.g., deciding on naming conventions for files and tests, creating libraries and defining the modularity of test suites).</li>
					<li>Identify metrics and measurement methods to monitor test automation in use, including usability, maintainability and expandability.</li>
					<li>Assess whether the benefits can be achieved at reasonable cost. This will be an opportunity to reset expectations once the TASs have been used.</li>
					<li>Determine what skills are required and which of those are available and which are missing.</li>
				</ul>
				<p><strong>Identify a suitable project</strong></p>
				<p>The pilot project should be selected carefully using the following guidelines:</p>
				<ul>
					<li>Do not select a critical project. When the deployment of the <abbr title="Test Automation Solution">TAS</abbr> causes delay, this should not have major impact on critical projects. The deployment of the <abbr title="Test Automation Solution">TAS</abbr> will cost time at the beginning. The project team should be aware of this.</li>
					<li>Do not select a trivial project. A trivial project is not a good candidate since success of the deployment does not imply success on non-trivial projects, and thus adds less to the information needed for the deployment.</li>
					<li>Involve the necessary stakeholders (including management) in the selection process.</li>
					<li>The <abbr title="System Under Test">SUT</abbr> of the pilot project should be a good reference for the other projects of the organization, e.g., the <abbr title="System Under Test">SUT</abbr> should contain representative <abbr title="Graphical User Interface">GUI</abbr> components that have to be automated.</li>
				</ul>
				<p><strong>Plan the pilot</strong></p>
				<p>The pilot should be treated as a regular development project: make a plan, reserve budget and resources, report on the progress, define milestones, etc. An extra attention point is to make sure that the people working on the <abbr title="Test Automation Solution">TAS</abbr> deployment (i.e., a champion) can spend enough effort on the deployment even when other projects demand the resources for their activities. It is important to have management commitment, particularly on any shared resources. These people will likely not be able to work full-time on the deployment.</p>
				<p>When the <abbr title="Test Automation Solution">TAS</abbr> has not been provided by a vendor, but is developed in-house, the corresponding developers will need to be involved in the deployment activities.</p>
				<p><strong>Conduct the pilot</strong></p>
				<p>Perform the pilot of the deployment and pay attention to the following points:</p>
				<ul>
					<li>Does the <abbr title="Test Automation Solution">TAS</abbr> provide the functionality as expected (and promised by the vendor)? If not, this needs to be addressed as soon as possible. When the <abbr title="Test Automation Solution">TAS</abbr> is developed in-house the corresponding developers need to assist the deployment by providing any missing functionality.</li>
					<li>Do the <abbr title="Test Automation Solution">TAS</abbr> and the existing process support each other? If not they need to be aligned.</li>
				</ul>
				<p><strong>Evaluate the pilot</strong></p>
				<p>Use all stakeholders for the evaluation.</p>

				<h3 id="4-1-2-deployment"><a class="color-inherit" href="#4-1-2-deployment">4.1.2 Deployment</a></h3>
				<p>Once the pilot has been assessed, the <abbr title="Test Automation Solution">TAS</abbr> should only be deployed to the rest of the department/organization if the pilot has been deemed successful. Rollout should be undertaken incrementally and be well-managed. Success factors for deployment include:</p>
				<ul>
					<li>An incremental rollout: Perform the rollout to the rest of the organization in steps, in increments. In this way, the support to the new users comes in "waves" rather than all at once. This allows the usage of the <abbr title="Test Automation Solution">TAS</abbr> to increase in steps. Possible bottlenecks can be identified and solved before they become real problems. Licenses can be added when necessary.</li>
					<li>Adapting and improving processes to fit with the use of the <abbr title="Test Automation Solution">TAS</abbr>: When different users use the <abbr title="Test Automation Solution">TAS</abbr>, different processes come in touch with the <abbr title="Test Automation Solution">TAS</abbr>, and need to be tuned to the <abbr title="Test Automation Solution">TAS</abbr>, or the <abbr title="Test Automation Solution">TAS</abbr> may need (small) adaptions to the processes.</li>
					<li>Providing training and coaching/mentoring for new users: New users need training and coaching in the use of the new <abbr title="Test Automation Solution">TAS</abbr>. Make sure this is in place. Training/workshops should be provided to the users before they actually use the <abbr title="Test Automation Solution">TAS</abbr>.</li>
					<li>Defining usage guidelines: It is possible to write guidelines, checklists and FAQs for the usage of the <abbr title="Test Automation Solution">TAS</abbr>. This can prevent extensive support questions.</li>
					<li>Implementing a way to gather information about the actual use: There should be an automated way to gather information about the actual usage of the <abbr title="Test Automation Solution">TAS</abbr>. Ideally not only the usage itself, but also what parts of the <abbr title="Test Automation Solution">TAS</abbr> (certain functionalities) are being used. In this way, the usage of the <abbr title="Test Automation Solution">TAS</abbr> can be monitored easily.</li>
					<li>Monitoring <abbr title="Test Automation Solution">TAS</abbr> use, benefits and costs: Monitoring the usage of the <abbr title="Test Automation Solution">TAS</abbr> over a certain period of time indicates whether the <abbr title="Test Automation Solution">TAS</abbr> is indeed used. This information can also be used to re-calculate the business case (e.g., how much time has been saved, how many problems prevented).</li>
					<li>Providing support for the test and development teams for a given <abbr title="Test Automation Solution">TAS</abbr>.</li>
					<li>Gathering lessons learned from all teams: Perform evaluation/retrospective meetings with the different teams that use the <abbr title="Test Automation Solution">TAS</abbr>. In this way, lessons learned can be identified. The teams will feel that their input is necessary and wanted to improve the usage of the <abbr title="Test Automation Solution">TAS</abbr>.</li>
					<li>Identifying and implementing improvements: Based on the feedback of the team and the monitoring of the <abbr title="Test Automation Solution">TAS</abbr>, identify and implement steps for improvement. Also communicate this clearly to the stakeholders</li>
				</ul>
				<h3 id="4-1-3-deployment-of-the-tas-within-the-software-lifecycle"><a class="color-inherit" href="#4-1-3-deployment-of-the-tas-within-the-software-lifecycle">4.1.3 Deployment of the <abbr title="Test Automation Solution">TAS</abbr> Within the Software Lifecycle</a></h3>
				<p>The deployment of a <abbr title="Test Automation Solution">TAS</abbr> depends greatly on the phase of development of the software project which will be tested by the <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<p>Usually, a new <abbr title="Test Automation Solution">TAS</abbr> or a new version of it, is deployed either in the beginning of the project or when reaching a milestone, such as code freeze or the end of a sprint. This is because the deployment activities, with all the testing and modifications involved, require time and effort. Also this is a good way to mitigate the risk of the <abbr title="Test Automation Solution">TAS</abbr> not working and causing disruptions in the test automation process. However, if there are critical issues that need to be fixed for the <abbr title="Test Automation Solution">TAS</abbr> or if a component of the environment in which it runs needs to be replaced, then the deployment will be done independently from the development phase of the <abbr title="System Under Test">SUT</abbr>.</p>
				<h2 id="4-2-risk-assessment-and-mitigation-strategies"><a class="color-inherit" href="#4-2-risk-assessment-and-mitigation-strategies">4.2 Risk Assessment and Mitigation Strategies</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-4.2.1 (K4) Analyze deployment risks and identify technical issues that could lead to failure of the test automation project, and plan mitigation strategies</p>
				<p>Technical issues can lead to product- or project risks. Typical technical issues include:</p>
				<ul>
					<li>Too much abstraction can lead to difficulty in understanding what really happens (e.g., with keywords)</li>
					<li>Data-driven: data tables can become too large/complex/cumbersome</li>
					<li>Dependency of the <abbr title="Test Automation Solution">TAS</abbr> to use certain operating system libraries or other components that may not be available in all the target environments of the <abbr title="System Under Test">SUT</abbr></li>
				</ul>
				<p>Typical deployment project risks include:</p>
				<ul>
					<li>Staffing issues: getting the right people to maintain the code base may be difficult</li>
					<li>New <abbr title="System Under Test">SUT</abbr> deliverables may cause the <abbr title="Test Automation Solution">TAS</abbr> to operate incorrectly</li>
					<li>Delays in introducing automation</li>
					<li>Delays in updating <abbr title="Test Automation Solution">TAS</abbr> based on the changes done to the <abbr title="System Under Test">SUT</abbr></li>
					<li>The <abbr title="Test Automation Solution">TAS</abbr> cannot capture the (non-standard) objects it is intended to track</li>
				</ul>
				<p>Potential failure points of the <abbr title="Test Automation Solution">TAS</abbr> project include:</p>
				<ul>
					<li>Migration to a different environment</li>
					<li>Deployment to the target environment</li>
					<li>New delivery from development</li>
				</ul>
				<p>There are a number of risk mitigation strategies that can be employed to deal with these risk areas. These are discussed below.</p>
				<p>The <abbr title="Test Automation Solution">TAS</abbr> has a software lifecycle of its own, whether it is in-house developed or an acquired solution. One thing to remember is that the <abbr title="Test Automation Solution">TAS</abbr>, like any other software, needs to be under version control and its features documented. Otherwise, it becomes very difficult to deploy different parts of it and make them work together, or work in certain environments.</p>
				<p>Also, there has to be a documented, clear, and easy to follow deployment procedure. This procedure is version dependent; therefore, it has to be included under version control as well.</p>
				<p>There are two distinct cases when deploying a <abbr title="Test Automation Solution">TAS</abbr>:</p>
				<ol>
					<li>Initial deployment</li>
					<li>Maintenance deployment - <abbr title="Test Automation Solution">TAS</abbr> already exists and needs to be maintained</li>
				</ol>
				<p>Before starting with the first deployment of a <abbr title="Test Automation Solution">TAS</abbr>, it is important to be sure it can run in its own environment, it is isolated from random changes, and test cases can be updated and managed. Both the <abbr title="Test Automation Solution">TAS</abbr> and its infrastructure must be maintained.</p>
				<p>In the case of first time deployment, the following basic steps are needed:</p>
				<ul>
					<li>Define the infrastructure in which the <abbr title="Test Automation Solution">TAS</abbr> will run</li>
					<li>Create the infrastructure for the <abbr title="Test Automation Solution">TAS</abbr></li>
					<li>Create a procedure for maintaining the <abbr title="Test Automation Solution">TAS</abbr> and its infrastructure</li>
					<li>Create a procedure for maintaining the test suite that the <abbr title="Test Automation Solution">TAS</abbr> will execute</li>
				</ul>
				<p>The risks related to first time deployment include:</p>
				<ul>
					<li>Total execution time of the test suite may be longer than the planned execution time for the test cycle. In this case it is important to make sure that the test suite gets enough time to be executed entirely before the next scheduled test cycle begins.</li>
					<li>Installation and configuration issues with test environment exist (e.g., database setup and initial load, services start/stop). In general, the <abbr title="Test Automation Solution">TAS</abbr> needs an effective way to setup needed preconditions for the automated test cases within the test environment</li>
				</ul>
				<p>For maintenance deployments, there are additional considerations. The <abbr title="Test Automation Solution">TAS</abbr> in itself needs to evolve, and the updates for it have to be deployed into production. Before deploying an updated version of the <abbr title="Test Automation Solution">TAS</abbr> into production, it needs to be tested like any other software. It is therefore necessary to check the new functionality, to verify that the test suite can be run on the updated <abbr title="Test Automation Solution">TAS</abbr>, that reports can be sent, and that there are no performance issues or other functional regressions. In some cases the entire test suite may need to be changed to fit the new version of the <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<p>When maintenance deployment occurs, the following steps are needed:</p>
				<ul>
					<li>Make an assessment of the changes in the new version of the <abbr title="Test Automation Solution">TAS</abbr> compared to the old one</li>
					<li>Test the <abbr title="Test Automation Solution">TAS</abbr> for both new functionality and regressions</li>
					<li>Check if the test suite needs to be adapted to the new version of the <abbr title="Test Automation Solution">TAS</abbr></li>
				</ul>
				<p>An update also incurs the following risks and corresponding mitigation actions:</p>
				<ul>
					<li>The test suite needs to change to run on the updated <abbr title="Test Automation Solution">TAS</abbr>: make the necessary changes to the test suite and test them before deploying them on to the <abbr title="Test Automation Solution">TAS</abbr>.</li>
					<li>Stubs, drivers and interfaces used in testing need to change to fit with the updated <abbr title="Test Automation Solution">TAS</abbr>: make the necessary changes to the test harness and test it before deploying to the <abbr title="Test Automation Solution">TAS</abbr>.</li>
					<li>The infrastructure needs to change to accommodate the updated <abbr title="Test Automation Solution">TAS</abbr>: make an assessment of the infrastructure components that need to be changed, perform the changes and test them with the updated <abbr title="Test Automation Solution">TAS</abbr>.</li>
					<li>The updated <abbr title="Test Automation Solution">TAS</abbr> has additional defects or performance issues: perform an analysis of risks vs. benefits. If the issues discovered make it impossible to update the <abbr title="Test Automation Solution">TAS</abbr>, it may be best not to proceed with the update or to wait for a next version of the <abbr title="Test Automation Solution">TAS</abbr>. If the issues are negligible compared to the benefits, the <abbr title="Test Automation Solution">TAS</abbr> can still be updated. Be sure to create a release note with known issues to notify the test automation engineers and other stakeholders and try to get an estimate on when the issues are going to be fixed.</li>
				</ul>
				<h2 id="4-3-test-automation-maintenance"><a class="color-inherit" href="#4-3-test-automation-maintenance">4.3 Test Automation Maintenance</a></h2>
				<p>Developing test automation solutions is not trivial. They need to be modular, scalable, understandable, reliable, and testable. To add even more complexity, test automation solutions â€“ like any other software system, have to evolve. Whether due to internal changes or changes in the environment in which they operate, maintenance is an important aspect of architecting a <abbr title="Test Automation Solution">TAS</abbr>. Maintaining the <abbr title="Test Automation Solution">TAS</abbr> by adapting it to new types of systems to be tested, by accommodating support for new software environments, or by making it compliant to new laws and regulations, helps to ensure reliable and safe operation of the <abbr title="Test Automation Solution">TAS</abbr>. It also optimizes the life span and the performance of the <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<h3 id="4-3-1-types-of-maintenance"><a class="color-inherit" href="#4-3-1-types-of-maintenance">4.3.1 Types of Maintenance</a></h3>
				<p class="learning-objective color-secondary">ALTA-E-4.3.1 (K2) Understand which factors support and affect <abbr title="Test Automation Solution">TAS</abbr> maintainability</p>
				<p>Maintenance is done on an existing operational <abbr title="Test Automation Solution">TAS</abbr> and is triggered by modifications, migration, or retirement of the system. This process can be structured in the following categories:</p>
				<ul>
					<li>Preventive maintenance - Changes are made to make the <abbr title="Test Automation Solution">TAS</abbr> support more test types, test on multiple interfaces, test multiple versions of the <abbr title="System Under Test">SUT</abbr> or support test automation for a new <abbr title="System Under Test">SUT</abbr>.</li>
					<li>Corrective maintenance - Changes are made to correct failures of the <abbr title="Test Automation Solution">TAS</abbr>. The best way to maintain a <abbr title="Test Automation Solution">TAS</abbr> in operation, thus reducing the risk in using it, is through the execution of regular maintenance tests.</li>
					<li>Perfective maintenance - The <abbr title="Test Automation Solution">TAS</abbr> is optimized and non-functional issues are fixed. They can address the performance of the <abbr title="Test Automation Solution">TAS</abbr>, its usability, robustness or reliability.</li>
					<li>Adaptive maintenance - As new software systems are launched in the market (operating systems, database managers, web browsers, etc.), it may be required that the <abbr title="Test Automation Solution">TAS</abbr> supports them. Also, it may be the case that the <abbr title="Test Automation Solution">TAS</abbr> needs to comply with new laws, regulations or industry-specific requirements. In this case, changes are made to the <abbr title="Test Automation Solution">TAS</abbr> to make it adapt accordingly. Note: usually, conformance to laws and regulations creates mandatory maintenance with specific rules, requirements and sometimes auditing requirements. Also, as integrating tools are updated and new versions created, tool integration endpoints need to be maintained and kept functional.</li>
				</ul>
				<h3 id="4-3-2-scope-and-approach"><a class="color-inherit" href="#4-3-2-scope-and-approach">4.3.2 Scope and Approach</a></h3>
				<p>Maintenance is a process that can affect all layers and components of a <abbr title="Test Automation Solution">TAS</abbr>. The scope of it depends on:</p>
				<ul>
					<li>The size and complexity of the <abbr title="Test Automation Solution">TAS</abbr></li>
					<li>The size of the change</li>
					<li>The risk of the change</li>
				</ul>
				<p>Given the fact that maintenance refers to <abbr title="Test Automation Solution">TAS</abbr> in operation, an impact analysis is necessary to determine how the system may be affected by the changes. Depending on the impact, the changes need to be introduced incrementally and tests need to be carried out after each step to ensure the continuous functioning of the <abbr title="Test Automation Solution">TAS</abbr>. Note: maintaining the <abbr title="Test Automation Solution">TAS</abbr> can be difficult if its specifications and documentation are outdated.</p>
				<p>Because time efficiency is the main contributing factor to the success of test automation, it becomes critical to have good practices for maintaining the <abbr title="Test Automation Solution">TAS</abbr> including:</p>
				<ul>
					<li>The deployment procedures and usage of the <abbr title="Test Automation Solution">TAS</abbr> must be clear and documented</li>
					<li>The third party dependencies must be documented, together with drawbacks and known issues</li>
					<li>The <abbr title="Test Automation Solution">TAS</abbr> must be modular, so parts of it can be easily replaced</li>
					<li>The <abbr title="Test Automation Solution">TAS</abbr> must run in an environment that is replaceable or with replaceable components</li>
					<li>The <abbr title="Test Automation Solution">TAS</abbr> must separate test scripts from the <abbr title="Test Automation Framework">TAF</abbr> itself</li>
					<li>The <abbr title="Test Automation Solution">TAS</abbr> must run isolated from the development environment, so that changes to the <abbr title="Test Automation Solution">TAS</abbr> will not adversely affect the test environment</li>
					<li>The <abbr title="Test Automation Solution">TAS</abbr> together with the environment, test suite and testware artifacts must be under configuration management</li>
				</ul>
				<p>There are also considerations for the maintenance of the third party components and other libraries as follows:</p>
				<ul>
					<li>Very often it is the case that the <abbr title="Test Automation Solution">TAS</abbr> will use third party components to run the tests. It may also be the case that the <abbr title="Test Automation Solution">TAS</abbr> depends on third party libraries (e.g., the <abbr title="User Interface">UI</abbr> automation libraries). All the third party component parts of the <abbr title="Test Automation Solution">TAS</abbr> must be documented and under configuration management.</li>
					<li>It is necessary to have a plan in case these external components need to be modified or fixed. The person responsible for the <abbr title="Test Automation Solution">TAS</abbr> maintenance needs to know who to contact or where to submit an issue.</li>
					<li>There must be documentation regarding the license under which the third party components are used, so that there is information on whether they can be modified, to what degree and by whom.</li>
					<li>For each of the third party components, it is necessary to get information about updates and new versions. Keeping the third party components and libraries up to date is a preventive action that pays off the investment in the long-term.</li>
				</ul>
				<p>Considerations for naming standards and other conventions include:</p>
				<ul>
					<li>The idea of naming standards and other conventions has a simple reason: the test suite and the <abbr title="Test Automation Solution">TAS</abbr> itself has to be easy to read, understand, change and maintain. This saves time in the maintenance process and also minimizes the risk of introducing regressions or wrong fixes that could otherwise be easily avoided.</li>
					<li>It is easier to introduce new people to the test automation project when standard naming conventions are used.</li>
					<li>The naming standards can refer to variables and files, test scenarios, keywords and keyword parameters. Other conventions refer to pre-requisites and post-actions for test execution, the content of the test data, the test environment, status of test execution, and execution logs and reports.</li>
					<li>All the standards and conventions must be agreed upon and documented when starting a test automation project.</li>
				</ul>
				<p>Documentation considerations include:</p>
				<ul>
					<li>The need for good and current documentation for both the test scenarios and the <abbr title="Test Automation Solution">TAS</abbr> is quite clear, but there are two issues related to this: someone has to write it and someone has to maintain it.</li>
					<li>While the code of the test tool can be either self-documenting or semi-automatically documented, all the design, components, integrations with third parties, dependencies and deployment procedures need to be documented by someone.</li>
					<li>It is a good practice to introduce the writing of documentation as part of the development process. A task should not be considered as done unless it is documented or the documentation is updated.</li>
				</ul>
				<p>Training material considerations include:</p>
				<ul>
					<li>If the documentation for the <abbr title="Test Automation Solution">TAS</abbr> is well-written, it can be used as a basis for the training material of the <abbr title="Test Automation Solution">TAS</abbr>.</li>
					<li>The training material is a combination of functional specifications of the <abbr title="Test Automation Solution">TAS</abbr>, design and architecture of the <abbr title="Test Automation Solution">TAS</abbr>, deployment and maintenance of the <abbr title="Test Automation Solution">TAS</abbr>, usage of the <abbr title="Test Automation Solution">TAS</abbr> (user manual), practical examples and exercises, and tips and tricks.</li>
					<li>The maintenance of the training material consists of initially writing it and then reviewing it periodically. It is done in practice by the team members designated as trainers on the <abbr title="Test Automation Solution">TAS</abbr> and it most likely happens towards the end of a lifecycle iteration of the <abbr title="System Under Test">SUT</abbr> (at the end of sprints, for instance).</li>
				</ul>
			</article>

			<article id="5-test-automation-reporting-and-metrics">
				<h1><a class="color-inherit" href="#5-test-automation-reporting-and-metrics">5 Test Automation Reporting and Metrics</a></h1>
				<h2 id="5-1-selection-of-tas-metrics"><a class="color-inherit" href="#5-1-selection-of-tas-metrics">5.1 Selection of <abbr title="Test Automation Solution">TAS</abbr> Metrics</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-5.1.1 (K2) Classify metrics that can be used to monitor the test automation strategy and effectiveness</p>
				<p>This section focuses on the metrics that can be used to monitor the test automation strategy and the effectiveness and efficiency of the <abbr title="Test Automation Solution">TAS</abbr>. These are separate from the <abbr title="System Under Test">SUT</abbr> related metrics used to monitor the <abbr title="System Under Test">SUT</abbr> and the (functional and non-functional) testing of the <abbr title="System Under Test">SUT</abbr>. Those are selected by the projectâ€™s overall Test Manager. Test automation metrics allow the <abbr title="Test Automation Manager">TAM</abbr> and <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> to track progress toward the goals for test automation and to monitor the impact of changes made to the test automation solution.</p>
				<p>The <abbr title="Test Automation Solution">TAS</abbr> metrics can be divided into two groups: external and internal. The external metrics are those used to measure the <abbr title="Test Automation Solution">TAS</abbr>â€™s impact on other activities (in particular the testing activities). The internal metrics are those used to measure the effectiveness and efficiency of the <abbr title="Test Automation Solution">TAS</abbr> in fulfilling its objectives.</p>
				<p>The measured <abbr title="Test Automation Solution">TAS</abbr> metrics typically include the following:</p>
				<ul>
					<li>
						External <abbr title="Test Automation Solution">TAS</abbr> metrics
						<ul>
							<li>Automation benefits</li>
							<li>Effort to build automated tests</li>
							<li>Effort to analyze automated test incidents</li>
							<li>Effort to maintain automated tests</li>
							<li>Ratio of failures to defects</li>
							<li>Time to execute automated tests</li>
							<li>Number of automated test cases</li>
							<li>Number of pass and fail results</li>
							<li>Number of false-fail and false-pass results</li>
							<li>Code coverage</li>
						</ul>
					</li>
					<li>
						Internal <abbr title="Test Automation Solution">TAS</abbr> metrics
						<ul>
							<li>Tool scripting metrics</li>
							<li>Automation code defect density</li>
							<li>Speed and efficiency of <abbr title="Test Automation Solution">TAS</abbr> components</li>
						</ul>
					</li>
				</ul>
				<p>These are each described below.</p>
				<p><strong>Automation benefits</strong></p>
				<p>It is particularly important to measure and report the benefits of a <abbr title="Test Automation Solution">TAS</abbr>. This is because the costs (in terms of the number of people involved over a given period of time) are easy to see. People working outside testing will be able to form an impression of the overall cost but may not see the benefits achieved.</p>
				<p>Any measure of benefit will depend on the objective of the <abbr title="Test Automation Solution">TAS</abbr>. Typically this may be a savings of time or effort, an increase in the amount of testing performed (breadth or depth of coverage, or frequency of execution), or some other advantage such as increased repeatability, greater use of resources, or fewer manual errors. Possible measures include:</p>
				<ul>
					<li>Number of hours of manual test effort saved</li>
					<li>Reduction in time to perform regression testing</li>
					<li>Number of additional cycles of test execution achieved</li>
					<li>Number or percentage of additional tests executed</li>
					<li>Percentage of automated test cases related to the entire set of test cases (although automated cannot easily be compared to manual test cases)</li>
					<li>Increase in coverage (requirements, functionality, structural)</li>
					<li>Number of defects found earlier because of the <abbr title="Test Automation Solution">TAS</abbr> (when the average benefit of defects found earlier is known, this can be "calculated" to a sum of prevented costs)</li>
					<li>Number of defects found because of the <abbr title="Test Automation Solution">TAS</abbr> which would not have been found by manual testing (e.g., reliability defects)</li>
				</ul>
				<p>Note that test automation generally saves manual test effort. This effort can be devoted to other kinds of (manual) testing (e.g., exploratory testing). Defects found by these additional tests can also be seen as indirect benefits of the <abbr title="Test Automation Solution">TAS</abbr>, as the test automation enabled these manual tests to be executed. Without the <abbr title="Test Automation Solution">TAS</abbr> these tests would not have been executed and subsequently the additional defects would not have been found.</p>
				<p><strong>Effort to build automated tests</strong></p>
				<p>The effort to automate tests is one of the key costs associated with test automation. This is often more than the cost of running the same test manually and therefore can be a detriment to expanding the use of test automation. While the cost to implement a specific automated test will depend largely on the test itself, other factors such as the scripting approach used, familiarity with the test tool, the environment, and the skill level of the test automation engineer will also have an impact.</p>
				<p>Because larger or more complex tests typically take longer to automate than short or simple tests, computing the build cost for test automation may be based on an average build time. This may be further refined by considering the average cost for a specific set of tests such as those targeting the same function or those at a given test level. Another approach is to express the build cost as a factor of the effort required to run the test manually (equivalent manual test effort, <abbr title="Equivalent manual test effor">EMTE</abbr>). For example, it may be that it takes twice the manual test effort to automate a test case, or two times the <abbr title="Equivalent manual test effor">EMTE</abbr>.</p>
				<p><strong>Effort to analyze <abbr title="System Under Test">SUT</abbr> failures</strong></p>
				<p>Analyzing failures in <abbr title="System Under Test">SUT</abbr> discovered through automated test execution can be significantly more complex than for a manually executed test because the events leading up to the failure of a manual test are often known by the tester running the test. This can be mitigated as described in design level in Chapter 3.1.4 and in reporting level in Chapters 5.3 & 5.4. This measure can be expressed as an average per failed test case or it may be expressed as a factor of <abbr title="Equivalent manual test effor">EMTE</abbr>. The latter being particularly suitable where the automated tests vary significantly in complexity and execution length.</p>
				<p>The available logging of the <abbr title="System Under Test">SUT</abbr> and the <abbr title="Test Automation Solution">TAS</abbr> play a crucial role in analyzing failures. The logging should provide enough information to perform this analysis efficiently. Important logging features include:</p>
				<ul>
					<li><abbr title="System Under Test">SUT</abbr> logging and <abbr title="Test Automation Solution">TAS</abbr> logging should be synchronized</li>
					<li>The <abbr title="Test Automation Solution">TAS</abbr> should log the expected and actual behavior</li>
					<li>The <abbr title="Test Automation Solution">TAS</abbr> should log the actions to be performed</li>
				</ul>
				<p>The <abbr title="System Under Test">SUT</abbr>, on the other hand, should log all actions that are performed (regardless of whether the action is the result of manual or automated testing). Any internal errors should be logged and any crash dumps and stack traces should be available.</p>
				<p><strong>Effort to maintain automated tests</strong></p>
				<p>The maintenance effort required to keep automated tests in sync with the <abbr title="System Under Test">SUT</abbr> can be very significant and ultimately may outweigh the benefits achieved by the <abbr title="Test Automation Solution">TAS</abbr>. This has been the cause of failure for many automation efforts. Monitoring the maintenance effort is therefore important to highlight when steps need to be taken to reduce the maintenance effort or at least prevent it from growing unchecked.</p>
				<p>Measures of maintenance effort can be expressed as a total for all the automated tests requiring maintenance for each new release of the <abbr title="System Under Test">SUT</abbr>. They may also be expressed as an average per updated automated test or as a factor of <abbr title="Equivalent manual test effor">EMTE</abbr>.</p>
				<p>A related metric is the number or percentage of tests requiring maintenance work.</p>
				<p>When maintenance effort for automated tests is known (or can be derived), this information can play a crucial role in deciding whether or not to implement certain functionality or to fix a certain defect. The effort required to maintain the test case due to the changed software should be considered with the change of the <abbr title="System Under Test">SUT</abbr>.</p>
				<p><strong>Ratio of failures to defects</strong></p>
				<p>A common problem with automated tests is that many of them can fail for the same reason â€“ a single defect in the software. While the purpose of tests is to highlight defects in the software, having more than one test highlight the same defect is wasteful. This is particularly the case for automated testing as the effort required to analyze each failed test can be significant. Measuring the number of automated tests that fail for a given defect can help indicate where this may be a problem. The solution lies in the design of the automated tests and their selection for execution</p>
				<p><strong>Time to execute automated tests</strong></p>
				<p>One of the easier metrics to determine is the time it takes to execute the automated tests. In the beginning of the <abbr title="Test Automation Solution">TAS</abbr> this might not be important, but as the number of automated test cases increases, this metric may become quite important.</p>
				<p><strong>Number of automated test cases</strong></p>
				<p>This metric can be used to show the progression made by the test automation project. But one has to take into account that just the number of automated test cases does not reveal a lot of information; for example, it does not indicate that the test coverage has increased.</p>
				<p><strong>Number of pass and fail results</strong></p>
				<p>This is a common metric and tracks how many automated tests passed and how many failed to achieve the expected result. Failures have to be analyzed to determine if the failure was due to a defect in the <abbr title="System Under Test">SUT</abbr> or was due to external issues such as a problem with the environment or with the <abbr title="Test Automation Solution">TAS</abbr> itself.</p>
				<p><strong>Number of false-fail and false-pass results</strong></p>
				<p>As was seen in several previous metrics, it can take quite some time to analyze test failures. This is even more frustrating when it turns out to be a false alarm. This happens when the problem is in the <abbr title="Test Automation Solution">TAS</abbr> or test case but not in the <abbr title="System Under Test">SUT</abbr>. It is important that the number of false alarms (and the potentially wasted effort) are kept low. False-fails can reduce confidence in the <abbr title="Test Automation Solution">TAS</abbr>. Conversely, false-pass results may be more dangerous. When a false-pass occurs, there was a failure in the <abbr title="System Under Test">SUT</abbr>, but it was not identified by the test automation so a pass result was reported. In this case, a potential defect may escape detection. This can occur because the verification of the outcome was not done properly, an invalid test oracle was used or the test case was expecting the wrong result.</p>
				<p>Note that false alarms can be caused by defects in the test code (see metric "Automation code defect density") but may also be caused by an unstable <abbr title="System Under Test">SUT</abbr> that is behaving in an unpredictable manner (e.g., timing out). Test hooks can also cause false alarms due to the level of intrusion they are causing.</p>
				<p><strong>Code coverage</strong></p>
				<p>Knowing the <abbr title="System Under Test">SUT</abbr> code coverage provided by the different test cases can reveal useful information. This can also be measured at a high level, e.g., the code coverage of the regression test suite. There is no absolute percentage that indicates adequate coverage, and 100% code coverage is unattainable in anything other than the simplest of software applications. However, it is generally agreed that more coverage is better as it reduces overall risk of software deployment.  This metric can indicate activity in the <abbr title="System Under Test">SUT</abbr> as well. For example, if the code coverage drops, this most likely means that functionality has been added to the <abbr title="System Under Test">SUT</abbr>, but no corresponding test case has been added to the automated test suite.</p>
				<p><strong>Tool scripting metrics</strong></p>
				<p>There are many metrics that can be used to monitor automation script development. Most of these are similar to source code metrics for the <abbr title="System Under Test">SUT</abbr>. Lines of code (LOC) and cyclomatic complexity can be used to highlight overly large or complex scripts (suggesting possible redesign is needed).</p>
				<p>The ratio of comments to executable statements can be used to give a possible indication of the extent of script documentation and annotation. The number of non-conformances to scripting standards can give an indication of the extent to which those standards are being followed.</p>
				<p><strong>Automation code defect density</strong></p>
				<p>Automation code is no different than the code of the <abbr title="System Under Test">SUT</abbr> in that it is software and will contain defects. Automation code should not be considered less important than the <abbr title="System Under Test">SUT</abbr> code. Good coding practices and standards should be applied and the result of these monitored by metrics such as code defect density. These will be easier to collect with the support of a configuration management system.</p>
				<p><strong>Speed and efficiency of <abbr title="Test Automation Solution">TAS</abbr> components</strong></p>
				<p>Differences in the time it takes to perform the same test steps in the same environment can indicate a problem in the <abbr title="System Under Test">SUT</abbr>. If the <abbr title="System Under Test">SUT</abbr> is not performing the same functionality in the same elapsed time, investigation is needed. This may indicate a variability in the system that is not acceptable and one that could worsen with increased load The <abbr title="Test Automation Solution">TAS</abbr> needs to be performing well enough for it not to hinder the performance of <abbr title="System Under Test">SUT</abbr>. If the performance is critical requirement for the <abbr title="System Under Test">SUT</abbr> then the <abbr title="Test Automation Solution">TAS</abbr> needs to be designed in a way that takes this into account.</p>
				<p><strong>Trend metrics</strong></p>
				<p>With many of these metrics it is the trends (i.e., the way in which the measures change over time) that may be more valuable to report than the value of a measure at a specific time. For example, knowing that the average maintenance cost per automated test requiring maintenance is more than it was for the previous two releases of the <abbr title="System Under Test">SUT</abbr> may prompt action to determine the cause of the increase and undertake steps to reverse the trend.</p>
				<p>The cost of measuring should be as low as possible and this can often be achieved by automating the collection and reporting.</p>
				<h2 id="5-2-implementation-of-measurement"><a class="color-inherit" href="#5-2-implementation-of-measurement">5.2 Implementation of Measurement</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-5.2.1 (K3) Implement metrics collection methods to support technical and management requirements. Explain how measurement of the test automation can be implemented.</p>
				<p>Since a test automation strategy has automated testware at its core, the automated testware can be enhanced to record information about its use. Where abstraction is combined with structured testware, any enhancements made to the underlying testware can be utilized by all of the higher level automated test scripts. For example, enhancing the underlying testware to record the start and end time of execution for a test may well apply to all tests.</p>
				<p><strong>Features of automation that support measurement and report generation</strong></p>
				<p>The scripting languages of many test tools support measurement and reporting through facilities that can be used to record and log information before, during, and after test execution of individual tests, sets of tests and an entire test suite.</p>
				<p>The reporting on each of a series of test runs needs to have in place an analysis feature to take into account the results of the previous test runs so it can highlight trends (such as changes in the test success rate).</p>
				<p>Automating testing typically requires automation of both the test execution and the test verification, the latter being achieved by comparing specific elements of the test outcome with a pre-defined expected outcome. This comparison is generally best undertaken by a test tool. The level of information that is reported as a result of this comparison must be considered. It is important that the status of the test be determined correctly (e.g., pass, fail). In the case of a failed status, more information about the cause of the failure will be required (e.g., screen shots).</p>
				<p>Distinguishing between expected differences in the actual and expected outcome of a test is not always trivial though tool support can help greatly in defining comparisons that ignore the expected differences (such as dates and times) while highlighting any unexpected differences.</p>
				<p><strong>Integration with other third party tools (spreadsheets, XML, documents, databases, report tools, etc.)</strong></p>
				<p>When information from the execution of automated test cases is used in other tools (for tracking and reporting, e.g., updating traceability matrix), it is possible to provide the information in a format that is suitable for these third party tools. This is often achieved through existing test tool functionality (export formats for reporting) or by creating customized reporting that is output in a format consistent with other programs (â€œ.xlsâ€ for Excel, â€œ.docâ€ for Word, â€œ.htmlâ€ for Web, etc.).</p>
				<p><strong>Visualization of results (dashboards, charts, graphs, etc.)</strong></p>
				<p>Test results should be made visible in charts. Consider using colors to indicate problems in the test execution such as traffic lights to indicate the progress of the test execution / automation so that decisions can be made based on reported information. Management is particularly interested in visual summaries to see the test result in one glance; in case more information is needed, they can still dive in to the details.</p>
				<h2 id="5-3-logging-of-the-tas-and-the-sut"><a class="color-inherit" href="#5-3-logging-of-the-tas-and-the-sut">5.3 Logging of the <abbr title="Test Automation Solution">TAS</abbr> and the <abbr title="System Under Test">SUT</abbr></a></h2>
				<p class="learning-objective color-secondary">ALTA-E-5.3.1 (K4) Analyze test logging of both <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr> data</p>
				<p>Logging is very important in the <abbr title="Test Automation Solution">TAS</abbr>, including logging for both the test automation itself and the <abbr title="System Under Test">SUT</abbr>. Test logs are a source that frequently are used to analyze potential problems. In the following section are examples of test logging, categorized by <abbr title="Test Automation Solution">TAS</abbr> or <abbr title="System Under Test">SUT</abbr>.</p>
				<p><abbr title="Test Automation Solution">TAS</abbr> logging (whether the <abbr title="Test Automation Framework">TAF</abbr> or the test case itself logs the information is not so important and depends on the context) should include the following:</p>
				<ul>
					<li>Which test case is currently under execution, including start and end time.</li>
					<li>The status of the test case execution because, while failures can easily be identified in log files, the framework itself should also have this information and should report via a dashboard. The execution status of the test case can be pass, fail or <abbr title="Test Automation Solution">TAS</abbr> error. The result of <abbr title="Test Automation Solution">TAS</abbr> error is used for situations where the problem is not in the <abbr title="System Under Test">SUT</abbr>.</li>
					<li>Details of the test log at a high level (logging significant steps) including timing information.</li>
					<li>Dynamic information about the <abbr title="System Under Test">SUT</abbr> (e.g., memory leaks) that the test case was able to identify with the help of third party tools. Actual results and failures of these dynamic measurements should be logged with the test case that was executing when the incident was detected.</li>
					<li>In the case of reliability testing / stress testing (where numerous cycles are performed) a counter should be logged, so it can be easily determined how many times test cases have been executed.</li>
					<li>When test cases have random parts (e.g., random parameters, or random steps in state-machine testing), the random number/choices should be logged.</li>
					<li>All actions a test case performs should be logged in such a way that the log file (or parts of it) can be played back to re-execute the test with exactly the same steps and the same timing. This is useful to check for the reproducibility of an identified failure and to capture additional information. The test case action information could also be logged on the <abbr title="System Under Test">SUT</abbr> itself for use when reproducing customer-identified issues (the customer runs the scenario, the log information is captured and can then be replayed by the development team when troubleshooting the issue).</li>
					<li>Screenshots and other visual captures can be saved during test execution for further use during failure analysis</li>
					<li>Whenever a test case encounters a failure, the <abbr title="Test Automation Solution">TAS</abbr> should make sure that all information needed to analyze the problem is available/stored, as well as any information regarding the continuation of testing, if applicable. Any associated crash dumps and stack traces should be saved by the <abbr title="Test Automation Solution">TAS</abbr> to a safe location. Also any log files which could be overwritten (cyclic buffers are often used for log files on the <abbr title="System Under Test">SUT</abbr>) should be copied to this location where they will be available for later analysis.</li>
					<li>Use of color can help to distinguish different types of logged information (e.g., errors in red, progress information in green).</li>
				</ul>
				<p><abbr title="System Under Test">SUT</abbr> logging:</p>
				<ul>
					<li>When the <abbr title="System Under Test">SUT</abbr> identifies a problem, all necessary information needed to analyze the issue should be logged, including date and time stamps, source location of issue, error messages, etc.</li>
					<li>The <abbr title="System Under Test">SUT</abbr> can log all user interaction (directly via the available user interface, but also via network interfaces, etc.). In this way issues identified by customers can be analyzed properly, and development can try to reproduce the problem.</li>
					<li>At startup of the system, configuration information should be logged to a file, consisting of the different software/firmware versions, configuration of the <abbr title="System Under Test">SUT</abbr>, configuration of the operating system, etc.</li>
				</ul>
				<p>All the different logging information should be easily searchable. A problem identified in the log file by the <abbr title="Test Automation Solution">TAS</abbr> should be easily identified in the log file of the <abbr title="System Under Test">SUT</abbr>, and vice versa (with or without additional tooling). Synchronizing various logs with a time stamp facilitates correlation of what occurred when an error was reported.</p>
				<h2 id="5-4-test-automation-reporting"><a class="color-inherit" href="#5-4-test-automation-reporting">5.4 Test Automation Reporting</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-5.4.1 (K2) Explain how a test execution report is constructed and published</p>
				<p>The test logs give detailed information about the execution steps, actions and responses of a test case and/or test suite. However, the logs alone cannot provide a good overview of the overall execution result. For this, it is necessary to have in place reporting functionality. After each execution of the test suite, a concise report must be created and published. A reusable report generator component could be used for this.</p>
				<p><strong>Content of the reports</strong></p>
				<p>The test execution report must contain a summary giving an overview of the execution results, the system being tested and the environment in which the tests were run which is appropriate for each of the stakeholders.</p>
				<p>It is necessary to know which tests have failed and the reasons for failure. To make troubleshooting easier, it is important to know the history of the execution of the test and who is responsible for it (generally the person who created or last updated it). The responsible person needs to investigate the cause of failure, report the issues related to it, follow-up on the fix of the issue(s), and check that the fix has been correctly implemented.</p>
				<p>Reporting is also used to diagnose any failures of the <abbr title="Test Automation Framework">TAF</abbr> components (see Chapter 7).</p>
				<p><strong>Publishing the reports</strong></p>
				<p>The report should be published for everyone interested in the execution results. It can be uploaded on a website, sent to a mailing list or uploaded to another tool such as a test management tool. From a practical side, it is most likely that the ones interested in the execution result will look at it and analyze it if they are given a subscription facility and can receive the report by email.</p>
				<p>Option is to identify problematic parts of the <abbr title="System Under Test">SUT</abbr>, is to keep a history of the reports, so that statistics about test cases or test suites with frequent regressions can be gathered.</p>
			</article>

			<article id="6-transitioning-manual-testing-to-an-automated-environment">
				<h1><a class="color-inherit" href="#6-transitioning-manual-testing-to-an-automated-environment">6 Transitioning Manual Testing to an Automated Environment</a></h1>
				<h2 id="6-1-criteria-for-automation"><a class="color-inherit" href="#6-1-criteria-for-automation">6.1 Criteria for Automation</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-6.1.1 (K3) Apply criteria for determining the suitability of tests for automation</p>
				<p class="learning-objective color-secondary">ALTA-E-6.1.2 (K2) Understand the factors in transitioning from manual to automation testing</p>
				<p>Traditionally, organizations have developed manual test cases. When deciding to migrate toward an automated test environment, one must evaluate the current state of manual tests and determine the most effective approach to automating these testing assets. The existing structure of a manual test may or may not be suited for automation, in which case a complete re-write of the test to support automation may be necessary. Alternatively, relevant components of existing manual tests (e.g., input values, expected results, navigational path) may be extracted from existing manual tests and reused for automation. A manual test strategy that takes into account automation will allow for tests whose structure facilitates migration to automation.</p>
				<p>Not all tests can or should be automated, and sometimes the first iteration of a test may be manual. Therefore there are two aspects of transitioning to consider: the initial conversion of existing manual tests to automation, and the subsequent transition of new manual tests to automation</p>
				<p>Also note that certain test types can only be executed (effectively) in an automated way, e.g., reliability tests, stress tests, or performance tests.</p>
				<p>With test automation it is possible to test applications and systems without a user interface. In this case, testing can be done on the integration level via interfaces in the software. While these kinds of test cases could also be executed manually (using manually entered commands to trigger the interfaces), this may not be practical. For example, with automation it may be possible to insert messages in a message queue system. In this way testing can start earlier (and can identify defects earlier), when manual testing is not yet possible</p>
				<p>Prior to commencing an automated testing effort, one needs to consider the applicability and viability of creating automated vs. manual tests. The suitability criteria may include, but are not limited to:</p>
				<ul>
					<li>Frequency of use</li>
					<li>Complexity to automate</li>
					<li>Compatibility of tool support</li>
					<li>Maturity of test process</li>
					<li>Suitability of automation for the stage of the software product lifecycle</li>
					<li>Sustainability of the automated environment</li>
					<li>Controllability of the <abbr title="System Under Test">SUT</abbr></li>
				</ul>
				<p>Each of these is explained in more detail below</p>
				<p><strong>Frequency of use</strong></p>
				<p>How often a test needs to be run is one consideration as to the viability of whether or not to automate. Tests that are run more regularly, as a part of a major or minor release cycle, are better candidates for automation as they will be used frequently. As a general rule, the greater the number of application releasesâ€”and therefore corresponding test cyclesâ€”the greater the benefit of automating tests. As functional tests become automated, they can be used in subsequent releases as a part of regression testing. Automated tests used in regression testing will provide high return on investment (ROI) and risk mitigation for the existing code base.</p>
				<p>If a test script is run once a year, and the <abbr title="System Under Test">SUT</abbr> changes within the year, it may not be feasible or efficient to create an automated test. The time it might take to adapt the test on a yearly basis to conform to the <abbr title="System Under Test">SUT</abbr> might be best done manually</p>
				<p><strong>Complexity to automate</strong></p>
				<p>In the cases where a complex system needs to be tested, there may be a tremendous benefit from automation to spare the manual tester the difficult task of having to repeat complex steps which are tedious, time-consuming, and error-prone to execute</p>
				<p>However, certain test scripts may be difficult or not cost-effective to automate. There is a range of factors that might affect this, including: an <abbr title="System Under Test">SUT</abbr> that is not compatible with existing available automated test solutions; the requirement to produce substantial program code and develop calls to APIs in order to automate; the multiplicity of systems that need to be addressed as part of a test execution; the interaction with external interfaces and/or proprietary systems; some aspects of usability testing; the amount of time needed to test the automation scripts, etc.</p>
				<p><strong>Compatibility and tool support</strong></p>
				<p>There is a wide range of development platforms used to create applications. The challenge to the tester is to know what available test tools exist (if any) to support any given platform, and to what extent the platform is supported. Organizations use a variety of testing tools, including those from commercial vendors, open source, and in-house developed. Each organization will have different needs and resources to support test tools. Commercial vendors typically provide for paid support, and in the case of the market leaders, usually have an eco-system of experts who can assist with test tool implementation. Open source tools may offer support such as online forums from which users can get information and post questions. In-house developed test tools rely on existing staff to provide support.</p>
				<p>The issue of test tool compatibility should not be underestimated. Embarking on a project of test automation without fully understanding the level of compatibility between test tools and the <abbr title="System Under Test">SUT</abbr> can have disastrous results. Even if most of the tests for the <abbr title="System Under Test">SUT</abbr> can be automated, there might be the situation where the most critical tests cannot.</p>
				<p><strong>Maturity of test process</strong></p>
				<p>In order to effectively implement automation within a test process, that process must be structured, disciplined and repeatable. Automation brings an entire development process into the existing testing process which requires managing the automation code and related components.</p>
				<p><strong>Suitability of automation for the stage of the software product lifecycle</strong></p>
				<p>An <abbr title="System Under Test">SUT</abbr> has a product lifecycle which can span from years to decades. As the development of a system begins, the system changes and expands to address defects and add refinements to meet end user needs. In the early stages of a systemâ€™s development, change may be too rapid to implement an automated testing solution. As screen layouts and controls are optimized and enhanced, creating automation in a dynamically changing environment may require continuous re-work, which is not efficient or effective. This would be similar to trying to change a tire on a moving car; itâ€™s better to wait for the car to stop. For large systems in a sequential development environment, when a system has stabilized and includes a core of functionality, this then becomes the best time to begin the implementation of automated tests.</p>
				<p>Over time, systems reach the end of their product lifecycles, and are either retired or redesigned to use newer and more efficient technology. Automation is not recommended for a system nearing the end of its lifecycle as there will be little value in undertaking such a short-lived initiative. However, for systems that are being redesigned using a different architecture but preserving the existing functionality, an automated testing environment which defines data elements will be equally useful in the old and new systems. In this case, reuse of test data would be possible and recoding of the automated environment to be compatible with the new architecture would be necessary.</p>
				<p><strong>Sustainability of the environment</strong></p>
				<p>A test environment for automation needs to be flexible and adaptable to the changes that will occur to the <abbr title="System Under Test">SUT</abbr> over time. This includes the ability to rapidly diagnose and correct problems with automation, the ease with which automation components can be maintained, and the facility with which new features and support can be added into the automated environment. These attributes are an integral part of the overall design and implementation of the <abbr title="Generic Test Automation Architecture">gTAA</abbr>.</p>
				<p><strong>Controllability of the <abbr title="System Under Test">SUT</abbr> (preconditions, setup and stability)</strong></p>
				<p>The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> should identify control and visibility characteristics in the <abbr title="System Under Test">SUT</abbr> that will aid in the creation of effective automated tests. Otherwise the test automation relies on <abbr title="User Interface">UI</abbr> interactions only, resulting in an less maintainable test automation solution. See Section 2.3 on Design for Testability and Automation for more information.</p>
				<p><strong>Technical planning in support of ROI analysis</strong></p>
				<p>Test automation can provide varying degrees of benefit to a test team. However, a significant level of effort and cost is associated with the implementation of an effective automated testing solution. Prior to incurring the time and effort to develop automated tests, an assessment should be conducted to evaluate what the intended and potential overall benefit and outcome of implementing test automation might be. Once this is determined, activities necessary to effect such a plan should be defined and associated costs should be determined in order to calculate the ROI.</p>
				<p>To adequately prepare for transitioning to an automated environment, the following areas need to be addressed:</p>
				<ul>
					<li>Availability of tools in the test environment for test automation</li>
					<li>Correctness of test data and test cases</li>
					<li>Scope of the test automation effort</li>
					<li>Education of test team to the paradigm shift</li>
					<li>Roles and responsibilities</li>
					<li>Cooperation between developers and test automation engineers</li>
					<li>Parallel effort</li>
					<li>Test automation reporting</li>
				</ul>
				<p><strong>Availability of tools in the test environment for test automation</strong></p>
				<p>Selected test tools need to be installed and confirmed to be functioning in the test lab environment. This may involve downloading any service packs or release updates, selecting the appropriate installation configurationâ€”including add-insâ€”necessary to support the <abbr title="System Under Test">SUT</abbr>, and ensuring the <abbr title="Test Automation Solution">TAS</abbr> functions correctly in the test lab environment vs. the automation development environment.</p>
				<p><strong>Correctness of test data and test cases</strong></p>
				<p>Correctness and completeness of manual test data and test cases is necessary to ensure use with automation will provide predictable results. Tests run under automation need explicit data for input, navigation, synchronization, and validation.</p>
				<p><strong>Scope of the test automation effort</strong></p>
				<p>In order to show early success in automation and gain feedback on technical issues which may impact progress, starting off with a limited scope will facilitate future automation tasks. A pilot project may target one area of a systemâ€™s functionality that is representative of overall system interoperability. Lessons learned from the pilot will help adjust future time estimates and schedules, and identify areas requiring specialized technical resources. A pilot project provides q quick way to show early automation success, which bolsters further management support.</p>
				<p>To help in this, test cases to be automated should be selected wisely. Pick the cases that require little effort to automate, but provide a high added value. Automatic regression or smoke tests can be implemented and add considerable value as these tests normally are executed quite often, even daily. Another good candidate to start with is reliability testing. These tests are often composed of steps and are executed over and over again, revealing problems which are hard to detect manually. These reliability tests take little effort to implement, but can show added value very soon.</p>
				<p>These pilot projects put the automation in the spotlight (manual test effort saved, or serious issues identified) and pave the way for further extensions (effort and money).</p>
				<p>Additionally, prioritization should be given to tests that are critical to the organization as these will show the greatest value initially. However, within this context, it is important that as part of a pilot effort, the most technically challenging tests to automate are avoided. Otherwise, too much effort will be spent trying to develop automation with too few results to show. As a general rule, identifying tests which share characteristics with a large part of the application will provide the necessary momentum to keep the automation effort alive.</p>
				<p><strong>Education of test team to paradigm shift</strong></p>
				<p>Testers come in many flavors: some are domain experts having come from the end user community or involvement as a business analyst, while others have strong technical skills which enable them to better understand the underlying system architecture. For testing to be effective, a broad mix of backgrounds is preferable. As the test team shifts to automation, roles will become more specialized. Changing the makeup of the test team is essential for automation to be successful, and educating the team early on of the intended change will help reduce anxiety over roles or the possible thought of being made redundant. When addressed correctly, the shift toward automation should get everybody on the test team very excited and ready to participate in the organizational and technical change.</p>
				<p><strong>Roles and responsibilities</strong></p>
				<p>Test automation should be an activity in which everybody can participate. However, that does not equate to everybody having the same role. Designing, implementing, and maintaining an automated test environment is technical in nature, and as such should be reserved for individuals with strong programming skills and technical backgrounds. The results of an automated testing development effort should be an environment that is usable by technical and non-technical individuals alike. In order to maximize the value of an automated test environment there is a need for individuals with domain expertise and testing skills as it will be necessary to develop the appropriate test scripts (including corresponding test data). These will be used to drive the automated environment and provide the targeted test coverage. Domain experts review reports to confirm application functionality, while technical experts ensure that the automated environment is operating correctly and efficiently. These technical experts can also be developers with an interest in testing. Experience in software development is essential for designing software which is maintainable, and this is of utmost importance in test automation. Developers can focus on the test automation framework or test libraries. The implementation of test cases should stay with testers.</p>
				<p><strong>Cooperation between developers and test automation engineers</strong></p>
				<p>Successful test automation also requires the involvement of the software development team as well as testers. Developers and testers will need to work much more closely together for test automation so that developers can provide support personnel and technical information on their development methods and tools. Test automation engineers may raise concerns about the testability of system designs and developer code. This especially will be the case if standards are not followed, or if developers use odd, homegrown or even very new libraries/objects. For example, developers might choose a third party <abbr title="Graphical User Interface">GUI</abbr> control which may not be compatible with the selected automation tool. Finally, an organizationâ€™s project management team must have a clear understanding about the types of roles and responsibilities required for a successful automation effort.</p>
				<p><strong>Parallel effort</strong></p>
				<p>As a part of transition activities, many organizations create a parallel team to begin the process of automating existing manual test scripts. The new automated scripts are then incorporated into the testing effort, replacing the manual scripts. However, prior to doing so, it is often recommended to compare and validate that the automated script is performing the same test and validation as the manual script it is replacing.</p>
				<p>In many instances, an assessment of the manual scripts will be made prior to conversion to automation. As a result of such an assessment, it might be determined that there is a need to restructure existing manualtest scripts to a more efficient and effective approach under automation.</p>
				<p><strong>Automation reporting</strong></p>
				<p>There are various reports that can automatically be generated by a <abbr title="Test Automation Solution">TAS</abbr>. These include pass/fail status of individual scripts or steps within a script, overall test execution statistics, and overall performance of the <abbr title="Test Automation Solution">TAS</abbr>. It is equally important to have visibility into the correct operation of the <abbr title="Test Automation Solution">TAS</abbr> so that any application specific results which are reported can be deemed accurate and complete (See Chapter 7: Verifying the <abbr title="Test Automation Solution">TAS</abbr>).</p>
				<h2 id="6-2-identify-steps-needed-to-implement-automation-within-regression-testing"><a class="color-inherit" href="#6-2-identify-steps-needed-to-implement-automation-within-regression-testing">6.2 Identify Steps Needed to Implement Automation within Regression Testing</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-6.2.1 (K2) Explain the factors to consider in implementing automated regression testing</p>
				<p>Regression testing provides a great opportunity to use automation. A regression test bed grows as todayâ€™s functional tests become tomorrowâ€™s regression tests. It is only a matter of time before the number of regression tests becomes greater than the time and resources available to a traditional manual test team.</p>
				<p>In developing steps to prepare to automate regression tests. A number of questions must be asked:</p>
				<ul>
					<li>How frequently should the tests be run?</li>
					<li>What is the execution time for each test, for the regression suite?</li>
					<li>Is there functional overlap between tests?</li>
					<li>Do tests share data?</li>
					<li>Are the tests dependent on each other?</li>
					<li>What pre-conditions are required before test execution?</li>
					<li>What % of <abbr title="System Under Test">SUT</abbr> coverage do the tests represent?</li>
					<li>Do the tests currently execute without failure?</li>
					<li>What should happen when regression tests take too long?</li>
				</ul>
				<p>Each of these is explained in more detail below.</p>
				<p><strong>Frequency of test execution</strong></p>
				<p>Test that are executed often as part of regression testing are the best candidates for automation. These tests have already been developed, exercise known <abbr title="System Under Test">SUT</abbr> functionality, and will have their execution time reduced tremendously through the use of automation.</p>
				<p><strong>Test execution time</strong></p>
				<p>The time it takes to execute any given test or an entire test suite is an important parameter in evaluating the value of implementing automated testing within regression testing. One option is to start by implementing automation on time-consuming tests. This will allow for each test to run more quickly and efficiently, while also adding additional cycles of automated regression test execution. The benefit is additional and more frequent feedback on <abbr title="System Under Test">SUT</abbr> quality, and reduced deployment risk.</p>
				<p><strong>Functional overlap</strong></p>
				<p>When automating existing regression tests, it is a good practice to identify any functional overlap that exists between and among test cases and, where possible, reduce that overlap in the equivalent automated test. This will bring further efficiencies in the automated test execution time, which will be significant as more and more automated test cases are executed. Often, tests developed using automation will take on a new structure since they depend on reusable components and shared data repositories. It is not uncommon to decompose existing manual tests into several smaller automated tests. Likewise, consolidation of several manual tests into a larger automated test may be the appropriate solution. Manual tests need to be evaluated individually, and as a group, so that an effective conversion strategy can be developed.</p>
				<p><strong>Data sharing</strong></p>
				<p>Tests often share data. This can occur when tests use the same record of data to execute different <abbr title="System Under Test">SUT</abbr> functionality. An example of this might be test case â€œAâ€ which verifies an employeeâ€™s available vacation time, while test case â€œBâ€ might verify what courses the employee took as part of their career development goals. Each test case uses the same employee, but verifies different parameters. In a manual test environment, the employee data would typically be duplicated many times across each manual test case which verified employee data using this employee. However, in an automated test, data which is shared shouldâ€”where possible and feasibleâ€”be stored and accessed from a single source to avoid duplication, or introduction of errors.</p>
				<p><strong>Test interdependency</strong></p>
				<p>When executing complex regression test scenarios, one test may have a dependency on one or more other tests. This occurrence can be quite common and may happen, by way of example, as a result of a new â€œOrder IDâ€ that gets created as a result of a test step. Subsequent tests may want to verify that: a) the new order is correctly displayed in the system, b) changes to the order are possible, or c) deleting the order is successful. In each case, the â€œOrder IDâ€ value which is dynamically created in the first test must be captured for reuse by later tests. Depending on the design of the <abbr title="Test Automation Solution">TAS</abbr>, this can be addressed.</p>
				<p><strong>Test preconditions</strong></p>
				<p>Often a test cannot be executed prior to setting initial conditions. These conditions may include selecting the correct database or the test data set from which to test, or setting initial values or parameters. Many of these initialization steps that are required to establish a testâ€™s precondition can be automated. This allows for a more reliable and dependable solution when these steps cannot be missed prior to executing the tests. As regression tests are converted to automation, these preconditions need to be a part of the automation process.</p>
				<p><strong><abbr title="System Under Test">SUT</abbr> coverage</strong></p>
				<p>Every time tests are executed, part of an <abbr title="System Under Test">SUT</abbr>â€™s functionality is exercised. In order to ascertain overall <abbr title="System Under Test">SUT</abbr> quality, tests need to be designed in order to have the broadest and deepest coverage. Additionally, code coverage tools can be used to monitor execution of automated tests to help quantify the effectiveness of the tests. Through automated regression testing, over time we can expect that additional tests will provide additional coverage. Measuring this provides an effective means of quantifying the value of the tests themselves.</p>
				<p><strong>Executable tests</strong></p>
				<p>Before converting a manual regression test into an automated test, it is important to verify that the manual test operates correctly. This then provides the correct starting point to ensure a successful conversion to an automated regression test. If the manual test does not execute correctlyâ€”either because it was poorly written, uses invalid data, is out of date or out of sync with the current <abbr title="System Under Test">SUT</abbr>, or as a result of an <abbr title="System Under Test">SUT</abbr> defectâ€”converting it to automation prior to understanding and/or resolving the root cause of the failure will create a non-functioning automated test which is wasteful and unproductive.</p>
				<p><strong>Large regression test sets</strong></p>
				<p>The set of regression tests for an <abbr title="System Under Test">SUT</abbr> can become quite large, so large that the test set cannot be completely executed overnight, or over the weekend. In that case, concurrent execution of test cases is a possibility if multiple <abbr title="System Under Test">SUT</abbr>s are available (for PC applications this probably does not pose a problem, but when the <abbr title="System Under Test">SUT</abbr> consists of an airplane or space rocket this is a different story). <abbr title="System Under Test">SUT</abbr>s can be scarce and/or expensive making concurrency an unrealistic option. In this case, a possibility may be to run only parts of the regression test. Over time (weeks) the complete set eventually will be run. The choice of which part of the regression test suite to execute can also be based on a risk analysis (which parts of the <abbr title="System Under Test">SUT</abbr> have been changed lately?).</p>
				<h2 id="6-3-factors-to-consider-when-implementing-automation-within-new-feature-testing"><a class="color-inherit" href="#6-3-factors-to-consider-when-implementing-automation-within-new-feature-testing">6.3 Factors to Consider when Implementing Automation within New Feature Testing</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-6.3.1 (K2) Explain the factors to consider in implementing automation within new feature testing</p>
				<p>In general it is easier to automate test cases for new functionality as the implementation is not yet finished (or better: not yet started). The test engineer can use his knowledge to explain to the developers and architects what exactly is needed in the new functionality such that it can be tested effectively and efficiently by the test automation solution</p>
				<p>As new features are introduced into an <abbr title="System Under Test">SUT</abbr>, testers are required to develop new tests against these new features and corresponding requirements. The <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> must solicit feedback from test designers with domain expertise and determine if the current <abbr title="Test Automation Solution">TAS</abbr> will meet the needs of the new features. This analysis includes, but is not limited to, the existing approach used, third party development tools, test tools used, etc.</p>
				<p>Changes to the <abbr title="Test Automation Solution">TAS</abbr> must be evaluated against the existing automated testware components so that changes or additions are fully documented, and do not affect the behavior (or performance) of existing <abbr title="Test Automation Solution">TAS</abbr> functionality.</p>
				<p>If a new feature is implemented with, as an example, a different class of object, it may be necessary to make updates or additions to the testware components. Additionally, compatibility with existing test tools must be evaluated and, where necessary, alternative solutions identified. For example, if using a keyword- driven approach, it may be necessary to develop additional keywords or modify/expand existing keywords to accommodate the new functionality.</p>
				<p>There may be a requirement to evaluate additional testing tools to support the new environment under which the new functionality exists. For example, a new testing tool might be necessary if the existing testing tool only supports HTML.</p>
				<p>New test requirements may affect existing automated tests and testware components. Therefore, prior to making any changes, existing automated tests should be run against the new/updated <abbr title="System Under Test">SUT</abbr> to verify and record any changes to proper operation of the existing automated tests. This should include mapping interdependencies to other tests. Any new changes in technology will necessitate evaluating the current testware components (including test tools, function libraries, APIs, etc.) and compatibility with the existing <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<p>When existing requirements change, the effort to update test cases which verify these requirements should be part of the project schedule (work breakdown structure). Traceability from the requirements to the test cases will indicate which test cases need to be updated. These updates should be part of the overall plan.</p>
				<p>Finally, one needs to determine if the existing <abbr title="Test Automation Solution">TAS</abbr> will continue to meet current <abbr title="System Under Test">SUT</abbr> needs. Are implementation techniques still valid, or is a new architecture required, and can this be done by extending current capability?</p>
				<p>When new functionality is being introduced, this is an opportunity for test engineers to make sure that the newly defined functionality will be testable. During the design phase, testing should be taken into account by planning to provide test interfaces which can be used by scripting languages or the test automation tool to verify the new functionality. See Section 2.3, Design for Testability and Automation, for more information.</p>
				<h2 id="6-4-factors-to-consider-when-implementing-automation-of-confirmation-testing"><a class="color-inherit" href="#6-4-factors-to-consider-when-implementing-automation-of-confirmation-testing">6.4 Factors to Consider when Implementing Automation of Confirmation Testing</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-6.4.1 (K2) Explain the factors to consider in implementing automated confirmation testing</p>
				<p>Confirmation testing is performed following a code fix that addresses a reported defect. A tester typically follows the steps necessary to replicate the defect to verify that the defect no longer exists.</p>
				<p>Defects have a way of reintroducing themselves into subsequent releases (this may indicate a configuration management problem) and therefore confirmation tests are prime candidates for automation. Using automation will help reduce execution time for confirmation testing. The confirmation test can be added to, and complement, the existing automated regression test bed.</p>
				<p>The automated confirmation test typically has a narrow scope of functionality. Implementation can occur at any point once a defect is reported and the steps needed to replicate it are understood. Automated confirmation tests can be incorporated into a standard automated regression suite or, where practical, subsumed into existing automated tests. With either approach, the value of automating defect confirmation testing still holds.</p>
				<p>Tracking automated confirmation tests allows for additional reporting of time and number of cycles expended in resolving defects.</p>
				<p>In addition to confirmation testing regression testing is necessary to ensure new defects have not been introduced as a side effect of the defect fix. Impact analysis may be required to determine the appropriate scope of regression testing.</p>
			</article>

			<article id="7-verifying-the-tas">
				<h1><a class="color-inherit" href="#7-verifying-the-tas">7 Verifying the <abbr title="Test Automation Solution">TAS</abbr></a></h1>
				<h2 id="7-1-verifying-automated-test-environment-components"><a class="color-inherit" href="#7-1-verifying-automated-test-environment-components">7.1 Verifying Automated Test Environment Components</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-7.1.1 (K3) Verify the correctness of an automated test environment including test tool setup</p>
				<p>The test automation team needs to verify that the automated test environment is working as expected. These checks are done, for example, before starting automated testing. </p>
				<p>There are a number of steps that can be taken to verify the components of the automated test environment. Each of these is explained in more detail below:</p>
				<p><strong>Test tool installation, setup, configuration, and customization</strong></p>
				<p>The <abbr title="Test Automation Solution">TAS</abbr> is comprised of many components. Each of these needs to be accounted for to ensure reliable and repeatable performance. At the core of a <abbr title="Test Automation Solution">TAS</abbr> are the executable components, corresponding functional libraries, and supporting data and configuration files. The process of configuring a <abbr title="Test Automation Solution">TAS</abbr> may range from the use of automated installation scripts to manually placing files in corresponding folders. Testing tools, much like operating systems and other applications, regularly have service packs or may have optional or required add-ins to ensure compatibility with any given <abbr title="System Under Test">SUT</abbr> environment.</p>
				<p>Automated installation (or copy) from a central repository has advantages. It can be guaranteed that tests on different <abbr title="System Under Test">SUT</abbr>s have been performed with the same version of the <abbr title="Test Automation Solution">TAS</abbr>, and the same configuration of the <abbr title="Test Automation Solution">TAS</abbr>, where this is appropriate. Upgrades to the <abbr title="Test Automation Solution">TAS</abbr> can be made through the repository. Repository usage and the process to upgrade to a new version of the <abbr title="Test Automation Solution">TAS</abbr> should be the same as for standard development tools.</p>
				<p><strong>Test scripts with known passes and failures</strong></p>
				<p>When known passing test cases fail, it is immediately clear that something is fundamentally wrong and should be fixed as soon as possible. Conversely, when test cases pass even though they should have failed, we need to identify the component that did not function correctly. It is important to verify the correct generation of log files and performance metrics as well as automated setup and teardown of the test case/script. It is also helpful to execute a few tests from the different test types and levels (functional tests, performance tests, component tests, etc.). This should also be performed on the level of the framework.</p>
				<p><strong>Repeatability in setup/teardown of the test environment</strong></p>
				<p>A <abbr title="Test Automation Solution">TAS</abbr> will be implemented on a variety of systems and servers. To ensure that the <abbr title="Test Automation Solution">TAS</abbr> works properly in each environment, it is necessary to have a systematic approach to loading and unloading the <abbr title="Test Automation Solution">TAS</abbr> from any given environment. This is successfully achieved when the building and rebuilding of the <abbr title="Test Automation Solution">TAS</abbr> provides no discernible difference in how it operates within and across multiple environments. Configuration management of the <abbr title="Test Automation Solution">TAS</abbr> components ensures that a given configuration can dependably be created.</p>
				<p><strong>Configuration of the test environment and components</strong></p>
				<p>Understanding and documenting the various components that comprise the <abbr title="Test Automation Solution">TAS</abbr> provides the necessary knowledge for what aspects of the <abbr title="Test Automation Solution">TAS</abbr> may be affected or require change when the <abbr title="System Under Test">SUT</abbr> environment changes.</p>
				<p><strong>Connectivity against internal and external systems/interfaces</strong></p>
				<p>Once a <abbr title="Test Automation Solution">TAS</abbr> is installed in a given <abbr title="System Under Test">SUT</abbr> environment, and prior to actual use against an <abbr title="System Under Test">SUT</abbr>, a set of checks or preconditions should be administered to ensure that connectivity to internal and external systems, interfaces, etc., is available. Establishing preconditions for automation is essential in ensuring that the <abbr title="Test Automation Solution">TAS</abbr> has been installed and configured correctly</p>
				<p><strong>Intrusiveness of automated test tools</strong></p>
				<p>The <abbr title="Test Automation Solution">TAS</abbr> often will be tightly coupled with the <abbr title="System Under Test">SUT</abbr>. This is by design so that there is a high level of compatibility especially as it pertains to <abbr title="Graphical User Interface">GUI</abbr> level interactions. However, this tight integration may also have negative effects. These may include: a <abbr title="System Under Test">SUT</abbr> behaves differently when the <abbr title="Test Automation Solution">TAS</abbr> resides within the <abbr title="System Under Test">SUT</abbr> environment; the <abbr title="System Under Test">SUT</abbr> has different behavior than when used manually; <abbr title="System Under Test">SUT</abbr> performance is affected with the <abbr title="Test Automation Solution">TAS</abbr> in the environment or when executing the <abbr title="Test Automation Solution">TAS</abbr> against the <abbr title="System Under Test">SUT</abbr>.</p>
				<p>The level of intrusion/intrusiveness differs with the chosen automated test approach. For example:</p>
				<ul>
					<li>When interfacing with the <abbr title="System Under Test">SUT</abbr> from external interfaces, the level of intrusion will be very low. External interfaces can be electronic signals (for physical switches), USB signals for USB devices (like keyboards). With this approach the end user is simulated in the best way. In this approach the software of the <abbr title="System Under Test">SUT</abbr> is not changed at all for testing purposes. The behavior and the timing of the <abbr title="System Under Test">SUT</abbr> are not influenced by the test approach. Interfacing with the <abbr title="System Under Test">SUT</abbr> in this way can be very complex. Dedicated hardware might be necessary, hardware description languages are needed to interface with the <abbr title="System Under Test">SUT</abbr>, etc. For software only systems this is not a typical approach, but for products with embedded software this approach is more common.</li>
					<li>When interfacing with the <abbr title="System Under Test">SUT</abbr> on the <abbr title="Graphical User Interface">GUI</abbr> level, the <abbr title="System Under Test">SUT</abbr> environment is adapted in order to inject <abbr title="User Interface">UI</abbr> commands and to extract information needed by the test cases. The behavior of the <abbr title="System Under Test">SUT</abbr> is not directly changed, but the timing is affected which can result in an impact on the behavior. The level of intrusion is higher than in the previous point but interfacing with the <abbr title="System Under Test">SUT</abbr> in this way is less complex. Often commercial off-the-shelf tools can be used for this type of automation.</li>
					<li>Interfacing with the <abbr title="System Under Test">SUT</abbr> can be done via test interfaces in the software or by using existing interfaces already provided by the software. The availability of these interfaces (APIs) is an important part of the design for testability. The level of intrusion can be quite high in this case. Automated tests use interfaces which might not be used by end users of the system at all (test interfaces) or interfaces may be used in a different context than in the real world. On the other hand, it is very easy and inexpensive to perform automated tests via interfaces (API). Testing the <abbr title="System Under Test">SUT</abbr> via test interfaces can be a solid approach as long as the potential risk is understood.</li>
				</ul>
				<p>A high level of intrusion can show failures during testing that are not evident in real world use conditions. If this causes failures with the automated tests, the confidence in the test automation solution can drop dramatically. Developers may require that failures identified by automated testing should first be reproduced manually, if possible, in order to assist with the analysis.</p>
				<p><strong>Framework Component Testing</strong></p>
				<p>Much like any software development project, the automated framework components need to be individually tested and verified. This may include functional and non-functional (performance, resource utilization, usability, etc.) testing.</p>
				<p>For example, components that provide object verification on <abbr title="Graphical User Interface">GUI</abbr> systems need to be tested for a wide range of object classes in order to establish that object verification functions correctly. Likewise, error logs and reports should produce accurate information regarding the status of automation and <abbr title="System Under Test">SUT</abbr> behavior.</p>
				<p>Examples of non-functional testing may include understanding framework performance degradation, utilization of system resources that may indicate problems such as memory leaks. Interoperability of components within and/or outside of the framework.</p>
				<h2 id="7-2-verifying-the-automated-test-suite"><a class="color-inherit" href="#7-2-verifying-the-automated-test-suite">7.2 Verifying the Automated Test Suite</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-7.2.1 (K3) Verify the correct behavior for a given automated test script and/or test suite</p>
				<p>Automated test suites need to be tested for completeness, consistency, and correct behavior. Different kinds of verification checks can be applied to make sure the automated test suite is up and running at any given time, or to determine that it is fit for use.</p>
				<p>There are a number of steps that can be taken to verify the automated test suite. These include:</p>
				<ul>
					<li>Executing test scripts with known passes and failures</li>
					<li>Checking the test suite</li>
					<li>Verifying new tests that focus on new features of the framework</li>
					<li>Considering the repeatability of tests</li>
					<li>Checking that there are enough verification points in the automated test suite.</li>
				</ul>
				<p>Each of these is explained in more detail below.</p>
				<p><strong>Executing test scripts with known passes and failures</strong></p>
				<p>When known passing test cases fail, it is immediately clear that something is fundamentally wrong and should be fixed as soon as possible. Conversely, when a test suite passes even though it should have failed, it is necessary to identify the test case that did not function correctly. It is important to verify the correct generation of log files, performance data, setup and teardown of the test case/script. It is also helpful to execute a few tests from the different test types and levels (functional tests, performance tests, component tests, etc.).</p>
				<p><strong>Checking the test suite</strong></p>
				<p>Check the test suite for completeness (test cases all have expected results, test data present), and correct version with the framework and <abbr title="System Under Test">SUT</abbr>.</p>
				<p><strong>Verifying new tests that focus on new features of the framework</strong></p>
				<p>The first time a new feature of the <abbr title="Test Automation Solution">TAS</abbr> is actually being used in test cases, it should be verified and monitored closely to ensure the feature is working correctly. </p>
				<p><strong>Considering repeatability of tests</strong></p>
				<p>When repeating tests, the result/verdict of the test should always be the same. Having test cases in the test set which do not give a reliable result (e.g., race conditions) could be moved from the active automated test suite and analyzed separately to find the root cause. Otherwise time will be spent repeatedly on these test runs to analyze the problem.</p>
				<p>Intermittent failures need to be analyzed. The problem can be in the test case itself or in the framework (or it might even be an issue in the <abbr title="System Under Test">SUT</abbr>). Log file analysis (of the test case, framework and <abbr title="System Under Test">SUT</abbr>) can identify the root cause of the problem. Debugging may also be necessary. Support from the test analyst, software developer, and domain expert may be needed to find the root cause.</p>
				<p><strong>Checking that there are enough verification points in the automated test suite and/or test cases</strong></p>
				<p>It must be possible to verify that the automated test suite has been executed and has achieved the expected results. Evidence must be provided to ensure the test suite and/or test cases have run as expected. This evidence can include logging at the start and end of each test case, recording the test execution status for each completed test case, verification that the post conditions have been achieved, etc.</p>
			</article>

			<article id="8-continuous-improvement">
				<h1><a class="color-inherit" href="#8-continuous-improvement">8 Continuous Improvement</a></h1>
				<h2 id="8-1-options-for-improving-test-automation"><a class="color-inherit" href="#8-1-options-for-improving-test-automation">8.1 Options for Improving Test Automation</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-8.1.1 (K4) Analyze the technical aspects of a deployed test automation solution and provide recommendations for improvement</p>
				<p>In addition to the on-going maintenance tasks necessary to keep the <abbr title="Test Automation Solution">TAS</abbr> synchronised with the <abbr title="System Under Test">SUT</abbr>, there are typically many opportunities to improve the <abbr title="Test Automation Solution">TAS</abbr>. <abbr title="Test Automation Solution">TAS</abbr> improvements may be undertaken to achieve a range of benefits including greater efficiency (further reducing manual intervention), better ease of use, additional capabilities and improved support for testing activities. The decision as to how the <abbr title="Test Automation Solution">TAS</abbr> is improved will be influenced by the benefits that will add the most value to a project.</p>
				<p>Specific areas of a <abbr title="Test Automation Solution">TAS</abbr> that may be considered for improvement include scripting, verification, architecture, pre- and post-processing, documentation, and tool support. These are described in more detail below.</p>
				<p><strong>Scripting</strong></p>
				<p>Scripting approaches vary from the simple structured approach to data-driven approaches and on to the more sophisticated keyword-driven approaches, as described in Section 3.2.2. It may be appropriate to upgrade the current <abbr title="Test Automation Solution">TAS</abbr> scripting approach for all new automated tests. The approach may be retrofitted to all the existing automated tests or at least those that involve the greatest amount of maintenance effort.</p>
				<p>Rather than change the scripting approach altogether, <abbr title="Test Automation Solution">TAS</abbr> improvements may focus on the implementation of scripts. For example:</p>
				<ul>
					<li>Assess test case/step/procedure overlap in an effort to consolidate automated tests. Test cases containing similar sequences of actions should not implement these steps multiple times. These steps should be made into a function and added to a library, so that they can be reused. These library functions can then be used by different test cases. This increases the maintainability of the testware. When test steps are not identical but similar, parameterization may be necessary. <strong>Note:</strong> this is a typical approach in keyword-driven testing.</li>
					<li>Establish an error recovery process for the <abbr title="Test Automation Solution">TAS</abbr> and <abbr title="System Under Test">SUT</abbr>. When an error occurs during the execution of test cases, the <abbr title="Test Automation Solution">TAS</abbr> should be able to recover from this error condition in order to be able to continue with the next test case. When an error occurs in the <abbr title="System Under Test">SUT</abbr>, the <abbr title="Test Automation Solution">TAS</abbr> needs to be able to perform necessary recovery actions on the <abbr title="System Under Test">SUT</abbr> (e.g., a reboot of the complete <abbr title="System Under Test">SUT</abbr>).</li>
					<li>
						Evaluate wait mechanisms to ensure the best type is being used.
						<br>There are three common wait mechanisms:
						<ol>
							<li>Hard-coded waits (wait a certain number of milliseconds) can be a root cause for many test automation problems.</li>
							<li>
								Dynamic waiting by polling, e.g., checking for a certain state change or action has taken place, is much more flexible and efficient:
								<ul>
									<li>It waits only the needed time and no test time is wasted</li>
									<li>When for some reason the process takes longer, the polling will just wait until the condition is true. Remember to include a timeout mechanism, otherwise the test may wait forever in case of a problem.</li>
								</ul>
							</li>
							<li>An even better way is to subscribe to the event mechanism of the <abbr title="System Under Test">SUT</abbr>. This is much more reliable than the other two options, but the test scripting language needs to support event subscription and the <abbr title="System Under Test">SUT</abbr> needs to offer these events to the test application. Remember to include a timeout mechanism, otherwise the test may wait forever in case of a problem</li>
						</ol>
					</li>
					<li>Treat the testware as software. Development and maintenance of testware is just a form of software development. As such good coding practices (e.g., using coding guidelines, static analysis, code reviews) should be applied. It may even be a good idea to use software developers (instead of test engineers) to develop certain parts of the testware (e.g., libraries).</li>
					<li>Evaluate existing scripts for revision/elimination. Several scripts may be troublesome (e.g., failing now and then, or high maintenance costs), and it may be wise to redesign these scripts. Other test scripts can be removed from the suite because they are no longer adding any value.</li>
				</ul>
				<p><strong>Test Execution</strong></p>
				<p>When an automated regression test suite is not finished overnight, this should not come as a surprise. When the testing takes too long, it may be necessary to test concurrently on different systems, but this is not always possible. When expensive systems (targets) are used for testing, it can be a constraint that all testing must be done on a single target. It may be necessary to split the regression test suite into multiple parts, each executing in a defined period of time (e.g., in a single night). Further analysis of the automated test coverage may reveal duplication. Removing duplication can reduce execution time and can yield further efficiencies. Further analysis of the automated test coverage may reveal duplication. Removing duplication can reduce execution time and can yield further efficiencies.</p>
				<p><strong>Verification</strong></p>
				<p>Before creating new verification functions, adopt a set of standard verification methods for use by all automated tests. This will avoid the re-implementation of verification actions across multiple tests. When verification methods are not identical but similar, the use of parameterization will aid in allowing a function to be used across multiple types of objects.</p>
				<p><strong>Architecture</strong></p>
				<p>It may be necessary to change the architecture in order to support improvements of the testability of the <abbr title="System Under Test">SUT</abbr>. These changes may be made in the architecture of the <abbr title="System Under Test">SUT</abbr> and/or in the architecture of the automation. This can provide a major improvement in the test automation, but may require significant changes and investment in the <abbr title="System Under Test">SUT</abbr>/<abbr title="Test Automation Solution">TAS</abbr>. For example, if the <abbr title="System Under Test">SUT</abbr> is going to be changed to provide APIs for testing then the <abbr title="Test Automation Solution">TAS</abbr> should also be refactored accordingly. Adding these kinds of features at a later stage can be quite expensive; it is much better to think of this at the start of automation (and in the early stages of the development of the <abbr title="System Under Test">SUT</abbr> â€“ see Section 2.3 Design for Testability and Automation).</p>
				<p><strong>Pre- and post-processing</strong></p>
				<p>Provide standard setup and teardown tasks. These are also known as pre-processing (setup) and post- processing (teardown). This saves the tasks being implemented repeatedly for each automated test not only reducing maintenance costs but also reducing the effort required to implement new automated tests.</p>
				<p><strong>Documentation</strong></p>
				<p>This covers all forms of documentation from script documentation (what the scripts do, how they should be used, etc.), user documentation for the <abbr title="Test Automation Solution">TAS</abbr>, and the reports and logs produced by the <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<p><strong><abbr title="Test Automation Solution">TAS</abbr> features</strong></p>
				<p>Add additional <abbr title="Test Automation Solution">TAS</abbr> features and functions such as detailed reporting, logs, integration to other systems, etc. Only add new features when these will indeed be used. Adding unused features only increases complexity and decreases reliability and maintainability.</p>
				<p><strong><abbr title="Test Automation Solution">TAS</abbr> updates and upgrades</strong></p>
				<p>By updating or upgrading to new versions of the <abbr title="Test Automation Solution">TAS</abbr>, new functions may become available that can be used by the test cases (or failures may be corrected). The risk is that updating the framework (by either upgrading the existing test tools or introducing new ones) might have a negative impact on existing test cases. Test the new version of the test tool by running sample tests before rolling out the new version. The sample tests should be representative of the automated tests of different applications, different test types and, where appropriate, different environments.</p>
				<h2 id="8-2-planning-the-implementation-of-test-automation-improvement"><a class="color-inherit" href="#8-2-planning-the-implementation-of-test-automation-improvement">8.2 Planning the Implementation of Test Automation Improvement</a></h2>
				<p class="learning-objective color-secondary">ALTA-E-8.2.1 (K4) Analyze the automated testware, including test environment components, tools and supporting function libraries, in order to understand where consolidation and updates should be made following a given set of test environment or <abbr title="System Under Test">SUT</abbr> changes</p>
				<p>Changes to an existing <abbr title="Test Automation Solution">TAS</abbr> require careful planning and investigation. Much effort has been expended in creating a robust <abbr title="Test Automation Solution">TAS</abbr> consisting of a <abbr title="Test Automation Framework">TAF</abbr> and component libraries. Any change, no matter how trivial, can have wide ranging impact on the reliability and performance of the <abbr title="Test Automation Solution">TAS</abbr>.</p>
				<p><strong>Identify changes in the test environment components</strong></p>
				<p>Evaluate what changes and improvement need to be made. Do these require changes to the testing software, customized function libraries, OS? Each of these has an impact on how the <abbr title="Test Automation Solution">TAS</abbr> performs. The overall goal is to ensure automated tests continue to run in an efficient manner. Changes should be made incrementally so that the impact on the <abbr title="Test Automation Solution">TAS</abbr> can be measured through a limited run of test scripts. Once it is found that no detrimental effect exists, changes can be fully implemented. A full regression run is the final step toward validating that the change did not adversely affect the automated scripts. During execution of these regression scripts, errors may be found. Identifying the root cause of these errors (through reporting, logs, data analysis, etc.) will provide a means to ensure that they are not resulting from the automation improvement activity.</p>
				<p><strong>Increase efficiency and effectiveness of core <abbr title="Test Automation Solution">TAS</abbr> function libraries</strong></p>
				<p>As a <abbr title="Test Automation Solution">TAS</abbr> matures, new ways are discovered to perform tasks more efficiently. These new techniques (which include optimizing code in functions, using newer operating system libraries, etc.) need to be incorporated into the core function libraries that are used by the current project and all projects.</p>
				<p><strong>Target multiple functions that act on the same control type for consolidation</strong></p>
				<p>A large part of what occurs during an automated test run is the interrogation of controls in the <abbr title="Graphical User Interface">GUI</abbr>. This interrogation serves to provide information about that control (e.g., visible/not visible, enabled/not enabled, size and dimensions, data, etc.). With this information, an automated test can select an item from a dropdown list, enter data into a field, read a value from a field, etc. There are several functions that can act upon controls to elicit this information. Some functions are extremely specialized, while others are more general in nature. For example, there may be a specific function that works only on dropdown lists. Alternatively, there may be a function (or one may be created and used within the <abbr title="Test Automation Solution">TAS</abbr>) that works with several functions by specifying a function as one of its parameters. Therefore, a <abbr title="Test Automation Architecture"><abbr title="Test Automation Engineer">TAE</abbr></abbr> may use several functions that can be consolidated into fewer functions, achieving the same results and minimizing the maintenance requirement.</p>
				<p><strong>Refactor the <abbr title="Test Automation Architecture">TAA</abbr> to accommodate changes in the <abbr title="System Under Test">SUT</abbr></strong></p>
				<p>Through the life of a <abbr title="Test Automation Solution">TAS</abbr>, changes will need to be made to accommodate changes in the <abbr title="System Under Test">SUT</abbr>. As the <abbr title="System Under Test">SUT</abbr> evolves and matures, the underlying <abbr title="Test Automation Architecture">TAA</abbr> will have to evolve as well to ensure that the capability is there to support the <abbr title="System Under Test">SUT</abbr>. Care must be taken when extending features so that they are not implemented in a bolt-on manner, but instead are analyzed and changed at the architectural level of the automated solution. This will ensure that as new <abbr title="System Under Test">SUT</abbr> functionality requires additional scripts, compatible components will be in place to accommodate these new automated tests.</p>
				<p><strong>Naming conventions and standardization</strong></p>
				<p>As changes are introduced, naming conventions for new automation code and function libraries need to be consistent with previously defined standards (see Section 4.3.2 Scope and Approach).</p>
				<p><strong>Evaluation of existing scripts for <abbr title="System Under Test">SUT</abbr> revision/elimination</strong></p>
				<p>The process of change and improvement also includes an assessment of existing scripts, their use and continued value. For example, if certain tests are complex and time consuming to run, decomposing them into several smaller tests can be more viable and efficient. Targeting tests that run infrequently or not at all for elimination will pare down the complexity of the <abbr title="Test Automation Solution">TAS</abbr> and bring greater clarity to what needs to be maintained.</p>
			</article>

		</div><!--/column-->
	</div><!--/flex row-->
</div><!--/container-->

</body>
